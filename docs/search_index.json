[["index.html", "Integration of Photo Interpreted and LiDAR Attributes into a Polygonal Forest Inventory Framework 1 Introduction 1.1 Project Summary 1.2 Project Partners", " Integration of Photo Interpreted and LiDAR Attributes into a Polygonal Forest Inventory Framework Ethan Berman Research Scientist Integrated Remote Sensing Studio University of British Columbia 2023-04-27 1 Introduction Welcome! This website serves as the information hub for the Integration of Photo Interpreted and LiDAR Attributes into a Polygonal Forest Inventory Framework project. The project is led by Prof. Nicholas Coops from 2021-2023 and made possible through a Forestry Futures Trust Ontario grant. Here you will find details about the project partners, project objectives, and analyses completed including coding demos and results. For further inquiries please feel free to email me. 1.1 Project Summary The acquisition of Airborne Laser Scanning (ALS) Single Photon Lidar over the forested area of Ontario is redefining how forest attributes are predicted and monitored throughout the Province. A key question remains however, of how to aggregate these area-based (raster) estimates of forest attributes into traditional strategic or tactical-level inventory polygons. This project is designed to address this need. Outcomes of the project will be open source segmentation and attribute prediction tools to develop a polygon based forest inventory inclusive of both ALS and interpreted forest stand attributes as well as knowledge transfer activities and demonstration at a number of forest management units. 1.2 Project Partners This project is principally a partnership between the University of British Columbia, GreenFirst Forest Products, the Ontario Ministry of Natural Resources and Forestry, and Forestry Futures Trust Ontario. GreenFirst Forest Products (previously RYAM Forest Management) and the University of British Columbia have a history of successful collaboration including the use of LiDAR and aerial photogrammetry for spruce budworm assessment and regeneration mapping. This project will also build upon the very strong links UBC has with the Canadian Wood Fiber Centre (CWFC), which has thus far culminated in more than 20 peer-reviewed publications on CWFC-funded work, including the award winning best practices guide. In addition, UBC, GreenFirst Forest Products and the Ontario Ministry of Natural Resources and Forestry are involved in a large NSERC funded project, Silv@21, which is a collaborative research program across Canada focused on developing and applying new silvicultural approaches to forest management under changing forestry demand and markets, climate and diverse public expectations. The questions addressed throughout this project are beyond the scope of Silv@21 and therefore an additional effort was needed. Working with the province and industry with advanced remote sensing data sources such as ALS ensures that the new forest inventory provides detailed information on forest composition and structure to inform how much wood can be harvested sustainably across the province. By developing tools which allow for integration of raster-based forest inventory attributes from the ALS and utilizing the recognized benefits of the Forest Resources Inventory (FRI) polygonal inventory, Forestry Futures Trust Ontario has the opportunity to advance forest inventory within Ontario in a significant way. Issues around data management, modeling and fusion of these datasets together into one integrated accurate and operational inventory is of critical importance. This project provides open access tools and methodologies to allow this to be done. The partnerships assembled through private industry, government and universities to address the needs highlighted in this project are critically important and are in direct support of the new Ontario Forest Sector strategy and the Crown Forest Sustainability Act by providing foundational information for economically sustainable forest management planning and spatial planning in the Province. "],["people.html", "2 People 2.1 University of British Columbia 2.2 GreenFirst Forest Products 2.3 Ontario Ministry of Natural Resources and Forestry 2.4 Laval University", " 2 People This project would not be possible without dedicated collaborators. 2.1 University of British Columbia Prof. Nicholas Coops, project lead Professor Nicholas Coops holds a Canada Research Chair in Remote Sensing (Tier 1) at UBC. He has published &gt;460 total referenced peer-reviewed journal publications and is internationally recognised as a scientific leader in the field of remote sensing. He was the principal investigator of the AWARE project, a 5-year research project focused on developing LiDAR applications in Canada for forestry applications. He is a co-author of the Canadian Forest Service LiDAR best practice guide series, the most downloaded CFS information handbook ever, focused on LiDAR data processing. In 2020 he was the co-recipient of the Marcus Wallenberg prize for scientific achievements contributing to significantly broadening knowledge and technical development within the field of Forestry. Coops has lead a previous, successful Forestry Futures Trust Ontario grant investigating the application of SPL data for forest inventory and plot stratification in Ontario. Responsibilities As the principal investigator, Coops will manage the budget and supervise Ethan Berman, the research scientist working on the project. At the completion of the project Berman will ensure the software is suitable for open and public release, allowing the methods to be applied to other forest management areas in Ontario covered by SPL data and to be used by Provincial staff. Ethan Berman, research scientist Ethan Berman is a research scientist in the Integrated Remote Sensing Studio (IRSS) at UBC. He has a background in mathematics and remote sensing, and has worked extensively with large spatial datasets, developing novel approaches and building models to answer questions relating to forests, snow, vegetation, and climate change. Ethan received his MSc in 2019 under the supervision of Prof. Coops and before starting work on this project was a consultant for the United States Geological Survey, constructing, curating, and managing spatial datasets for a team of ecologists. Responsibilities Berman will be responsible for data management, algorithm development and testing, and delivery of open source software tools to the project partners. He will be supervised by Prof. Coops at UBC. 2.2 GreenFirst Forest Products Chris McDonell, partner Chris McDonell is the Chief Forester for Ontario at GreenFirst Forest Products. He is accountable for relations between GreenFirst and First Nation and Metis communities in Ontario and Quebec. He is also the coordinator of forest certification standard implementation (FSC) and liaison with various public, private and community organizations. Grant McCartney, partner Grant McCartney was a spatial analyst and forest information systems coordinator with RYAM Forest Management (now GreenFirst Forest Products). He recently transitioned to a role at Forsite Consultants. He performed spatial analysis in support of forest management planning (FMP) and forestry operations on the Gordon Cosens, Romeo Malette and Martel – Magpie Forests in Northeastern Ontario, Canada and led the acquisition of remotely sensed data including Digital Aerial Photogrammetry (DAP) and LiDAR for RYAM. He has been an industry partner in numerous research efforts including: the Assessment of Wood Attributes for REmote sensing (AWARE) project, CWFC – FIP , past FFT - KKTD projects and the recently approved Silv@21. Responsibilities We will work with GreenFirst Forest Products at a number of their forest management areas where FRI data and EFI data exist. GreenFirst partners will provide plot data, SPL and any other relevant datasets over the areas to ensure accurate model development. They will also attend project meetings, assess model accuracy with GreenFirst staff and test developed software as required. 2.3 Ontario Ministry of Natural Resources and Forestry Ian Sinclair, partner Ian Sinclair is a terrestrial landscape analyst with the Ontario Ministry of Natural Resources and Forestry, where he is responsible for the development of remotely sensed inventory and landscape level monitoring and reporting programs. His focus includes the development of LiDAR attributes and remotely sensed applications to support a continuous forest inventory program. Ian also coordinates acquisition of the Ministry’s SPL program and supports the operational aspects of the Vegetation Sampling Network Protocol for development of LiDAR based forest inventories. Geordie Robere-McGugan, partner Geordie Robere-McGugan is the Inventory Development Specialist at the Ontario Ministry of Natural Resources and Forestry and the lead on the modernization of the Forest Resources Inventory ensuring the inventory products meet the Provincial Policy Framework. Responsibilities Sinclair and Robere-McGugan have significant expertise in the FRI mapping process, SPL data capture and inventory development. Including government partners on the project team ensures that the outcomes will have immediate uptake by other users within the Ministry and there will be a successful technology transfer and knowledge exchange. Sinclair and Robere-McGugan will be available for project update meetings and to give advice to the research team. They will also be available to test versions of the developed open source code and help ensure documentation is easy to access and appropriate. 2.4 Laval University Alexis Achim, advisor Alexis Achim is a professor in the Faculty of Forestry, Geography and Geomatics at Laval University. His research interests are at the interface between forest management and wood science. He specializes in modelling the effects of forest management practices on wood fibre attributes. He was the scientific leader of the research theme on wood properties modelling within ForValueNet, the NSERC strategic Network on forest management for value-added products. Responsibilities Achim will serve as a project advisor, providing valuable feedback to the development and implementation of scientific methods used to achieve the project objectives. "],["project-details.html", "3 Project Details 3.1 Objective 3.2 Themes 3.3 Description 3.4 Study Area 3.5 Methodology 3.6 Knowledge &amp; Technology Transfer", " 3 Project Details 3.1 Objective The objective of this project is to develop open source tools to integrate current ALS-based forest inventory attributes with pre-existing Ontario-wide Forest Resources Inventory (FRI) data to map key forest attributes of interest (specifically age and species composition) in a polygonal format over entire forest management areas in Ontario. 3.2 Themes Enhance the production of the Enhanced Forest Resource Inventory (eFRI) Integrate and refine photo interpreted attributes and ALS Develop object-based image assessment techniques to segment forest stands from ALS attributes Build a methodology to impute age and species composition from the FRI into newly-segmented forest stands Deliver open source code and methods 3.3 Description A key requirement of sustainable forest management is the establishment and maintenance of forest inventories to provide accurate and timely information on the state of the forest to support a variety of purposes and information needs. In Ontario, where extensive forest management practices dominate, forest inventories are derived from a multi-stage process that involves acquiring aerial photography, using the photos to delineate homogenous units or forest stands, and then interpreting attributes from the photography for those delineated stands, often with the aid of stereo vision. Due to the capability of ALS to provide a detailed three-dimensional view of forest structure, the technology and related analysis approaches have transformed the derivation of forest inventories. Attributes such as volume, biomass, diameter, and basal area as well as information on canopy cover and its vertical distribution have all been shown to meet or exceed accuracy requirements in operational forest management with relative root mean square errors (%RMSE) typically lower than 20%. A key issue in utilising ALS data to derive forest inventory information is that a number of forest attributes required to provide the full complement of variables necessary for strategic-level forest inventories such as species group, composition, stand age, and site productivity, are not well predicted from ALS data alone, due to the lack of spectral and other information in the returned point cloud. A second issue is that the development of Enhanced Forest Inventories (EFI) using the area-based approaches (ABA) results in raster-based predictions of forest attributes. While these raters are produced at a fine grid cell (typically 20 x 20 m), most forest management decision making processes, such as harvest planning and scheduling, growth and yield estimation and longer term estate planning relies on polygonal data structures, such as those manually derived from photo interpreters. As a result, two gaps are evident when introducing ALS-based inventories into a forest management framework for strategic and tactical decision making. First, raster based EFI information needs to be transformed into polygonal structures and second, new approaches are needed to derive the attributes required for sustainable forest management decision making that are not readily predicted from ALS data alone. The goal of this project is to develop and apply an open source methodological framework that combines raster ALS/EFI predictions of stand structure with environmental and satellite data to produce comprehensive polygonal forest inventory information in a spatially exhaustive way, over large forested areas of Ontario. These new layers will be polygon based, with the full set of required attributes, at a scale comparable to existing photo-interpreted inventory, to ensure they are compatible with subsequent forest growth and yield, harvest and estate planning software and activities. Once new forest stand polygons are generated, conventional FRI attributes such as species and age are carried forward and integrated using an imputation algorithm. 3.4 Study Area We propose two forest management units as focus sites for this research. The Romeo Malette Forest, which is an actively managed forest by GreenFirst Forest Products and is a typical example of a boreal forest management unit in this region. Forest harvesting practices result in an array of stand development stages which are associated with a range of vegetation structures. A second focus site will be French-Severn Forest. This area has a distinct and complementary ecology, resulting in a range of species and structures not evident in the northern boreal. 3.5 Methodology First, we employ image segmentation (geographic object-based image analysis) to generate new up-to-date forest stands using key ALS-derived forest attributes of forest height, canopy cover, and coefficient of variation. Second, we ask what is the optimal selection of primary ALS and modelled EFI attributes to drive the imputation of photo-interpreted forest composition attributes, and what is the impact of imputation parameters on the correspondence between imputed and interpreted forest age and species composition. After establishing an optimal imputation model, we then undertake imputation to examine how the distribution of key forest attributes changes from the conventional FRI to an integrated inventory containing both ALS and imputed photo-interpreted attributes. Methodological details along with code demonstrations and results are contained in the appropriate sections below. This work is guided by the need to present the forest management community with an open-source, reproducible workflow to update forest inventories, while maintaining the forest stand as the baseline unit and integrating ALS data with important attributes still contained in conventional photo-interpreted forest inventories. 3.6 Knowledge &amp; Technology Transfer This project is designed to support research, development and technology transfer in the use of transformation technologies such as innovative remote sensing and environmental datasets such as ALS for forest management across Canada’s forest sector. Specifically this project is designed to support the Provincial government of Ontario to improve the accuracy of the forest inventory through the innovative use of ALS data, as well as exploiting the available information on species and age in the current FRI. By undertaking these activities we contribute to the ongoing transformation of the forest sector through the development and adoption of innovative science-based solutions in particular by linking to the new Ontario government’s Forest Sector Strategy. As the application of ALS and other 3-dimensional data to forest inventories becomes more common, the need for joint projects with industry is required to communicate the successes and caveats of these technologies and their appropriate application across Canada. The ultimate outcomes of this project will be a workforce that is more informed on the use of remote sensing technologies for next generation forest inventory applications. We will work with Ontario FRI staff and GreenFirst Forest Products managers on a number of key methods of outcome dissemination. These will include: Peer-reviewed scientific publications A series of online workshops A technical report describing the methods (this document) Open access to R and other developed programming scripts to undertake the modelling and comparisons. More broadly key outputs of this project will be an increased awareness and capacity building of operational forest managers on the use of these technologies for next generation forest inventory, which will lead to an increased ability to dissolve current boundaries between operational (tactical) and strategic (forest level) inventories. In addition, more thorough adoption of ALS technologies as a critical component of forest inventories will benefit other Canadian industries such as survey providers and technology developers by increasing the market and consolidating their international competitiveness, key goals of the Ontario Forest Sector strategy. "],["segmentation.html", "4 Segmentation 4.1 Introduction 4.2 Generic Region Merging Parameters 4.3 Data Requirements 4.4 Set Code and File Parameters 4.5 Pre-processing 4.6 Execute GRM Segmentation 4.7 Post-processing 4.8 Performance Analysis 4.9 Results: GRM Algorithm 4.10 Results: Summary Statisics 4.11 Results: Visual examples of GRM and FRI Polygons 4.12 Results: Distribution Plots 4.13 Caveats 4.14 Future Work 4.15 Summary", " 4 Segmentation 4.1 Introduction Forest stand polygons are familiar to forest managers, used by many forest planning and valuating workflows and software, and thus are preferable for management and monitoring. As the EFI becomes more prevalent, there is an immediate need to create a systematic workflow to convert raster-based metrics into a familiar polygonal format with the goal of ensuring usability and a seamless transition from the traditional forest inventory to the EFI. Although it is straight-forward to average EFI attributes over existing inventory polygons, these polygons are out of date (Bilyk et al., 2021) and do not represent the current conditions of dynamic forested environments. Manual delineation of forest stands is challenging since there are a lack of expert photo interpreters, the process is slow and expensive, and the subjective nature of the delineation makes monitoring of resources through time difficult (Wulder et al., 2008). Thus, to improve forest inventories our first step is to automatically delineate forest stand polygons from ALS data representing current conditions. Segmenting grid data (such as metrics derived from ALS) into meaningful polygonal objects is a well-documented process of geographic object-based image analysis. Generic Region Merging (GRM), available from Orfeo Toolbox, is a region merging algorithm which has three options for homogeneity criterion: Baatz &amp; Schape, Eclidean Distance, and Full Lambda. The Baatz &amp; Schape criterion (Baatz and Schäpe, 2000), is the same algorithm used by the popular subscription based Multi-resolution Segmentation (eCognition software), and thus GRM provides an open-source framework to access the most popular segmentation method for remote sensing applications (Blaschke, 2010). How do we know if the segmentation is “good”? There are no universal criteria to assess the ability of segmentation algorithms to accurately delineate features in data. Judging performance depends on the application, and may include factors such as radiometric variation within segments, contrast to other segments, segment shape, size and distribution, number of segments, and other application specific metrics such as number of landcover classes within each segment, or classification accuracy. In terms of applying segmentation to ALS data in order to generate representative forest stand polygons, we assess the segmentation results in terms of ease, efficiency, and replicability of the algorithm (thus choosing GRM), segment shape, number of segments, line work (not having multiple sets of parallel lines), and effectiveness at explaining input metric variation. Results are discussed below. 4.2 Generic Region Merging Parameters Output polygon specifications depend largely on parameter selection. For the GRM algorithm, the parameters include threshold, weight of spectral homogeneity, and weight of spatial homogeneity. Although the algorithm is based on the same homogeneity criterion as Multi-resolution Segmentation (eCognition software), the parameter selection is executed differently. Segments will merge if the value of the criterion is under the threshold, which controls spectral variation and segment size. The weight of spectral heterogeneity is a value between 0.1 – 0.9 and sets the priority of color vs. shape. This value inherently sets the weight of shape, since the weight of spectral homogeneity (color) and the weight of shape sum to 1. The weight of spatial homogeneity is a value between 0.1 – 0.9 that controls the priority of compactness vs. smoothness. The value inherently sets the weight of smoothness, since the weight of spatial homogeneity (compactness) and the weight of smoothness sum to 1. eCognition has a great video that explains parameter selection for their Multi-resolution Segmentation algorithm. The concepts apply directly to the GRM algorithm, just remember the parameters you adjust in GRM are slightly different. 4.3 Data Requirements The GRM segmentation workflow requires the following data layers, which are input into the algorithm at the beginning of the code: Gridded raster layers of the following ALS metrics, which are used to segment the data into polygons: p95: 95th percentile of returns height &gt; 1.3 meters classified as vegetation cc: Canopy cover. Percentage of first returns &gt; 2 meters classified as vegetation cv: Coefficient of variation of returns height &gt; 1.3 meters classified as vegetation We use these layers as they represent the height, cover, and vertical complexity of the forest. We do not go into detail about how to pre-process ALS layers from the point cloud. These metrics were processed using the lidR package in R. One thing to note is that we are using ALS metrics derived directly from the point cloud, as opposed to EFI attributes modelled using an area-based approach. EFI attributes are only valid in forested areas, and we want to generate polygons for the entire landscape, later classifying polygons as forested or not. Forest Resources Inventory polygons (shapefile) The polygons need to have a “polytype” attribute field, as the “WAT” value is used to mask water A roads layer (shapefile) This layer is used to mask roads and re-add them after segmentation. It may not be necessary to mask small roads that can be included in segmentation. VLCE 2.0 Canada-wide Landcover data. Download here. We use 2018 to match the year of aquisition of the ALS data. A valid Orfeo Toolbox Installation Orfeo Toolbox is run from the command line but we execute the GRM function through R. All analyses are run in R using RStudio – so having a valid installation is necessary too. 4.4 Set Code and File Parameters Now working in R, we will showcase a demo workflow, segmenting the Romeo Malette Forest (RMF) into forest stand polygons. The first step is to install packages (if not already installed) and set the code and file parameters. The input file locations are referenced and the GRM parameters are set. After conducting a performance analysis with many algorithm iterations, we are setting threshold (10), spectral homogeneity (0.1) and spatial homogeneity (0.5). ################################## ### INSTALL PACKAGES IF NEEDED ### ################################## # install.packages(c(&#39;terra&#39;, # &#39;tidyverse&#39;, # &#39;exactextractr&#39;, # &#39;sf&#39;, # &#39;janitor&#39;, # &#39;berryFunctions&#39;, # &#39;lwgeom&#39;, # &#39;magrittr&#39;, # &#39;gridExtra&#39;)) # make sure to have OTB installed from here: # https://www.orfeo-toolbox.org/ ##################### ### LOAD PACKAGES ### ##################### # load packages library(terra) library(tidyverse) library(exactextractr) library(sf) library(janitor) library(berryFunctions) library(lwgeom) library(magrittr) library(gridExtra) #################################### ### SET CODE AND FILE PARAMETERS ### #################################### # set file names for ALS input variables # these should be gridded raster data with the same CRS and extent # p95 = 95th percentile of returns height &gt; 1.3 m # cc = canopy cover (% of firest returns &gt; 2 m) # cv = coefficient of variation of returns height &gt; 1.3 m p95_f &lt;- &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_p95.tif&#39; cc_f &lt;- &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_2m_cov.tif&#39; cv_f &lt;- &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_cv.tif&#39; # set file location of roads shape file (spatial lines) # roads will be polygonized, masked from segmentation # and re-added to final dataset as polygons roads_f &lt;- &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Roads/RMF_roads.shp&#39; # set file location of FRI polygons shape file # FRI POLYTYPE should have a &quot;WAT&quot; classification to mask water polygons fri &lt;- &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Polygons Inventory/RMF_PolygonForest.shp&#39; # set output folder for files generated # make sure no &quot;/&quot; at end of folder location! out_dir &lt;- &#39;C:/Users/bermane/Desktop/RMF&#39; # set folder location of OTB (where you installed OTB earlier) otb_dir &lt;- &quot;C:/OTB/bin&quot; # set GRM segmentation parameters # the default are listed below # refer to paper or OTB GRM webpage for description of parameters thresh &lt;- &quot;10&quot; spec &lt;- &quot;0.1&quot; spat &lt;- &quot;0.5&quot; # set file location of 2018 VLCE 2.0 landcover data # using 2018 because it is the year of Romeo ALS acquisition # can change based on ALS acquisition year # download here: # https://opendata.nfis.org/mapserver/nfis-change_eng.html lc_f &lt;- &#39;D:/ontario_inventory/VLCE/CA_forest_VLCE2_2018.tif&#39; 4.5 Pre-processing Before executing the GRM algorithm, we prepare the data. First, a three band raster at 20 m spatial resolution is generated from the gridded ALS metrics. Each band is smoothed using the mean value of a 5x5 neighborhood. Next, we mask roads and water features from the raster, and combine them into a polygon layer to be re-added to the segmentation output. We then remove pixels that have missing values in any of the raster bands, and scale the bands to comprise values from 0-100, setting the minimum and maximum values to be the 1st and 99th percentile of data observations. We write the raster and masked polygons to disk to be loaded later. ################################################### ### LOAD MULTI BAND ALS RASTER FOR SEGMENTATION ### ################################################### # stack rasters spl &lt;- rast(c(p95_f, cc_f, cv_f)) # apply smoothing function on 5 cell square spl[[1]] &lt;- focal(spl[[1]], w = 5, fun = &quot;mean&quot;) spl[[2]] &lt;- focal(spl[[2]], w = 5, fun = &quot;mean&quot;) spl[[3]] &lt;- focal(spl[[3]], w = 5, fun = &quot;mean&quot;) # create ALS template with all values equal to 1 spl_temp &lt;- spl[[1]] spl_temp[] &lt;- 1 ################## ### MASK ROADS ### ################## # load roads layer roads &lt;- vect(roads_f) # reproject to match lidar roads &lt;- project(roads, spl) # create roads polygon spl_r &lt;- mask(spl_temp, roads, touches = T) npix &lt;- sum(values(spl_r), na.rm = T) spl_r &lt;- as.polygons(spl_r) names(spl_r) &lt;- &#39;POLYTYPE&#39; spl_r$POLYTYPE &lt;- &#39;RDS&#39; spl_r$nbPixels &lt;- npix # mask road pixels to NA spl &lt;- spl %&gt;% mask(., roads, inverse = T, touches = T) ########################### ### MASK WATER POLYGONS ### ########################### # water polygons from the FRI are masked and re-added after segmentation # load photo interpreted polygons poly &lt;- vect(fri) # subset polygons that are WAT poly_sub &lt;- poly[poly$POLYTYPE %in% c(&#39;WAT&#39;)] # reproject to match lidar poly_sub &lt;- project(poly_sub, spl) # loop through water polygons, mask raster, and vectorize for (i in 1:length(poly_sub)) { pt &lt;- poly_sub$POLYTYPE[i] if (i == 1) { spl_pt &lt;- spl_temp %&gt;% crop(poly_sub[i], snap = &#39;out&#39;) %&gt;% mask(poly_sub[i], touches = T) npix &lt;- sum(values(spl_pt), na.rm = T) spl_pt &lt;- as.polygons(spl_pt) names(spl_pt) &lt;- &#39;POLYTYPE&#39; spl_pt$POLYTYPE &lt;- pt spl_pt$nbPixels &lt;- npix } else{ if (is.error(spl_temp %&gt;% crop(poly_sub[i], snap = &#39;out&#39;) %&gt;% mask(poly_sub[i], touches = T)) == F) { spl_hold &lt;- spl_temp %&gt;% crop(poly_sub[i], snap = &#39;out&#39;) %&gt;% mask(poly_sub[i], touches = T) npix &lt;- sum(values(spl_hold), na.rm = T) spl_hold &lt;- as.polygons(spl_hold) names(spl_hold) &lt;- &#39;POLYTYPE&#39; spl_hold$POLYTYPE &lt;- pt spl_hold$nbPixels &lt;- npix spl_pt &lt;- rbind(spl_pt, spl_hold) } } } # reproject whole FRI to match lidar poly &lt;- project(poly, spl) # mask lidar outside of FRI spl &lt;- mask(spl, poly, inverse = F, touches = T) # mask WAT polygons spl &lt;- mask(spl, poly_sub, inverse = T, touches = T) ############################################### ### COMBINE ROAD AND WATER POLYGON DATASETS ### ############################################### spl_pt &lt;- rbind(spl_pt, spl_r) ########################################## ### DEAL WITH MISSING DATA AND RESCALE ### ########################################## # if any band is missing values set all to NA spl[is.na(spl[[1]])] &lt;- NA spl[is.na(spl[[2]])] &lt;- NA spl[is.na(spl[[3]])] &lt;- NA # create function to rescale values from 0 to 100 using 1 and 99 percentile scale_100 &lt;- function(x) { # calculate 1st and 99th percentile of input raster perc &lt;- values(x, mat = F) %&gt;% quantile(., probs = c(0.01, 0.99), na.rm = T) # rescale raster using 1st and 99th % x &lt;- (x - perc[1]) / (perc[2] - perc[1]) * 100 #reset values below 0 and above 100 x[x &lt; 0] &lt;- 0 x[x &gt; 100] &lt;- 100 return(x) } # rescale rasters from 0 to 100 spl[[1]] &lt;- scale_100(spl[[1]]) spl[[2]] &lt;- scale_100(spl[[2]]) spl[[3]] &lt;- scale_100(spl[[3]]) # check if main dir exists and create if (dir.exists(out_dir) == F) { dir.create(out_dir) } # check if temp dir exists and create if (dir.exists(file.path(out_dir, &#39;temp&#39;)) == F) { dir.create(file.path(out_dir, &#39;temp&#39;)) } # write raster to tif writeRaster(spl, filename = str_c(out_dir, &#39;/temp/spl_stack.tif&#39;), overwrite = T) # write spl_pt writeVector(spl_pt, str_c(out_dir, &#39;/temp/water_roads_polygons.shp&#39;), overwrite = T) 4.6 Execute GRM Segmentation The next step is to execute the GRM algorithm itself. We set the parameters, write a function to run GRM in the command line, and execute the function. The result is the segmented polygon layer, which then needs some post-processing before looking at performance results. ####################### ###RUN GRM ALGORITHM### ####################### # SET PARAMETERS rast_in &lt;- str_c(out_dir, &#39;/temp/spl_stack.tif&#39;) out_p &lt;- str_c(out_dir, &#39;/temp&#39;) name_out &lt;- str_c( &#39;grm_&#39;, thresh, &#39;_&#39;, gsub(&quot;.&quot;, &quot;&quot;, spec, fixed = TRUE), &#39;_&#39;, gsub(&quot;.&quot;, &quot;&quot;, spat, fixed = TRUE) ) # create function to run generic region merging grm_otb &lt;- function(otb_path = &quot;&quot;, raster_in = &quot;&quot;, out_path = &quot;&quot;, name = &quot;&quot;, method = &quot;bs&quot;, thresh = &quot;&quot;, spec = &quot;0.5&quot;, spat = &quot;0.5&quot;) { # Set configuration conf &lt;- paste( &quot;-in&quot;, raster_in, &quot;-out&quot;, paste(out_path, &quot;/&quot;, name, &quot;.tif&quot;, sep = &quot;&quot;), &quot;-criterion&quot;, method, &quot;-threshold&quot;, thresh, &quot;-cw&quot;, spec, &quot;-sw&quot;, spat ) # apply function in command line system(paste(otb_path, &quot;/otbcli_GenericRegionMerging&quot;, &quot; &quot;, conf, sep = &quot;&quot;)) # save configuration for further use write.table( x = conf, file = paste(out_path, &quot;/&quot;, name, &quot;_conf.txt&quot;, sep = &quot;&quot;), row.names = F, col.names = F ) } # run grm grm_otb( otb_path = otb_dir, raster_in = rast_in, out_path = out_p, name = name_out, thresh = thresh, spec = spec, spat = spat ) 4.7 Post-processing Before examining results, we mask missing values in the segmentation output, re-add water and road polygons (that were masked from input raster), and add landcover values. We use landcover to calculate two attributes: The dominant landcover type in each polygon (mode) Whether each polygon is considered forested (&gt; 50% landcover is forest). ########################### ### MASK MISSING VALUES ### ########################### # load grm raster p &lt;- rast(str_c(out_p, &quot;/&quot;, name_out, &quot;.tif&quot;)) # load seg raster mask &lt;- rast(rast_in) %&gt;% .[[1]] # mask grm raster p &lt;- mask(p, mask) # write grm raster writeRaster(p, paste(out_p, &quot;/&quot;, name_out, &quot;.tif&quot;, sep = &quot;&quot;), overwrite = T) # convert to vector based on cell value vec &lt;- as.polygons(p) # create table of number of pixels in each polygon num &lt;- as.vector(values(p)) num_pix &lt;- tabyl(num) # drop na row num_pix &lt;- na.omit(num_pix) # get pixel ids from vector vec_dat &lt;- tibble(id = values(vec)[, 1]) colnames(vec_dat) &lt;- &#39;id&#39; # loop through values and add to vector data vec_dat$nbPixels &lt;- NA for (i in 1:NROW(vec_dat)) { vec_dat$nbPixels[i] &lt;- num_pix$n[num_pix$num == vec_dat$id[i]] } # remove current column of data and add id # add nbPixels to vector vec &lt;- vec[, -1] vec$id &lt;- vec_dat$id vec$nbPixels &lt;- vec_dat$nbPixels ################################## ### ADD PRE-ALLOCATED POLYGONS ### ################################## # load polygon dataset p &lt;- vec # reproject segmented polygons to ensure same crs p &lt;- project(p, spl_pt) # add non-FOR POLYTYPE polygons back in p2 &lt;- rbind(p, spl_pt) ##################### ### ADD LANDCOVER ### ##################### # load VLCE 2.0 landcover dataset lc &lt;- rast(lc_f) # project polygons to CRS of raster p_lc &lt;- project(p2, lc) # crop raster lc &lt;- crop(lc, p_lc) # convert to sf p_lcsf &lt;- st_as_sf(p_lc) # extract landcover values lc_vals &lt;- exact_extract(lc, p_lcsf) # set landcover class key lc_key &lt;- c(`0` = &#39;NA&#39;, `20` = &#39;Water&#39;, `31` = &#39;Snow/Ice&#39;, `32` = &#39;Rock/Rubble&#39;, `33` = &#39;Exposed/Barren Land&#39;, `40` = &#39;Bryoids&#39;, `50` = &#39;Shrubland&#39;, `80` = &#39;Wetland&#39;, `81` = &#39;Wetland-Treed&#39;, `100` = &#39;Herbs&#39;, `210` = &#39;Coniferous&#39;, `220` = &#39;Broadleaf&#39;, `230` = &#39;Mixed Wood&#39;) # find dominant lc type in each polygon # if there are multiple modes keep them # apply over list lc_mode &lt;- sapply(lc_vals, function(x){ x$value &lt;- recode(x$value, !!!lc_key) x &lt;- x %&gt;% group_by(value) %&gt;% summarize(sum = sum(coverage_fraction)) m &lt;- x$value[which(x$sum == max(x$sum))] # m &lt;- get_mode2(x$value[x$coverage_fraction &gt;= cov_frac]) return(paste(m, collapse = &quot; &quot;)) }) # add to polygon dataset p2$dom_lc &lt;- lc_mode # set landcover class key with single forested class lc_key_for &lt;- c(`0` = &#39;NA&#39;, `20` = &#39;Water&#39;, `31` = &#39;Snow/Ice&#39;, `32` = &#39;Rock/Rubble&#39;, `33` = &#39;Exposed/Barren Land&#39;, `40` = &#39;Bryoids&#39;, `50` = &#39;Shrubland&#39;, `80` = &#39;Wetland&#39;, `81` = &#39;Forest&#39;, `100` = &#39;Herbs&#39;, `210` = &#39;Forest&#39;, `220` = &#39;Forest&#39;, `230` = &#39;Forest&#39;) # find pixels with forest at least 50% of pixel # apply over list lc_dom_for &lt;- sapply(lc_vals, function(x){ x$value &lt;- recode(x$value, !!!lc_key_for) x &lt;- x %&gt;% group_by(value) %&gt;% summarize(sum = sum(coverage_fraction)) m &lt;- x$value[which(x$sum == max(x$sum))] if((length(m) == 1) &amp; (m == &#39;Forest&#39;)[1]){ if(x$sum[x$value == m]/sum(x$sum) &gt;= 0.5){ return(&#39;Yes&#39;) }else{return(&#39;No&#39;)} }else{return(&#39;No&#39;)} }) # add to polygon dataset p2$dom_for &lt;- lc_dom_for 4.8 Performance Analysis To compare newly segmented forest stand polygons to those from the FRI, we calculate a full suite of summary and performance metrics including: Minimum, maximum, mean, and median polygon size Total number of polygons Mean, standard error, and standard deviation of polygon area, perimeter, perimeter to area ratio and shape index R2 to assess the performance of segmentation outputs in explaining variation in the input variables: P95 height, canopy cover, and coefficient of variation The details pertaining to the derivation of these metrics are included in a forthcoming journal article. The following code chunk calculates stats for both the GRM output and the FRI polygons, outputting them together in a file titled “summary_stats.csv”. ############################## ### ADD AREA AND PERIMETER ### ############################## # convert to sf p2_sf &lt;- st_as_sf(p2) # calculate perimeter p2$perim &lt;- st_perimeter(p2_sf) %&gt;% as.numeric # calculate area p2$area &lt;- st_area(p2_sf) %&gt;% as.numeric # write to file writeVector(p2, str_c(out_dir, &quot;/&quot;, name_out, &quot;.shp&quot;), overwrite = T) ########################################### ### EXTRACT FINAL POLYGON SUMMARY STATS ### ########################################### # create list of polygon files, names and parameters file &lt;- str_c(out_dir, &quot;/&quot;, name_out, &quot;.shp&quot;) out_loc &lt;- out_dir grm_input &lt;- str_c(out_dir, &#39;/temp/spl_stack.tif&#39;) name &lt;- name_out # create standard error function se &lt;- function(x) sd(x) / sqrt(length(x)) # load file p &lt;- vect(file) # convert to sf p_sf &lt;- st_as_sf(p) # subset non masked WAT and RD polygons p2_sf &lt;- p[is.na(p$POLYTYPE)] %&gt;% st_as_sf p2 &lt;- p[is.na(p$POLYTYPE)] %&gt;% as.data.frame # calculate perimeter to area ratio p2$p_to_a &lt;- p2$perim / p2$area p2$p_to_a &lt;- round(p2$p_to_a, 3) # calculate msi p2$msi &lt;- p2$perim / sqrt(pi * p2$area) # load original raster input file ras &lt;- rast(grm_input) # rename bands names(ras) &lt;- c(&#39;p95&#39;, &#39;cc&#39;, &#39;cv&#39;) # extract pixel values pvals &lt;- exact_extract(ras, p2_sf) # calculate SSE sse &lt;- sapply( pvals, FUN = function(x) { p95_mean &lt;- mean(x$p95, na.rm = T) cc_mean &lt;- mean(x$cc, na.rm = T) cv_mean &lt;- mean(x$cv, na.rm = T) return(c(sum((x$p95 - p95_mean) ^ 2, na.rm = T), sum((x$cc - cc_mean) ^ 2, na.rm = T), sum((x$cv - cv_mean) ^ 2, na.rm = T))) } ) # transpose sse &lt;- t(sse) # calculate final sums sse &lt;- colSums(sse) # unlist values pvals2 &lt;- do.call(rbind, pvals) # calculate global mean values p95_mean &lt;- mean(pvals2$p95, na.rm = T) cc_mean &lt;- mean(pvals2$cc, na.rm = T) cv_mean &lt;- mean(pvals2$cv, na.rm = T) rm(pvals2) # calculate SST sst &lt;- sapply( pvals, FUN = function(x) { return(c(sum((x$p95 - p95_mean) ^ 2, na.rm = T), sum((x$cc - cc_mean) ^ 2, na.rm = T), sum((x$cv - cv_mean) ^ 2, na.rm = T))) } ) # transpose sst &lt;- t(sst) # calculate final sums sst &lt;- colSums(sst) # calculate r2 values r2_p95 &lt;- 1 - (sse[1] / sst[1]) %&gt;% round(3) r2_cc &lt;- 1 - (sse[2] / sst[2]) %&gt;% round(3) r2_cv &lt;- 1 - (sse[3] / sst[3]) %&gt;% round(3) r2_all &lt;- (sum(r2_p95, r2_cc, r2_cv) / 3) %&gt;% round(3) # create dataframe with values wanted df &lt;- data.frame( alg = name, min_pix = (min(p2$nbPixels)), max_pix = (max(p2$nbPixels)), mean_pix = (mean(p2$nbPixels)), med_pix = (median(p2$nbPixels)), num_poly = NROW(p2), mean_area = mean(p2$area), se_area = se(p2$area), sd_area = sd(p2$area), mean_perim = mean(p2$perim), se_perim = se(p2$perim), sd_perim = sd(p2$perim), mean_p_a = mean(p2$p_to_a), se_p_a = se(p2$p_to_a), sd_p_a = sd(p2$p_to_a), mean_msi = mean(p2$msi), se_msi = se(p2$msi), sd_msi = sd(p2$msi), r2_p95 = r2_p95, r2_cc = r2_cc, r2_cv = r2_cv, r2_all = r2_all ) # round numeric columns df %&lt;&gt;% mutate_at(c( &#39;min_pix&#39;, &#39;max_pix&#39;, &#39;mean_pix&#39;, &#39;med_pix&#39;, &#39;mean_area&#39;, &#39;se_area&#39;, &#39;sd_area&#39;, &#39;mean_perim&#39;, &#39;se_perim&#39;, &#39;sd_perim&#39; ), function(x) round(x, 2)) %&gt;% mutate_at(c(&#39;mean_p_a&#39;, &#39;se_p_a&#39;, &#39;sd_p_a&#39;, &#39;mean_msi&#39;, &#39;se_msi&#39;, &#39;sd_msi&#39;), function(x) round(x, 4)) ##################### ### ADD FRI STATS ### ##################### # load interpreter derived polygons to extract statistics pfri &lt;- vect(fri) # convert to sf pfri_sf &lt;- st_as_sf(pfri) # calculate perimeter pfri$perim &lt;- st_perimeter(pfri_sf) %&gt;% as.numeric # calculate area pfri$area &lt;- st_area(pfri_sf) %&gt;% as.numeric # calculate nbPixels pfri$nbPixels &lt;- pfri$area / 400 # calculate perimeter to area ratio pfri$p_to_a &lt;- pfri$perim / pfri$area pfri$p_to_a &lt;- round(pfri$p_to_a, 3) # subset all non water/ucl polygons p2fri_sf &lt;- pfri[!(pfri$POLYTYPE %in% c(&#39;WAT&#39;, &#39;UCL&#39;))] %&gt;% st_as_sf p2fri &lt;- pfri[!(pfri$POLYTYPE %in% c(&#39;WAT&#39;, &#39;UCL&#39;))] %&gt;% as.data.frame # calculate msi p2fri$msi &lt;- p2fri$perim / sqrt(pi * p2fri$area) # load original raster input file ras &lt;- rast(grm_input) # rename bands names(ras) &lt;- c(&#39;p95&#39;, &#39;cc&#39;, &#39;cv&#39;) # extract pixel values pvals &lt;- exact_extract(ras, p2fri_sf) # calculate SSE sse &lt;- sapply( pvals, FUN = function(x) { # subset values based on coverage fraction x %&lt;&gt;% filter(coverage_fraction &gt;= 0.5) p95_mean &lt;- mean(x$p95, na.rm = T) cc_mean &lt;- mean(x$cc, na.rm = T) cv_mean &lt;- mean(x$cv, na.rm = T) return(c(sum((x$p95 - p95_mean) ^ 2, na.rm = T), sum((x$cc - cc_mean) ^ 2, na.rm = T), sum((x$cv - cv_mean) ^ 2, na.rm = T))) } ) # transpose sse &lt;- t(sse) # calculate final sums sse &lt;- colSums(sse) # unlist values pvals2 &lt;- do.call(rbind, pvals) # subset values based on coverage fraction pvals2 %&lt;&gt;% filter(coverage_fraction &gt;= 0.5) # calculate global mean values p95_mean &lt;- mean(pvals2$p95, na.rm = T) cc_mean &lt;- mean(pvals2$cc, na.rm = T) cv_mean &lt;- mean(pvals2$cv, na.rm = T) rm(pvals2) # calculate SST sst &lt;- sapply( pvals, FUN = function(x) { # subset values based on coverage fraction x %&lt;&gt;% filter(coverage_fraction &gt;= 0.5) return(c(sum((x$p95 - p95_mean) ^ 2, na.rm = T), sum((x$cc - cc_mean) ^ 2, na.rm = T), sum((x$cv - cv_mean) ^ 2, na.rm = T))) } ) # transpose sst &lt;- t(sst) # calculate final sums sst &lt;- colSums(sst) # calculate r2 values r2_p95 &lt;- 1 - (sse[1] / sst[1]) %&gt;% round(3) r2_cc &lt;- 1 - (sse[2] / sst[2]) %&gt;% round(3) r2_cv &lt;- 1 - (sse[3] / sst[3]) %&gt;% round(3) r2_all &lt;- (sum(r2_p95, r2_cc, r2_cv) / 3) %&gt;% round(3) # create dataframe with values wanted ms_df &lt;- data.frame( alg = &#39;FRI&#39;, min_pix = (min(p2fri$area / 400)), max_pix = (max(p2fri$area / 400)), mean_pix = (mean(p2fri$area / 400)), med_pix = (median(p2fri$area / 400)), num_poly = NROW(p2fri), mean_area = mean(p2fri$area), se_area = se(p2fri$area), sd_area = sd(p2fri$area), mean_perim = mean(p2fri$perim), se_perim = se(p2fri$perim), sd_perim = sd(p2fri$perim), mean_p_a = mean(p2fri$p_to_a), se_p_a = se(p2fri$p_to_a), sd_p_a = sd(p2fri$p_to_a), mean_msi = mean(p2fri$msi), se_msi = se(p2fri$msi), sd_msi = sd(p2fri$msi), r2_p95 = r2_p95, r2_cc = r2_cc, r2_cv = r2_cv, r2_all = r2_all ) # round numeric columns ms_df %&lt;&gt;% mutate_at(c(&#39;min_pix&#39;, &#39;max_pix&#39;, &#39;mean_pix&#39;, &#39;med_pix&#39;), function(x) round(x)) %&gt;% mutate_at(c( &#39;mean_area&#39;, &#39;se_area&#39;, &#39;sd_area&#39;, &#39;mean_perim&#39;, &#39;se_perim&#39;, &#39;sd_perim&#39; ), function(x) round(x, 2)) %&gt;% mutate_at(c(&#39;mean_p_a&#39;, &#39;se_p_a&#39;, &#39;sd_p_a&#39;, &#39;mean_msi&#39;, &#39;se_msi&#39;, &#39;sd_msi&#39;), function(x) round(x, 4)) # bind df df &lt;- rbind(df, ms_df) # write df as csv write.csv(df, file = str_c(out_loc, &#39;/summary_stats.csv&#39;), row.names = F) 4.9 Results: GRM Algorithm We tested many iterations of the GRM algorithm, manipulating parameters and assessing outputs. GRM consistently resulted in high R2 values across ALS metrics. We have showcased here the optimal output in terms of visual polygon shape and size as well as R2 values. The parameters used were a threshold of 10, spatial homogeneity of 0.1, and spectral homogeneity of 0.5. We found changes in the spectral homogeneity parameter to highly influence output polygon shape, and found the value of 0.1 (low spectral homogeneity weight) to result in a better visual output while still resulting in high R2 values. A spatial homogeneity value of 0.5 resulted in a good balance between compactness and smoothness and a threshold of 10 resulted in ~111,000 polygons total. The GRM output polygons are the most consistently compact, of uniform shape, and of a medium size. This output strikes the best balance between shape consistency, linework and distinguishing landcover. 4.10 Results: Summary Statisics First we will examine polygon size statistics of the GRM output compared to the FRI. ########################################### ### Summary Stats and Additional Tables ### ########################################### # Size t1 &lt;- as_tibble(df[, 1:6]) # change to ha t1[, 2:5] &lt;- round(t1[, 2:5] / 25, 2) names(t1) &lt;- c(&#39;Dataset&#39;, &quot;Min Ha&quot;, &#39;Max Ha&#39;, &quot;Mean Ha&quot;, &quot;Median Ha&quot;, &quot;Number of Polygons&quot;) knitr::kable(t1, caption = &quot;Table 1: Polygon Size Stats&quot;, label = NA) Table 1: Polygon Size Stats Dataset Min Ha Max Ha Mean Ha Median Ha Number of Polygons grm_10_01_05 0.04 43.56 4.78 3.96 111881 FRI 0.00 547.24 7.80 4.04 74662 Table 1 shows Polygon size stats. The median values are very close, but the mean GRM polygon size is smaller, as is the maximum polygon size. Thus, the GRM output has more polygons overall than the FRI. Next we will look at area and perimeter. # Area and Perim t2 &lt;- as_tibble(df[, c(1, 7:12)]) # change area to Ha t2[,2:4] &lt;- round(t2[,2:4] / 10000, 2) names(t2) &lt;- c(&quot;Dataset&quot;, &quot;Mean Area (Ha)&quot;, &quot;SE Area&quot;, &quot;SD Area&quot;, &quot;Mean Perimeter (m)&quot;, &quot;SE Perimeter&quot;, &quot;SD Perimeter&quot;) knitr::kable(t2, caption = &quot;Table 2: Polygon Area and Perimeter Stats&quot;, label = NA) Table 2: Polygon Area and Perimeter Stats Dataset Mean Area (Ha) SE Area SD Area Mean Perimeter (m) SE Perimeter SD Perimeter grm_10_01_05 4.78 0.01 3.29 1118.99 1.11 369.66 FRI 7.80 0.05 12.66 1723.23 6.37 1741.25 Table 2 shows area and perimeter stats. The mean area and perimeter of GRM polygons are smaller than the FRI. The variation is also smaller, since the GRM polygons are more uniform. Next we will look at shape index, which is an indication of the regularity of polygon shape. Values of 1 represent perfect circles, with values increasing without maximum as shape complexity increases. # Mean Shape Index t3 &lt;- as_tibble(df[, c(1, 16:18)]) names(t3) &lt;- c(&quot;Dataset&quot;, &quot;Mean Shape Index&quot;, &quot;SE Shape Index&quot;, &quot;SD Shape Index&quot;) knitr::kable(t3, caption = &quot;Table 3: Polygon Shape Index Stats&quot;, label = NA) Table 3: Polygon Shape Index Stats Dataset Mean Shape Index SE Shape Index SD Shape Index grm_10_01_05 3.0737 0.0014 0.4557 FRI 3.8667 0.0052 1.4111 We can see that GRM polygons are less complex (more compact) than FRI polygons and that shape varies less in the GRM output. The last table will show R2 values, which give an indication of how well the segmentation output is explaining the variation among ALS input variables. # R2 t4 &lt;- as_tibble(df[,c(1, 19:22)]) # round t4[,2:5] &lt;- round(t4[,2:5], 2) names(t4) &lt;- c(&#39;Dataset&#39;, &#39;R2 P95&#39;, &#39;R2 Can Cov&#39;, &#39;R2 Coeff Var&#39;, &#39;R2 Overall&#39;) knitr::kable(t4, caption = &quot;Table 4: Polygon R2 Stats&quot;, label = NA) Table 4: Polygon R2 Stats Dataset R2 P95 R2 Can Cov R2 Coeff Var R2 Overall grm_10_01_05 0.90 0.93 0.85 0.89 FRI 0.84 0.85 0.72 0.80 We can see that the GRM output has a high R2 value of 0.89 (overall R2), which is a strong indicator that the segmentation algorithm is effectively explaining structural differences between polygons. The manually segmented FRI polygons also have a very good R2 value, indicating the skill of interpreters in delineating forest structure. 4.11 Results: Visual examples of GRM and FRI Polygons The following code chunk loads a base image from part of the RMF to give an example of the forest stand polygon delineation. As the GRM algorithm outputs shapefiles, assessing the visual quality of the segmentation is best done in QGIS (or other GIS software). Note that we also don’t go into detail assessing the landcover attributes in this example. The landcover attributes are included in the output shapefile and can also be assessed in a GIS software. # load base imagery base &lt;- rast(&#39;D:/ontario_inventory/romeo/RMF_Sample_Base.tif&#39;) # load FRI and change to raster proj fri_poly &lt;- vect(fri) %&gt;% project(., base) %&gt;% crop(., base) # load GRM polygons and change to raster proj grm_poly &lt;- vect(file) %&gt;% project(., base) %&gt;% crop(., base) # plot base imagery with GRM polygons plot(grm_poly) plotRGB(base, stretch = &#39;lin&#39;, add = T) plot(grm_poly, border = &#39;red2&#39;, lwd = 2, add = T) title(main = &#39;GRM Polygons Overlaid on True Color Image&#39;, line = -2, cex.main = 3) The above plot shows the GRM derived polygons in part of the sample area. Note that the automated segmentation has clean edges around water and road features, since these features were masked in the first step, and the polygons re-added after segmentation. The GRM polygons are also much more compact and uniform than the FRI polygons, and generally do not have issues with the line-work (overlapping and parallel lines), which was an important point mentioned by project partners. # plot base imagery with FRI polygons plot(fri_poly) plotRGB(base, stretch = &#39;lin&#39;, add = T) plot(fri_poly, border = &#39;mediumvioletred&#39;, lwd = 2, add = T) title(main = &#39;FRI Polygons Overlaid on True Color Image&#39;, line = -2, cex.main = 3) The above plot shows the FRI polygons in part of the sample area, overlaid on recent true color imagery. Note the clean edges around water bodies, rivers/streams, and road features. 4.12 Results: Distribution Plots The last results we will show are plots of the distribution of various performance metrics. First will be density plots of polygon size. # plot density GRM in ha g1 &lt;- ggplot(data.frame(nbPixels = p2$nbPixels / 25), aes(x = nbPixels)) + geom_density(fill = &#39;grey&#39;) + xlim(c(0, 500/25)) + ylim(c(0, 0.20)) + geom_vline(aes(xintercept = median(nbPixels)), linetype = &quot;dashed&quot;, linewidth = 0.6) + theme_bw() + xlab(&#39;Number of Hectares&#39;) + ylab(&#39;Density&#39;) + ggtitle(&#39;GRM Distribution of Polygon Size&#39;) + theme(text = element_text(size = 25)) # plot density FRI in ha g2 &lt;- ggplot(data.frame(nbPixels = p2fri$nbPixels / 25), aes(x = nbPixels)) + geom_density(fill = &#39;grey&#39;) + xlim(c(0, 500/25)) + ylim(c(0, 0.20)) + geom_vline(aes(xintercept = median(nbPixels)), linetype = &quot;dashed&quot;, linewidth = 0.6) + theme_bw() + xlab(&#39;Number of Hectares&#39;) + ylab(&#39;Density&#39;) + ggtitle(&#39;FRI Distribution of Polygon Size&#39;) + theme(text = element_text(size = 25)) # plot together grid.arrange(g1, g2) Above is the distribution of polygon size within the sample area. The dotted line denotes the median values, which are very close between the FRI and GRM polygons. Both distributions have a long tail toward large polygons, but the GRM polygons have a more uniform size and less large polygons. Water and Unclassified polygons are not included. Lastly we plot the distribution of shape index. # plot shape index GRM g1 &lt;- ggplot(data.frame(msi = as.numeric(p2$msi)), aes(x = msi)) + geom_density(fill = &#39;grey&#39;) + xlim(c(0, 8)) + ylim(c(0, 1.5)) + geom_vline(aes(xintercept = median(msi)), linetype = &quot;dashed&quot;, linewidth = 0.6) + theme_bw() + xlab(&#39;Shape Index&#39;) + ylab(&#39;Density&#39;) + ggtitle(&#39;GRM Distribution of Shape Index&#39;) + theme(text = element_text(size = 25)) # plot shape index FRI g2 &lt;- ggplot(data.frame(msi = as.numeric(p2fri$msi)), aes(x = msi)) + geom_density(fill = &#39;grey&#39;) + xlim(c(0, 8)) + ylim(c(0, 1.5)) + geom_vline(aes(xintercept = median(msi)), linetype = &quot;dashed&quot;, linewidth = 0.6) + theme_bw() + xlab(&#39;Shape Index&#39;) + ylab(&#39;Density&#39;) + ggtitle(&#39;FRI Distribution of Shape Index&#39;) + theme(text = element_text(size = 25)) # plot together grid.arrange(g1, g2) The above plots show Shape Index for FRI and GRM polygons in the sample area (Water and Unclassified polygons are not included). The dotted line is the median value. Shape Index has a value of 1 for circular objects and increases without limits with increased complexity and irregularity. The GRM polygons have a much more compact shape (lower Shape Index) as well as a more uniform and tight distribution. These characteristics are visible in the above plots of the polygons. 4.13 Caveats This work showcases promising results for forest inventory users wishing to leverage new ALS data yet continue to work in a familiar polygonal framework, though there are still challenges and constraints. Automated segmentation outputs tend to feature more uniform segment shapes and sizes, that is to say the algorithms do not have to ability to consider a broad range of cues, whether conceptual or structural, and balance a range of needs regarding the utility of forest inventory polygons. The segments output from gridded ALS metrics do not always feature smooth transitions between segments, such as river banks, road edges, and forest/non-forest boundaries, due to the fact that the output segments, although in vector format, were delineated from raster data. This also creates complex segments and shapes in areas of spectral heterogeneity. Whereas an interpreter can conceptualize broader dynamics to delineate changes in landscape features, the segmentation algorithms are limited by applying the same region growing approach across the landscape, and cannot break apart or reconstruct segments to simplify the geometry throughout the process. The gridded ALS metrics we used were also not effective at delineating small and/or linear features, such as lakes, rivers, and roads. With input data at 20 m spatial resolution, and further smoothed using a 5x5 neighborhood, these features tend to get lost and therefore must be masked from the data before processing and readded as individual polygons after segmentation. Automated outputs cannot replace the expertise of forest interpreters but should rather be seen and used as complimentary (Wulder et al., 2008) or a substitution with a different set of inherent strengths and weaknesses. 4.14 Future Work There are several future pathways to further understand and utilize an automated segmentation approach in a forest inventory context. Although this work provides an example of an open-source segmentation approach available for forest inventory applications, there is a need to further explore the performance and utility of this approach across a range of ecosystems and diverse forested regions. In a large operational context, it may not be practical to manually conduct a sensitivity analysis and manipulate parameters to derive a satisfactory output, but rather build a system to optimize parameters automatically (Pukkala, 2020). Additionally, multi-spectral optical imagery has shown promise in distinguishing characteristics of forest stands and could be assessed for its utility as a guiding segmentation parameter in this methodology (Dechesne et al., 2017). 4.15 Summary We effectively applied segmentation over the Romeo Malette forest management unit (~630,000 ha) and described how algorithm parameters influence output polygon size, shape, and radiometric consistency, and can be manipulated to meet the needs of forest managers and other users of forest inventory data. In addition, we applied segmentation on ALS metrics that do not require substantial ground calibration and validation, which makes this approach feasible to use in difficult to reach regions that may or may not have been inventoried in the past, or in which EFIs have not yet been generated. The second step in this project is to impute age and species attributes from the FRI into newly-segmented polygons. This will help to bridge the gap between old and new forest inventories, while keeping forest attributes essential to forest management applications. "],["imputation.html", "5 Imputation 5.1 Introduction 5.2 Imputation Parameters 5.3 Data Requirements 5.4 Set Code and File Parameters 5.5 Pre-processing: Extract Variables into Polygons 5.6 Pre-processing: Calculate Species Attributes 5.7 Pre-processing: Forest Data Screening 5.8 Execute Imputation Algorithm Over FRI Polygons ONLY 5.9 Results: Imputation over FRI 5.10 Execute Imputation Algorithm From FRI to GRM Polygons 5.11 Results: Imputation X-Variable Performance 5.12 Results: Distribution of Age 5.13 Results: Distribution of Species 5.16 Caveats 5.17 Future Work 5.18 Summary", " 5 Imputation 5.1 Introduction After automatically delineating forest stands from ALS data, the second step of this project is to integrate the vast database of conventionally derived forest composition attributes with EFI structural attributes from ALS. EFIs contain forest attributes such as Lorey’s height, quadratic mean diameter at breast height, basal area, merchantable volume and above-ground biomass. Yet ALS is limited in its ability to accurately estimate additional important forest composition attributes such as species and forest stand age. Thus, although we can easily average ALS/EFI attributes using zonal statistics, we need a workflow to carry forward important interpreter-derived attributes from the conventional FRI. We achieve this through imputation. k-nearest neighbor (kNN) imputation is a widely used technique to derive spatially contiguous forest attributes for inventory and monitoring purposes (Chirici et al., 2016; McRoberts et al., 2010). kNN is applicable when desired forest attributes (Y-variables) exist in a subset of all observations (reference observations), and are imputed into observations that are missing Y-variable values (target observations). The algorithm works by minimizing a distance function between variables without missing values across the dataset (X-variables), finding k-reference observations closest to a single target observation, and imputing Y-variable values from k-reference observations into the target observation. In this case X-variables are derived from ALS and optical spaceborne remote sensing due to the contiguous nature of the data. This technique is distribution-free, multivariate, and non-parametric (Eskelson et al., 2009). It is based on the premise that observations with similar X-variable values should also have similar desired forest attribute values (Y-variable values). Thus, X-variable selection can be more important than imputation method (Brosofske et al., 2014) and therefore an iterative performance analysis is used to aid in optimal variable selection. Since all attributes are imputed together from k-nearest neighbors, kNN imputation maintains the relationship between interpreter derived attributes (Coops et al., 2021). In this example we will walk through the imputation of FRI age and species attributes into the forest stand polygons we generated using the GRM algorithm. As explained in a forthcoming journal article, we undertook an iterative performance analysis to select the optimal X-variables to use to drive the imputation algorithm. We will not test different combinations of X-variables in this example, rather just use the optimal selection found for the RMF study area, focusing on the process of imputing age and species attributes into new polygons and assessing the distribution of attributes in relation to the conventional FRI. 5.2 Imputation Parameters The kNN algorithm functions by minimizing the Euclidean distance, without weights, between n X-variables in a target observation (in this case a forested polygon) and k-reference observations. We tested combinations of n = 3/5/7 and use n = 7 in this example. In our testing, n = 7 performed slightly better than n = 5 and much better than n = 3. When k = 1, values from the nearest reference observation (X and Y-variables) are directly joined to the target observation. When k &gt; 1, values from the nearest reference observations are averaged using either the mean (for numeric variables) or mode (for categorical values). All Y-variables are imputed together, maintaining the relationship between variables in the reference observations. We tested combinations of k = 1/3/5 and use k = 5 in this example as it produced optimal results. In general, larger values of k lead to higher performance but more averaging of estimated attributes, which decreases variability. 5.3 Data Requirements The GRM segmentation workflow requires the following data layers, which are input into the algorithm at the beginning of the code: Gridded raster layers of the following ALS metrics and Sentinel-2 imagery, which are used as imputation X-variables as well as for polygon data screening: Imputation X-variables: avg: Mean returns height &gt; 1.3 m classified as vegetation rumple: Ratio of canopy outer surface area to ground surface area pcum8: Cumulative percentage of returns found in 80th percentile of returns height sd: Standard deviation of returns height &gt; 1.3 m classified as vegetation b6: Cloud-free composite of Sentinel-2 red-edge 2 (band 6; 740 nm) surface reflectance These five attributes comprise the optimal imputation X-variables from our performance analysis. The additional two variables used are the x and y coordinates of each polygon centroid, and are calculated automatically in the code. In our analysis, we found that these attributes derived directly from the ALS point cloud performed slightly better than modeled attributes from the EFI. Many of the ALS/EFI attributes are highly correlated and thus yield similar imputation results. Variables for polygon data screening: p95: 95th percentile of returns height &gt; 1.3 meters classified as vegetation cc: Canopy cover. Percentage of first returns &gt; 2 meters classified as vegetation We use these layers to screen FRI and GRM polygons to curate a set of polygons likely to be forested, since we do not want to impute attributes from/to polygons that are likely non-forested. We do not go into detail about how to pre-process ALS layers from the point cloud. These metrics were processed using the lidR package in R. Forest Resources Inventory polygons (shapefile) The polygons need to have a “SPCOMP”, “YRORG”, and “POLYID” field, which are used in the code below. Generic Region Merging segmented polygons (shapefile) The polygons generated in the segmentation step of this project. VLCE 2.0 Canada-wide Landcover data. Download here. We use 2018 to match the year of aquisition of the ALS data. All analyses are run in R using RStudio – so having a valid installation is necessary too. 5.4 Set Code and File Parameters Now working in R, we will showcase a demo workflow, imputing age and species composition attributes from the FRI into GRM polygons in the Romeo Malette Forest (RMF). Although imputation can be carried out over the entire suite of FRI attributes, we focus on age and species for the purpose of this analysis. The first step is to install packages (if not already installed) and set the code and file parameters. The input file locations are referenced. ################################## ### INSTALL PACKAGES IF NEEDED ### ################################## # install.packages(c(&#39;terra&#39;, # &#39;tidyverse&#39;, # &#39;exactextractr&#39;, # &#39;sf&#39;, # &#39;magrittr&#39;, # &#39;gridExtra&#39;, # &#39;RANN&#39;, # &#39;reshape2&#39;, # &#39;viridis&#39;, # &#39;scales&#39;, # &#39;janitor&#39;, # &#39;kableExtra&#39;)) # make sure to have OTB installed from here: # https://www.orfeo-toolbox.org/ ##################### ### LOAD PACKAGES ### ##################### # load packages library(terra) library(tidyverse) library(exactextractr) library(sf) library(magrittr) library(gridExtra) library(RANN) library(reshape2) library(viridis) library(scales) library(janitor) library(kableExtra) #################################### ### SET CODE AND FILE PARAMETERS ### #################################### # set file names for ALS input variables # first set the imputation X-variables lidar_imp &lt;- c(&#39;avg&#39; = &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_avg.tif&#39;, &#39;rumple&#39; = &#39;D:/ontario_inventory/romeo/SPL metrics/Z_METRICS_MOSAIC/individual/RMF_RUMPLE_MOSAIC_r_rumple.tif&#39;, &#39;pcum8&#39; = &#39;D:/ontario_inventory/romeo/SPL metrics/Z_METRICS_MOSAIC/individual/RMF_Z_METRICS_MOSAIC_zpcum8.tif&#39;, &#39;sd&#39; = &#39;D:/ontario_inventory/romeo/SPL metrics/Z_METRICS_MOSAIC/individual/RMF_Z_METRICS_MOSAIC_zsd.tif&#39;, &#39;b6&#39; = &#39;D:/ontario_inventory/romeo/Sentinel/red_edge_2.tif&#39;) # next set the data screening variables lidar_scr &lt;- c(&#39;p95&#39; = &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_p95.tif&#39;, &#39;cc&#39; = &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_2m_cov.tif&#39;) # set file location of FRI polygons shape file fri &lt;- &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Polygons Inventory/RMF_PolygonForest.shp&#39; # set file location of GRM polygons shape file grm &lt;- &#39;C:/Users/bermane/Desktop/RMF/grm_10_01_05.shp&#39; # set output folder for files generated # make sure no &quot;/&quot; at end of folder location! out_dir &lt;- &#39;C:/Users/bermane/Desktop/RMF&#39; # set file location of 2018 VLCE 2.0 landcover data # using 2018 because it is the year of Romeo ALS acquisition # can change based on ALS acquisition year # download here: # https://opendata.nfis.org/mapserver/nfis-change_eng.html lc_f &lt;- &#39;D:/ontario_inventory/VLCE/CA_forest_VLCE2_2018.tif&#39; 5.5 Pre-processing: Extract Variables into Polygons The first step before running imputation is to extract all of the variables needed into each of the polygons. We take the median value of all pixels covered by each polygon, weighted by percent coverage. We also calculate the polygon centroid x and y coordinates (in UTM) and the age of the forest stand in 2018 (year of ALS acquisition) for FRI polygons only. We start with the FRI polygons. ########################################### ### EXTRACT VARIABLES INTO FRI POLYGONS ### ########################################### # load FRI polygons poly &lt;- vect(fri) # convert to df dat_fri &lt;- as.data.frame(poly) # cbind centroids to dat dat_fri &lt;- cbind(dat_fri, centroids(poly) %&gt;% crds) # combine all LiDAR and aux variables to extract lidar_vars &lt;- c(lidar_imp, lidar_scr) # loop through LiDAR attributes to extract values for (i in seq_along(lidar_vars)) { # load LiDAR raster lidar_ras &lt;- rast(lidar_vars[i]) # project poly to crs of raster poly_ras &lt;- project(poly, lidar_ras) # convert to sf poly_ras &lt;- st_as_sf(poly_ras) #extract median values vec &lt;- exact_extract(lidar_ras, poly_ras, &#39;median&#39;) # aggregate into data frame if(i == 1){ vec_df &lt;- as.data.frame(vec) } else{ vec_df &lt;- cbind(vec_df, as.data.frame(vec)) } } # change column names of extracted attribute data frame colnames(vec_df) &lt;- names(lidar_vars) # add LiDAR attributes to FRI polygon data frame dat_fri &lt;- cbind(dat_fri, vec_df) # add 2018 age values dat_fri$AGE2018 &lt;- 2018 - dat_fri$YRORG # check if main dir exists and create if (dir.exists(out_dir) == F) { dir.create(out_dir) } # check if temp dir exists and create if (dir.exists(file.path(out_dir, &#39;temp&#39;)) == F) { dir.create(file.path(out_dir, &#39;temp&#39;)) } # save extracted dataframe for fast rebooting save(dat_fri, file = str_c(out_dir, &#39;/temp/dat_fri_extr.RData&#39;)) Next we extract the same variables into GRM polygons. ########################################### ### EXTRACT VARIABLES INTO GRM POLYGONS ### ########################################### # load GRM segmented polygons poly &lt;- vect(grm) # reproject to match FRI polygons poly &lt;- project(poly, vect(fri)) # convert to df dat_grm &lt;- as.data.frame(poly) # cbind centroids to dat dat_grm &lt;- cbind(dat_grm, centroids(poly) %&gt;% crds) # loop through LiDAR attributes to extract values for (i in seq_along(lidar_vars)) { # load LiDAR raster lidar_ras &lt;- rast(lidar_vars[i]) # project poly to crs of raster poly_ras &lt;- project(poly, lidar_ras) # convert to sf poly_ras &lt;- st_as_sf(poly_ras) #extract median values vec &lt;- exact_extract(lidar_ras, poly_ras, &#39;median&#39;) # aggregate into data frame if(i == 1){ vec_df &lt;- as.data.frame(vec) } else{ vec_df &lt;- cbind(vec_df, as.data.frame(vec)) } } # change column names of extracted attribute data frame colnames(vec_df) &lt;- names(lidar_vars) # add LiDAR attributes to FRI polygon data frame dat_grm &lt;- cbind(dat_grm, vec_df) # save extracted dataframe for fast rebooting save(dat_grm, file = str_c(out_dir, &#39;/temp/dat_grm_extr.RData&#39;)) 5.6 Pre-processing: Calculate Species Attributes The next step in pre-processing is to prepare the species attributes from the FRI. In the FRI, species type and percent composition are listed in a long string attribute “SPCOMP”. We break up this string into the following species attributes: First leading species Second leading species Three functional group classification (softwood, mixedwood, hardwood) Five functional group classification (jack pine dominated, black spruce dominated, mixed conifer, mixedwood, hardwood) See Queinnec et al. (2022) and Woods et al. (2011) for the derivation of three and five functional group classification. Note these group classifications were developed for the RMF and may be less relevant in other areas (specifically the five functional group classification). ############################################ ### FUNCTIONS FOR SPECIES CLASSIFICATION ### ############################################ # assign species name -- note this list was updated with all species in FSF FRI. # It may need to be adjusted for other areas assign_common_name &lt;- function(sp_abbrev) { sp_abbrev &lt;- toupper(sp_abbrev) dict &lt;- data.frame(SB = &quot;black spruce&quot;, LA = &quot;eastern larch&quot;, BW = &quot;white birch&quot;, BF = &quot;balsam fir&quot;, CE = &quot;cedar&quot;, SW = &quot;white spruce&quot;, PT = &quot;trembling aspen&quot;, PJ = &quot;jack pine&quot;, PO = &quot;poplar&quot;, PB = &quot;balsam poplar&quot;, PR = &quot;red pine&quot;, PW = &quot;white pine&quot;, SX = &quot;spruce&quot;, MR = &quot;red maple&quot;, AB = &quot;black ash&quot;, BY = &quot;yellow birch&quot;, OR = &#39;red oak&#39;, CW = &#39;eastern white cedar&#39;, MH = &#39;hard maple&#39;, HE = &#39;eastern hemlock&#39;, BD = &#39;basswood&#39;, CB = &#39;black cherry&#39;, BE = &#39;american beech&#39;, AW = &#39;white ash&#39;, PL = &#39;largetooth aspen&#39;, AG = &#39;red ash&#39;, OW = &#39;white oak&#39;, IW = &#39;ironwood&#39;, OB = &#39;bur oak&#39;, EW = &#39;white elm&#39;, MS = &#39;silver maple&#39;, PS = &#39;scots pine&#39;, OH = &#39;other hardwoods&#39;, BG = &#39;grey birch&#39;, AL = &#39;alder&#39;, SR = &#39;red spruce&#39;, BB = &#39;blue beech&#39;, MT = &#39;mountain maple&#39;, MB = &#39;black maple&#39;, OC = &#39;other conifers&#39;, SN = &#39;norway spruce&#39;, PE = &#39;silver poplar&#39;, HI = &#39;hickory&#39;, AX = &#39;ash&#39;) %&gt;% pivot_longer(everything(), names_to = &quot;abb&quot;, values_to = &quot;common&quot;) dict$common[match(sp_abbrev, dict$abb)] } # assign either coniferous or deciduous assign_type &lt;- function(sp_common) { sp_common &lt;- tolower(sp_common) ifelse(stringr::str_detect(sp_common, pattern = &quot;pine|spruce|fir|cedar|larch|conifers|hemlock&quot;), &quot;Coniferous&quot;, &quot;Deciduous&quot;) } #################################### ### CALCULATE SPECIES ATTRIBUTES ### #################################### # load fri poly_fri &lt;- st_read(fri) # separate SPCOMP string into individual columns poly_fri_for &lt;- poly_fri %&gt;% st_drop_geometry() %&gt;% filter(POLYTYPE == &quot;FOR&quot;) %&gt;% select(POLYID, POLYTYPE, SPCOMP) %&gt;% # these need to match FRI attr fields mutate(new_SP = str_match_all(SPCOMP, &quot;[A-Z]{2}[ ]+[0-9]+&quot;)) %&gt;% unnest(new_SP) %&gt;% mutate(new_SP = as.character(new_SP)) %&gt;% separate(new_SP, into = c(&quot;SP&quot;, &quot;PROP&quot;)) %&gt;% mutate(PROP = as.numeric(PROP), Common = assign_common_name(SP), sp_type = assign_type(Common)) # calculate polygon level species groups # percent species type poly_dom_type &lt;- poly_fri_for %&gt;% group_by(POLYID) %&gt;% summarize(per_conif = sum(PROP[sp_type == &quot;Coniferous&quot;]), per_decid = sum(PROP[sp_type == &quot;Deciduous&quot;])) # leading species poly_dom_sp &lt;- poly_fri_for %&gt;% group_by(POLYID) %&gt;% slice_max(PROP, n = 1, with_ties = FALSE) # combine type with leading species poly_dom_sp_group &lt;- inner_join(poly_dom_type, poly_dom_sp, by = &quot;POLYID&quot;) # calculate functional groups poly_dom_sp_group &lt;- poly_dom_sp_group %&gt;% mutate(SpeciesGroup1 = ifelse(PROP &gt;= 70, Common, ifelse(PROP &lt; 70 &amp; per_conif &gt;= 70, &quot;Mixed Coniferous&quot;, ifelse(PROP &lt; 70 &amp; per_decid &gt;= 70, &quot;Mixed Deciduous&quot;, &quot;Mixedwoods&quot;))), SpeciesGroup2 = ifelse(Common == &quot;jack pine&quot; &amp; PROP &gt;= 50 &amp; per_conif &gt;= 70, &quot;Jack Pine Dominated&quot;, ifelse( Common == &quot;black spruce&quot; &amp; PROP &gt;= 50 &amp; per_conif &gt;= 70, &quot;Black Spruce Dominated&quot;, ifelse( per_decid &gt;= 70, &quot;Hardwood&quot;, ifelse( per_decid &gt;= 30 &amp; per_decid &lt;= 70 &amp; per_conif &gt;= 30 &amp; per_conif &lt;= 70, &quot;Mixedwood&quot;, &quot;Mixed Conifers&quot; )))), SpeciesGroup3 = ifelse(per_conif &gt;= 70, &quot;Softwood&quot;, ifelse(per_decid &gt;= 70, &quot;Hardwood&quot;, &quot;Mixedwood&quot;))) %&gt;% mutate(across(.cols = starts_with(&quot;SpeciesGroup&quot;), .fns = as.factor)) %&gt;% select(POLYID, SpeciesGroup2, SpeciesGroup3) %&gt;% rename(class5 = SpeciesGroup2, class3 = SpeciesGroup3) # calculate leading and second species only poly_fri_sp &lt;- poly_fri %&gt;% st_drop_geometry() %&gt;% filter(POLYTYPE == &quot;FOR&quot;) %&gt;% select(POLYID, POLYTYPE, SPCOMP) %&gt;% # these need to match FRI attr fields mutate(new_SP = str_match_all(SPCOMP, &quot;[A-Z]{2}[ ]+[0-9]+&quot;)) %&gt;% unnest_wider(new_SP, names_sep = &#39;&#39;) %&gt;% rename(new_SP = new_SP1) %&gt;% mutate(new_SP = as.character(new_SP), new_SP2 = as.character(new_SP2)) %&gt;% separate(new_SP, into = c(&quot;SP&quot;, &quot;PROP&quot;)) %&gt;% separate(new_SP2, into = c(&quot;SP2&quot;, &quot;PROP2&quot;)) %&gt;% select(POLYID, SP, SP2) # join functional groups with leading and second species poly_dom_sp_group &lt;- left_join(poly_dom_sp_group, poly_fri_sp, by = &#39;POLYID&#39;) %&gt;% rename(SP1 = SP) # join to FRI extracted dataframe dat_fri &lt;- left_join(dat_fri, poly_dom_sp_group, by = &#39;POLYID&#39;) # re-save extracted dataframe for fast rebooting save(dat_fri, file = str_c(out_dir, &#39;/temp/dat_fri_extr.RData&#39;)) 5.7 Pre-processing: Forest Data Screening FRI and GRM polygons delineate the entire landscape, not just forest stands. Since we are only interested in imputing age and species attributes in forest stands, it is important to screen the FRI and GRM datasets and remove polygons that do not fit certain data criteria representative of forest stands. We want to ensure: We only use FRI polygons for imputation that are forested, to ensure accurate and representative data points We only impute values into GRM polygons that are forested, since age and species attributes do not apply to non-forested polygons The current data criteria being used is as follows: POLYTYPE == ‘FOR’ (for FRI polygons only) polygon &gt;= 50% forested landcover p95 &gt;= 5 meters (broad definition of ‘forest’) Canopy cover &gt;= 50% We first apply data screening to the FRI polygons. ##################### ### LOAD FRI DATA ### ##################### # load FRI polygons poly &lt;- vect(fri) # load FRI polygon data frame load(str_c(out_dir, &#39;/temp/dat_fri_extr.RData&#39;)) ############################# ### DATA SCREENING PART 1 ### ############################# # remove all non-forested polygons dat_fri &lt;- filter(dat_fri, POLYTYPE == &#39;FOR&#39;) # create smaller polygon set only FOR polytypes poly_fri &lt;- poly[poly$POLYTYPE == &#39;FOR&#39;] ############################# ### DATA SCREENING PART 2 ### ############################# # polygon landcover &gt; 50% forested # load VLCE 2.0 landcover dataset from 2018 lc &lt;- rast(lc_f) # project poly to crs of raster poly_lc &lt;- project(poly_fri, lc) # convert to sf poly_lcsf &lt;- st_as_sf(poly_lc) # extract landcover values lc_poly &lt;- exact_extract(lc, poly_lcsf) # set landcover class key with single forested class lc_key_for &lt;- c(`0` = &#39;NA&#39;, `20` = &#39;Water&#39;, `31` = &#39;Snow/Ice&#39;, `32` = &#39;Rock/Rubble&#39;, `33` = &#39;Exposed/Barren Land&#39;, `40` = &#39;Bryoids&#39;, `50` = &#39;Shrubland&#39;, `80` = &#39;Wetland&#39;, `81` = &#39;Forest&#39;, `100` = &#39;Herbs&#39;, `210` = &#39;Forest&#39;, `220` = &#39;Forest&#39;, `230` = &#39;Forest&#39;) # find pixels with forest at least 50% of pixel # apply over list lc_dom_for &lt;- sapply(lc_poly, function(x){ x$value &lt;- recode(x$value, !!!lc_key_for) x &lt;- x %&gt;% group_by(value) %&gt;% summarize(sum = sum(coverage_fraction)) m &lt;- x$value[which(x$sum == max(x$sum))] if((length(m) == 1) &amp; (m == &#39;Forest&#39;)[1]){ if(x$sum[x$value == m]/sum(x$sum) &gt;= 0.5){ return(&#39;Yes&#39;) }else{return(&#39;No&#39;)} }else{return(&#39;No&#39;)} }) # add new columns into dat dat_fri &lt;- dat_fri %&gt;% add_column(dom_for = lc_dom_for) # subset FRI data frame based on whether polygon dominated by forest dat_fri_scr &lt;- dat_fri %&gt;% filter(dom_for == &#39;Yes&#39;) ############################# ### DATA SCREENING PART 3 ### ############################# # require p95 &gt;= 5 # require cc &gt;= 50 dat_fri_scr %&lt;&gt;% filter(p95 &gt;= 5, cc &gt;= 50) # save extracted data frame for fast rebooting save(dat_fri_scr, file = str_c(out_dir, &#39;/temp/dat_fri_scr.RData&#39;)) We next apply the data screening to the GRM polygons. ##################### ### LOAD GRM DATA ### ##################### # load GRM polygon data frame load(str_c(out_dir, &#39;/temp/dat_grm_extr.RData&#39;)) ###################### ### DATA SCREENING ### ###################### # Don&#39;t screen GRM polygons for POLYTYPE == FOR # polygon landcover &gt; 50% forested # dom_for attribute already exists in GRM data from segmentation # require p95 &gt;= 5 # require cc &gt;= 50 dat_grm_scr &lt;- dat_grm %&gt;% filter(dom_for == &#39;Yes&#39;, p95 &gt;= 5, cc &gt;= 50) # save extracted data frame for fast rebooting save(dat_grm_scr, file = str_c(out_dir, &#39;/temp/dat_grm_scr.RData&#39;)) 5.8 Execute Imputation Algorithm Over FRI Polygons ONLY The goal of this imputation procedure is to estimate age and species composition in GRM segmented forest polygons. But it is also important to assess the performance of the algorithm. To best do this, we first conduct imputation over the FRI dataset ONLY. For each FRI polygon, we find the k-nearest neighbors (k=5), and calculate the age and species composition attributes to impute (a “leave one out” approach”). We can then compare the observed age and species composition of the polygon to the imputed values. Note we have to do this calculation on the FRI dataset alone because we do not have observed age and species composition values for the GRM segmented polygons. Also note that ALL attributes are imputed from the same k-nearest neighbors. The algorithm is not run for individual attributes. For age, we report root mean squared difference (RMSD), mean absolute error (MAE) and mean bias error (MBE). RMSD was used in the performance analysis to assess results between algorithm runs and to decide upon an optimal model (see forthcoming journal article). MAE gives an average error of imputed age in years. MBE gives an indication of whether we are imputing younger or older ages on average. For species composition, we report the percent of observed and imputed values that match (accuracy). We also calculate performance metrics on the X-variables used in imputation: relative RMSD (RRMSD) and relative MBE (RMBE), calculated by dividing RMSD and RMBE by the mean value of each variable and multiplying by 100. RRMSD and RMBE give a percent error that can be compared across variables and when variables have difficult to interpret units, such as several of the ALS metrics used in imputation. First we define the functions needed for imputation. ###################################################### ### FUNCTIONS TO RUN K NEAREST NEIGHBOR IMPUTATION ### ###################################################### # create mode function getmode &lt;- function(v) { uniqv &lt;- unique(v) uniqv[which.max(tabulate(match(v, uniqv)))] } # create rmsd function rmsd &lt;- function(obs, est){ sqrt(mean((est - obs) ^ 2)) } # create rrmsd function rrmsd &lt;- function(obs, est){ sqrt(mean((est - obs) ^ 2)) / mean(obs) * 100 } # create mae function mae &lt;- function(obs, est){ mean(abs(est - obs)) } # create mbe function mbe &lt;- function(obs, est){ mean(est - obs) } # create rmbe function rmbe &lt;- function(obs, est){ mean(est - obs) / mean(obs) * 100 } # create knn function to output performance results run_knn_fri &lt;- function(dat, vars, k) { # subset data dat_nn &lt;- dat %&gt;% select(all_of(vars)) # scale for nn computation dat_nn_scaled &lt;- dat_nn %&gt;% scale # run nearest neighbor nn &lt;- nn2(dat_nn_scaled, dat_nn_scaled, k = k + 1) # get nn indices nni &lt;- nn[[1]][, 2:(k + 1)] # add vars to tibble # take mean/mode if k &gt; 1 if(k &gt; 1){ for(i in seq_along(vars)){ if(i == 1){ nn_tab &lt;- tibble(!!vars[i] := dat_nn[,i], !!str_c(vars[i], &#39;_nn&#39;) := apply(nni, MARGIN = 1, FUN = function(x){ mean(dat_nn[x, i]) })) }else{ nn_tab %&lt;&gt;% mutate(!!vars[i] := dat_nn[,i], !!str_c(vars[i], &#39;_nn&#39;) := apply(nni, MARGIN = 1, FUN = function(x){ mean(dat_nn[x, i]) })) } } # add target vars to tibble nn_tab %&lt;&gt;% mutate(age = dat$AGE2018, sp1 = dat$SP1, sp2 = dat$SP2, class5 = dat$class5, class3 = dat$class3, age_nn = apply(nni, MARGIN = 1, FUN = function(x){ mean(dat$AGE2018[x]) }), sp1_nn = apply(nni, MARGIN = 1, FUN = function(x){ getmode(dat$SP1[x]) }), sp2_nn = apply(nni, MARGIN = 1, FUN = function(x){ getmode(dat$SP2[x]) }), class5_nn = apply(nni, MARGIN = 1, FUN = function(x){ getmode(dat$class5[x]) }), class3_nn = apply(nni, MARGIN = 1, FUN = function(x){ getmode(dat$class3[x]) })) } # take direct nn if k == 1 if(k == 1){ for(i in seq_along(vars)){ if(i == 1){ nn_tab &lt;- tibble(!!vars[i] := dat_nn[,i], !!str_c(vars[i], &#39;_nn&#39;) := dat_nn[nn[[1]][,2],i]) }else{ nn_tab %&lt;&gt;% mutate(!!vars[i] := dat_nn[,i], !!str_c(vars[i], &#39;_nn&#39;) := dat_nn[nn[[1]][,2],i]) } } # add target vars to tibble nn_tab %&lt;&gt;% mutate(age = dat$AGE2018, sp1 = dat$SP1, sp2 = dat$SP2, class5 = dat$class5, class3 = dat$class3, age_nn = dat$AGE2018[nn[[1]][,2]], sp1_nn = dat$SP1[nn[[1]][,2]], sp2_nn = dat$SP2[nn[[1]][,2]], class5_nn = dat$class5[nn[[1]][,2]], class3_nn = dat$class3[nn[[1]][,2]]) } # calculate fit metrics for vars for(i in seq_along(vars)){ if(i == 1){ perform_df &lt;- tibble(variable = vars[i], metric = c(&#39;rrmsd (%)&#39;, &#39;rmbe (%)&#39;), value = c(rrmsd(pull(nn_tab, vars[i]), pull(nn_tab, str_c(vars[i], &#39;_nn&#39;))), rmbe(pull(nn_tab, vars[i]), pull(nn_tab, str_c(vars[i], &#39;_nn&#39;))))) }else{ perform_df %&lt;&gt;% add_row(variable = vars[i], metric = c(&#39;rrmsd (%)&#39;, &#39;rmbe (%)&#39;), value = c(rrmsd(pull(nn_tab, vars[i]), pull(nn_tab, str_c(vars[i], &#39;_nn&#39;))), rmbe(pull(nn_tab, vars[i]), pull(nn_tab, str_c(vars[i], &#39;_nn&#39;))))) } } # calculate metrics for age perform_df %&lt;&gt;% add_row(variable = &#39;age&#39;, metric = c(&#39;rmsd (yrs)&#39;, &#39;mbe (yrs)&#39;, &#39;mae (yrs)&#39;), value = c(rmsd(nn_tab$age, nn_tab$age_nn), mbe(nn_tab$age, nn_tab$age_nn), mae(nn_tab$age, nn_tab$age_nn))) # calculate SP1 accuracy # create df of SP1 sp1 &lt;- data.frame(obs = nn_tab$sp1, est = nn_tab$sp1_nn) # create column of match or not sp1$match &lt;- sp1$obs == sp1$est # add total percent of matching SP1 to perform_df perform_df %&lt;&gt;% add_row(variable = &#39;leading species&#39;, metric = &#39;accuracy (%)&#39;, value = NROW(sp1[sp1$match == T,]) / NROW(sp1) * 100) # calculate SP2 accuracy # create df of SP2 sp2 &lt;- data.frame(obs = nn_tab$sp2, est = nn_tab$sp2_nn) # create column of match or not sp2$match &lt;- sp2$obs == sp2$est # add total percent of matching SP2 to perform_df perform_df %&lt;&gt;% add_row(variable = &#39;second species&#39;, metric = &#39;accuracy (%)&#39;, value = NROW(sp2[sp2$match == T,]) / NROW(sp2) * 100) # calculate class3 accuracy # create df of class3 class3 &lt;- data.frame(obs = nn_tab$class3, est = nn_tab$class3_nn) # create column of match or not class3$match &lt;- class3$obs == class3$est # add total percent of matching class3 to perform_df perform_df %&lt;&gt;% add_row(variable = &#39;three func group class&#39;, metric = &#39;accuracy (%)&#39;, value = NROW(class3[class3$match == T,]) / NROW(class3) * 100) # calculate class5 accuracy # create df of class5 class5 &lt;- data.frame(obs = nn_tab$class5, est = nn_tab$class5_nn) # create column of match or not class5$match &lt;- class5$obs == class5$est # add total percent of matching class5 to perform_df perform_df %&lt;&gt;% add_row(variable = &#39;five func group class&#39;, metric = &#39;accuracy (%)&#39;, value = NROW(class5[class5$match == T,]) / NROW(class5) * 100) # return df return(perform_df) } # create knn function to output imputed vs. observed table run_knn_fri_table &lt;- function(dat, vars, k) { # subset data dat_nn &lt;- dat %&gt;% select(all_of(vars)) # scale for nn computation dat_nn_scaled &lt;- dat_nn %&gt;% scale # run nearest neighbor nn &lt;- nn2(dat_nn_scaled, dat_nn_scaled, k = k + 1) # get nn indices nni &lt;- nn[[1]][, 2:(k + 1)] # add vars to tibble # take mean/mode if k &gt; 1 if(k &gt; 1){ for(i in seq_along(vars)){ if(i == 1){ nn_tab &lt;- tibble(!!vars[i] := dat_nn[,i], !!str_c(vars[i], &#39;_nn&#39;) := apply(nni, MARGIN = 1, FUN = function(x){ mean(dat_nn[x, i]) })) }else{ nn_tab %&lt;&gt;% mutate(!!vars[i] := dat_nn[,i], !!str_c(vars[i], &#39;_nn&#39;) := apply(nni, MARGIN = 1, FUN = function(x){ mean(dat_nn[x, i]) })) } } # add target vars to tibble nn_tab %&lt;&gt;% mutate(age = dat$AGE2018, sp1 = dat$SP1, sp2 = dat$SP2, class5 = dat$class5, class3 = dat$class3, age_nn = apply(nni, MARGIN = 1, FUN = function(x){ mean(dat$AGE2018[x]) }), sp1_nn = apply(nni, MARGIN = 1, FUN = function(x){ getmode(dat$SP1[x]) }), sp2_nn = apply(nni, MARGIN = 1, FUN = function(x){ getmode(dat$SP2[x]) }), class5_nn = apply(nni, MARGIN = 1, FUN = function(x){ getmode(dat$class5[x]) }), class3_nn = apply(nni, MARGIN = 1, FUN = function(x){ getmode(dat$class3[x]) })) } # take direct nn if k == 1 if(k == 1){ for(i in seq_along(vars)){ if(i == 1){ nn_tab &lt;- tibble(!!vars[i] := dat_nn[,i], !!str_c(vars[i], &#39;_nn&#39;) := dat_nn[nn[[1]][,2],i]) }else{ nn_tab %&lt;&gt;% mutate(!!vars[i] := dat_nn[,i], !!str_c(vars[i], &#39;_nn&#39;) := dat_nn[nn[[1]][,2],i]) } } # add target vars to tibble nn_tab %&lt;&gt;% mutate(age = dat$AGE2018, sp1 = dat$SP1, sp2 = dat$SP2, class5 = dat$class5, class3 = dat$class3, age_nn = dat$AGE2018[nn[[1]][,2]], sp1_nn = dat$SP1[nn[[1]][,2]], sp2_nn = dat$SP2[nn[[1]][,2]], class5_nn = dat$class5[nn[[1]][,2]], class3_nn = dat$class3[nn[[1]][,2]]) } # return nn table return(nn_tab) } Then we run imputation on the FRI dataset using the functions defined above. ######################################################## ### RUN KNN IMPUTATION USING OPTIMAL MODEL VARIABLES ### ######################################################## # load FRI screened polygons load(str_c(out_dir, &#39;/temp/dat_fri_scr.RData&#39;)) # subset only the attributes we need from the screened FRI polygons dat_fri_scr %&lt;&gt;% select(POLYID, AGE2018, avg, rumple, pcum8, sd, b6, x, y, SP1, SP2, class3, class5) # remove any polygons with missing values # in imputation X-variables dat_fri_scr &lt;- left_join(dat_fri_scr %&gt;% select(POLYID, AGE2018, avg, rumple, pcum8, sd, b6, x, y) %&gt;% na.omit, dat_fri_scr) # create vector of X-variables for imputation vars &lt;- c(&#39;avg&#39;, &#39;rumple&#39;, &#39;pcum8&#39;, &#39;sd&#39;, &#39;b6&#39;, &#39;x&#39;, &#39;y&#39;) # run_knn_fri function to get performance results perf &lt;- run_knn_fri(dat_fri_scr, vars, k = 5) # run_knn_fri function to get imputed vs. observed values nn_tab &lt;- run_knn_fri_table(dat_fri_scr, vars, k = 5) 5.9 Results: Imputation over FRI For the imputation over FRI we don’t generate output polygons, just the performance metrics listed above so we can assess the performance of the imputation. Having run the imputation function, we clean the output data and display results. # round values perf %&lt;&gt;% mutate(value = round(value, 2)) # factor variable and metric categories to order perf %&lt;&gt;% mutate(variable = factor(variable, levels = c(&#39;age&#39;, &#39;leading species&#39;, &#39;second species&#39;, &#39;three func group class&#39;, &#39;five func group class&#39;, vars))) %&gt;% mutate(metric = factor(metric, levels = c(&#39;rmsd (yrs)&#39;, &#39;mbe (yrs)&#39;, &#39;mae (yrs)&#39;, &#39;accuracy (%)&#39;, &#39;rrmsd (%)&#39;, &#39;rmbe (%)&#39;))) # cast df perf_cast &lt;- dcast(perf, variable ~ metric) # remove x and y perf_cast %&lt;&gt;% filter(!(variable %in% c(&#39;x&#39;, &#39;y&#39;))) # set NA to blank perf_cast[is.na(perf_cast)] &lt;- &#39;&#39; # display results knitr::kable(perf_cast, caption = &quot;Imputation Performance of FRI Forest Stand Polygons&quot;, label = NA) Imputation Performance of FRI Forest Stand Polygons variable rmsd (yrs) mbe (yrs) mae (yrs) accuracy (%) rrmsd (%) rmbe (%) age 23.31 -0.14 16.06 leading species 65.46 second species 37.62 three func group class 72.34 five func group class 60.73 avg 3.6 0 rumple 2.54 0.01 pcum8 0.9 0.13 sd 3.88 -0.33 b6 2.15 -0.1 The mean bias error (MBE) of age is -0.14 years, indicating the imputed estimates of age are not skewed toward younger or older values. The mean absolute error (MAE) of age is 16.06 years, which is the average difference between the observed and imputed value. Accuracy of leading species classification is 65.46%, and a much lower 37.62% for second leading species. Three and five functional group classification have respective accuracies of 72.34% and 60.73%. Relative root mean squared difference (RRMSD) of the imputation attributes (avg, sd, rumple, zpcum8, and red_edge_2) is below 4% for all attributes. These are low values, which demonstrate that the imputation algorithm is finding optimal matches within the database of available FRI polygons. Relative mean bias error (RMBE) of the imputation attributes is less than 1%, meaning that the nearest neighbor selections are not skewed toward positive or negative values of these attributes. RRMSD/RMBE are not calculated for x and y because the coordinates do not represent a value scale. We can also generate detailed confusion matrices of the imputed vs. observed species composition. Three functional group classification: # calculate 3 func group confusion matrix # build accuracy table accmat &lt;- table(&quot;pred&quot; = nn_tab$class3_nn, &quot;ref&quot; = nn_tab$class3) # UA ua &lt;- diag(accmat) / rowSums(accmat) * 100 # PA pa &lt;- diag(accmat) / colSums(accmat) * 100 # OA oa &lt;- sum(diag(accmat)) / sum(accmat) * 100 # build confusion matrix accmat_ext &lt;- addmargins(accmat) accmat_ext &lt;- rbind(accmat_ext, &quot;Users&quot; = c(pa, NA)) accmat_ext &lt;- cbind(accmat_ext, &quot;Producers&quot; = c(ua, NA, oa)) accmat_ext &lt;- round(accmat_ext, 2) dimnames(accmat_ext) &lt;- list(&quot;Imputed&quot; = colnames(accmat_ext), &quot;Observed&quot; = rownames(accmat_ext)) class(accmat_ext) &lt;- &quot;table&quot; # display results knitr::kable(accmat_ext %&gt;% round, caption = &quot;Confusion matrix of imputed vs. observed three functional group classification over FRI polygons. Rows are imputed values and columns are observed values.&quot;, label = NA) Confusion matrix of imputed vs. observed three functional group classification over FRI polygons. Rows are imputed values and columns are observed values. Hardwood Mixedwood Softwood Sum Users Hardwood 6651 2448 1044 10143 66 Mixedwood 2277 4426 2697 9400 47 Softwood 1397 4363 26134 31894 82 Sum 10325 11237 29875 51437 NA Producers 64 39 87 NA 72 Five functional group classification: # calculate 5 func group confusion matrix # build accuracy table accmat &lt;- table(&quot;pred&quot; = nn_tab$class5_nn, &quot;ref&quot; = nn_tab$class5) # UA ua &lt;- diag(accmat) / rowSums(accmat) * 100 # PA pa &lt;- diag(accmat) / colSums(accmat) * 100 # OA oa &lt;- sum(diag(accmat)) / sum(accmat) * 100 # build confusion matrix accmat_ext &lt;- addmargins(accmat) accmat_ext &lt;- rbind(accmat_ext, &quot;Users&quot; = c(pa, NA)) accmat_ext &lt;- cbind(accmat_ext, &quot;Producers&quot; = c(ua, NA, oa)) accmat_ext &lt;- round(accmat_ext, 2) dimnames(accmat_ext) &lt;- list(&quot;Imputed&quot; = colnames(accmat_ext), &quot;Observed&quot; = rownames(accmat_ext)) class(accmat_ext) &lt;- &quot;table&quot; # display results knitr::kable(accmat_ext %&gt;% round, caption = &quot;Confusion matrix of imputed vs. observed five functional group classification over FRI polygons. Rows are imputed values and columns are observed values.&quot;, label = NA) Confusion matrix of imputed vs. observed five functional group classification over FRI polygons. Rows are imputed values and columns are observed values. Black Spruce Dominated Hardwood Jack Pine Dominated Mixed Conifers Mixedwood Sum Users Black Spruce Dominated 14832 575 618 2623 2664 21312 70 Hardwood 480 6744 323 283 2618 10448 65 Jack Pine Dominated 366 218 2481 178 492 3735 66 Mixed Conifers 1325 186 112 1326 765 3714 36 Mixedwood 2064 2602 539 1169 5854 12228 48 Sum 19067 10325 4073 5579 12393 51437 NA Producers 78 65 61 24 47 NA 61 Leading species: # calculate leading species confusion matrix # create df of sp1 sp1 &lt;- data.frame(obs = nn_tab$sp1 %&gt;% as.factor, est = nn_tab$sp1_nn %&gt;% as.factor) # make estimate levels match obs levels levels(sp1$est) &lt;- c(levels(sp1$est), levels(sp1$obs)[!(levels(sp1$obs) %in% levels(sp1$est))]) sp1$est &lt;- factor(sp1$est, levels = levels(sp1$obs)) # create column of match or not sp1$match &lt;- sp1$obs == sp1$est # build accuracy table accmat &lt;- table(&quot;pred&quot; = sp1$est, &quot;ref&quot; = sp1$obs) # UA ua &lt;- diag(accmat) / rowSums(accmat) * 100 # PA pa &lt;- diag(accmat) / colSums(accmat) * 100 # OA oa &lt;- sum(diag(accmat)) / sum(accmat) * 100 # build confusion matrix accmat_ext &lt;- addmargins(accmat) accmat_ext &lt;- rbind(accmat_ext, &quot;Users&quot; = c(pa, NA)) accmat_ext &lt;- cbind(accmat_ext, &quot;Producers&quot; = c(ua, NA, oa)) accmat_ext &lt;- round(accmat_ext, 2) dimnames(accmat_ext) &lt;- list(&quot;Imputed&quot; = colnames(accmat_ext), &quot;Observed&quot; = rownames(accmat_ext)) class(accmat_ext) &lt;- &quot;table&quot; # display results knitr::kable(accmat_ext %&gt;% round, caption = &quot;Confusion matrix of imputed vs. observed leading species over FRI polygons. Rows are imputed values and columns are observed values.&quot;, label = NA) %&gt;% kable_styling(latex_options=&quot;scale_down&quot;) Confusion matrix of imputed vs. observed leading species over FRI polygons. Rows are imputed values and columns are observed values. AB BF BW CE LA MR PB PJ PO PR PT PW SB SW SX Sum Users AB 6 0 4 0 0 0 0 0 0 0 0 0 2 0 0 12 50 BF 0 31 35 11 6 0 0 7 15 1 7 0 67 12 0 192 16 BW 10 86 4248 320 33 0 15 228 401 1 1165 26 1333 166 0 8032 53 CE 0 21 166 428 43 0 0 18 18 0 45 0 471 10 1 1221 35 LA 0 2 16 19 57 0 0 11 9 0 15 0 165 5 0 299 19 MR 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 NaN PB 0 1 5 0 0 0 4 0 2 0 9 0 4 2 0 27 15 PJ 2 8 185 18 43 0 0 3111 92 6 449 1 630 46 0 4591 68 PO 5 21 248 17 16 0 5 88 876 1 290 4 155 21 0 1747 50 PR 0 0 1 0 0 0 0 0 1 0 0 0 2 0 0 4 0 PT 5 28 1042 64 23 0 52 443 346 1 3594 7 715 69 0 6389 56 PW 0 0 1 0 0 0 0 0 0 0 0 2 1 1 0 5 40 SB 6 393 1815 1316 759 1 19 1155 277 2 1106 14 21259 506 1 28629 74 SW 0 10 50 3 4 0 3 22 10 0 33 2 96 56 0 289 19 SX 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 NaN Sum 34 601 7816 2196 984 1 98 5083 2047 12 6713 56 24900 894 2 51437 NA Producers 18 5 54 19 6 0 4 61 43 0 54 4 85 6 0 NA 65 5.10 Execute Imputation Algorithm From FRI to GRM Polygons The last step is to run the imputation between the screened FRI polygons and the GRM segmented polygons. For each GRM segmented polygon, the k-nearest neighbors in the FRI data are found and used to impute age and species composition. Note we do not conduct imputation on all the GRM segmented polygons, but only the forested polygons that passed through the data screening filters. Although we cannot directly assess the error of age and species composition when imputing into the GRM segmented polygons, we can still assess the fit of the variables used in the imputation. We can also review maps and distributions comparing FRI age/species composition against the same attributes imputed into GRM segmented polygons. First we execute the imputation algorithm itself. Since there was no iterative performance analysis conducted on the FRI to GRM imputation, we do not define an imputation function, rather just run the algorithm in line. # load GRM screened polygons load(str_c(out_dir, &#39;/temp/dat_grm_scr.RData&#39;)) # create data frame for grm and fri metrics used in imputation dat_grm_imp &lt;- dat_grm_scr %&gt;% select(id, avg, rumple, pcum8, sd, b6, x, y) %&gt;% na.omit dat_fri_imp &lt;- dat_fri_scr %&gt;% select(avg, rumple, pcum8, sd, b6, x, y) %&gt;% na.omit # need to combine and scale all values together then separate again dat_comb_scaled &lt;- rbind(dat_grm_imp %&gt;% select(-id), dat_fri_imp) %&gt;% scale dat_grm_scaled &lt;- dat_comb_scaled[1:NROW(dat_grm_imp),] dat_fri_scaled &lt;- dat_comb_scaled[(NROW(dat_grm_imp)+1):(NROW(dat_grm_imp)+NROW(dat_fri_imp)),] # run nearest neighbor imputation k = 5 nn &lt;- nn2(dat_fri_scaled, dat_grm_scaled, k = 5) # get nn indices nni &lt;- nn[[1]] # add imputed attributes into GRM imputation data frame for(i in seq_along(vars)){ dat_grm_imp %&lt;&gt;% add_column( !!str_c(vars[i], &#39;_imp&#39;) := apply( nni, MARGIN = 1, FUN = function(x){ mean(dat_fri_imp[x, vars[i]]) } )) } # create vector of target variables tar_vars &lt;- c(&#39;AGE2018&#39;, &#39;SP1&#39;, &#39;SP2&#39;, &#39;class3&#39;, &#39;class5&#39;) # add age and species variables to GRM data frame for(i in seq_along(tar_vars)){ if(i == &#39;AGE2018&#39;){ dat_grm_imp %&lt;&gt;% add_column( !!tar_vars[i] := apply( nni, MARGIN = 1, FUN = function(x){ mean(dat_fri_scr[x, tar_vars[i]]) } )) } else{ dat_grm_imp %&lt;&gt;% add_column( !!tar_vars[i] := apply( nni, MARGIN = 1, FUN = function(x){ getmode(dat_fri_scr[x, tar_vars[i]]) } )) } } # update colnames dat_grm_imp %&lt;&gt;% rename(age = AGE2018) # add values back into main GRM data frame dat_grm &lt;- left_join(dat_grm, dat_grm_imp) 5.11 Results: Imputation X-Variable Performance Now having run the imputation and added the imputed attributes to the GRM polygon data, we can explore the results. We start by looking at the relative root mean squared difference (RRMSD) and relative mean bias error (RMBE) of the X-variables used in the algorithm. # calculate performance across imputation x-variables for(i in seq_along(vars)){ if(i == 1){ perf &lt;- tibble(variable = vars[i], metric = c(&#39;rrmsd (%)&#39;, &#39;rmbe (%)&#39;), value = c(rrmsd(dat_grm_imp[, vars[i]], dat_grm_imp[, str_c(vars[i], &#39;_imp&#39;)]), rmbe(dat_grm_imp[, vars[i]], dat_grm_imp[, str_c(vars[i], &#39;_imp&#39;)]))) }else{ perf %&lt;&gt;% add_row(variable = vars[i], metric = c(&#39;rrmsd (%)&#39;, &#39;rmbe (%)&#39;), value = c(rrmsd(dat_grm_imp[, vars[i]], dat_grm_imp[, str_c(vars[i], &#39;_imp&#39;)]), rmbe(dat_grm_imp[, vars[i]], dat_grm_imp[, str_c(vars[i], &#39;_imp&#39;)]))) } } # round to two decimal places perf %&lt;&gt;% mutate(value = round(value, 2)) # factor so table displays nicely perf %&lt;&gt;% mutate(variable = factor(variable, levels = vars)) %&gt;% mutate(metric = factor(metric, levels = c(&#39;rrmsd (%)&#39;, &#39;rmbe (%)&#39;))) # cast df perf_cast &lt;- dcast(perf, variable ~ metric) # remove x and y perf_cast %&lt;&gt;% filter(!(variable %in% c(&#39;x&#39;, &#39;y&#39;))) # display results of imputation rmsd knitr::kable(perf_cast, caption = &quot;Imputation X-Variable Performance between FRI and GRM Forest Stand Polygons&quot;, label = NA) Imputation X-Variable Performance between FRI and GRM Forest Stand Polygons variable rrmsd (%) rmbe (%) avg 3.82 -0.04 rumple 2.71 0.09 pcum8 0.88 0.12 sd 4.16 -0.40 b6 2.08 -0.16 The RRMSD results are all below 5% and RMBE does not show bias toward negative of positive difference. These results are comparable to those found when conducting imputation over FRI polygons only. 5.12 Results: Distribution of Age The next thing we can do is compare the distribution of imputed values in GRM polygons to observed values in FRI polygons. We can compare spatial patterns as well as overall distributions. We first generate a spatial comparison of age across the RMF. # load GRM polygons poly_grm &lt;- vect(grm) # add new data frame to polygons values(poly_grm) &lt;- dat_grm # save grm polygon output writeVector(poly_grm, str_c(out_dir, &#39;/grm_imp.shp&#39;), overwrite = T) # load FRI polygons poly_fri &lt;- vect(fri) # combine dat_fri with polygon attributes dat_fri_poly &lt;- left_join(as.data.frame(poly_fri), dat_fri) # we should only compare the screened FRI polygons so set # other values to NA dat_fri_poly$AGE2018[!(dat_fri_poly$POLYID %in% dat_fri_scr$POLYID)] &lt;- NA dat_fri_poly$class3[!(dat_fri_poly$POLYID %in% dat_fri_scr$POLYID)] &lt;- NA dat_fri_poly$class5[!(dat_fri_poly$POLYID %in% dat_fri_scr$POLYID)] &lt;- NA # re-input attributes into FRI polygons values(poly_fri) &lt;- dat_fri_poly rm(dat_fri_poly) # save fri polygon output with key values only in screened polygons writeVector(poly_fri, str_c(out_dir, &#39;/fri_scr.shp&#39;), overwrite = T) # create df to plot age grm_sf &lt;- st_as_sf(poly_grm) fri_sf &lt;- st_as_sf(poly_fri) # cut dfs grm_sf %&lt;&gt;% mutate(age_cut = cut(age, breaks = c(seq(0, 130, 10), 250))) fri_sf %&lt;&gt;% mutate(age_cut = cut(AGE2018, breaks = c(seq(0, 130, 10), 250))) # plot age p1 &lt;- ggplot(grm_sf) + geom_sf(mapping = aes(fill = age_cut), linewidth = 0.01) + coord_sf() + scale_fill_manual(values = viridis(14), name = &#39;Age&#39;, na.translate = F) + theme_void(base_size = 30) + ggtitle(&#39;Imputed Age of GRM \\nSegmented Forest Stands&#39;) + theme(plot.title = element_text(hjust = 0.5)) p2 &lt;- ggplot(fri_sf) + geom_sf(mapping = aes(fill = age_cut), linewidth = 0.01) + coord_sf() + scale_fill_manual(values = viridis(14), name = &#39;Age&#39;, na.translate = F) + theme_void(base_size = 30) + ggtitle(&#39;Age of FRI \\nForest Stands&#39;) + theme(plot.title = element_text(hjust = 0.5)) grid.arrange(p1, p2, ncol = 1) Imputed age values show a similar spatial distribution to observed age values at a broad scale. Of course, the values should be scrutinized on a fine scale in specific areas. This task is much easier to do in a GIS software using the output shapefiles. We next generate density plots of stand age. # density plots of age p1 &lt;- ggplot(dat_grm, aes(x = age)) + geom_density(fill = &#39;grey&#39;) + geom_vline(aes(xintercept = median(age, na.rm = T)), linetype = &quot;dashed&quot;, size = 0.6) + xlim(c(0,200)) + ylim(c(0, 0.02)) + theme_bw() + xlab(&#39;Age&#39;) + ylab(&#39;Density&#39;) + ggtitle(&#39;Imputed Age of GRM Segmented Forest Stands&#39;) + theme(text = element_text(size = 25), plot.title = element_text(size=30)) p2 &lt;- ggplot(as.data.frame(poly_fri), aes(x = AGE2018)) + geom_density(fill = &#39;grey&#39;) + geom_vline(aes(xintercept = median(AGE2018, na.rm = T)), linetype = &quot;dashed&quot;, size = 0.6) + xlim(c(0, 200)) + ylim(c(0, 0.02)) + theme_bw() + xlab(&#39;Age&#39;) + ylab(&#39;Density&#39;) + ggtitle(&#39;Age of FRI Forest Stands&#39;) + theme(text = element_text(size = 25), plot.title = element_text(size=30)) grid.arrange(p1, p2, ncol = 1) The distribution of imputed age in GRM polygons closely matches that of observed age in FRI polygons. The median age values (dotted lines) are 78 (GRM) and 83 (FRI). 5.13 Results: Distribution of Species Finally we explore the distribution of species, starting with spatial patterns of three functional group classification. # plot three func group classification p1 &lt;- ggplot(grm_sf) + geom_sf(mapping = aes(fill = class3), linewidth = 0.01) + coord_sf() + scale_fill_manual(values = c(&#39;#228833&#39;, &#39;#aa3377&#39;, &#39;#ccbb44&#39;), name = &#39;Species Class&#39;, na.translate = F) + theme_void(base_size = 30) + ggtitle(&#39;Imputed Three Functional \\nGroup Classification (GRM)&#39;) + theme(plot.title = element_text(hjust = 0.5)) p2 &lt;- ggplot(fri_sf) + geom_sf(mapping = aes(fill = class3), linewidth = 0.01) + coord_sf() + scale_fill_manual(values = c(&#39;#228833&#39;, &#39;#aa3377&#39;, &#39;#ccbb44&#39;), name = &#39;Species Class&#39;, na.translate = F) + theme_void(base_size = 30) + ggtitle(&#39;Three Functional \\nGroup Classification (FRI)&#39;) + theme(plot.title = element_text(hjust = 0.5)) grid.arrange(p1, p2, ncol = 1) We can observe a similar distribution of species classes. Five functional group classification: # plot five func group classification p1 &lt;- ggplot(grm_sf) + geom_sf(mapping = aes(fill = class5), linewidth = 0.01) + coord_sf() + scale_fill_manual(values = c(&#39;#ccbb44&#39;, &#39;#228833&#39;, &#39;#4477aa&#39;, &#39;#ee6677&#39;, &#39;#aa3377&#39;), name = &#39;Species Class&#39;, na.translate = F) + theme_void(base_size = 30) + ggtitle(&#39;Imputed Five Functional \\nGroup Classification (GRM)&#39;) + theme(plot.title = element_text(hjust = 0.5)) p2 &lt;- ggplot(fri_sf) + geom_sf(mapping = aes(fill = class5), linewidth = 0.01) + coord_sf() + scale_fill_manual(values = c(&#39;#ccbb44&#39;, &#39;#228833&#39;, &#39;#4477aa&#39;, &#39;#ee6677&#39;, &#39;#aa3377&#39;), name = &#39;Species Class&#39;, na.translate = F) + theme_void(base_size = 30) + ggtitle(&#39;Five Functional \\nGroup Classification (FRI)&#39;) + theme(plot.title = element_text(hjust = 0.5)) grid.arrange(p1, p2, ncol = 1) Overall distribution of three functional group classification: # create data frame for GRM 3 classes dat_grm_c3 &lt;- dat_grm %&gt;% tabyl(class3) %&gt;% filter(is.na(class3) == F) %&gt;% arrange(desc(class3)) %&gt;% mutate(prop = n / sum(.$n)*100) %&gt;% mutate(ypos = cumsum(prop) - 0.5*prop) %&gt;% mutate(lbl = round(prop)) # create data frame for FRI 3 classes dat_fri_c3 &lt;- poly_fri %&gt;% as.data.frame %&gt;% tabyl(class3) %&gt;% filter(is.na(class3) == F) %&gt;% arrange(desc(class3)) %&gt;% mutate(prop = n / sum(.$n)*100) %&gt;% mutate(ypos = cumsum(prop) - 0.5*prop) %&gt;% mutate(lbl = round(prop)) # plot p1 &lt;- ggplot(dat_grm_c3, aes(x = &quot;&quot;, y = prop, fill = class3)) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start = 0) + theme_void() + geom_text(aes(y = ypos, label = str_c(lbl, &quot;%&quot;)), size = 15) + theme(legend.title = element_text(size = 30), legend.text = element_text(size = 20), legend.key.width = unit(2, &#39;cm&#39;), plot.title = element_text(size=30)) + scale_fill_manual(values = c(&#39;#228833&#39;, &#39;#aa3377&#39;, &#39;#ccbb44&#39;)) + labs(fill = &quot;&quot;) + ggtitle(&quot;Imputed Three Functional \\nGroup Classification (GRM)&quot;) p2 &lt;- ggplot(dat_fri_c3, aes(x = &quot;&quot;, y = prop, fill = class3)) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start = 0) + theme_void() + geom_text(aes(y = ypos, label = str_c(lbl, &quot;%&quot;)), size = 15) + theme(legend.title = element_text(size = 30), legend.text = element_text(size = 20), legend.key.width = unit(2, &#39;cm&#39;), plot.title = element_text(size=30)) + scale_fill_manual(values = c(&#39;#228833&#39;, &#39;#aa3377&#39;, &#39;#ccbb44&#39;)) + labs(fill = &quot;&quot;) + ggtitle(&quot;Three Functional Group \\nClassification (FRI)&quot;) grid.arrange(p1, p2, ncol = 2) The distribution of three functional groups is very similar with slightly more softwood in the imputed data. Five functional group classification: # distribution of 5 species classes # create data frame for GRM 5 classes dat_grm_c5 &lt;- dat_grm %&gt;% tabyl(class5) %&gt;% filter(is.na(class5) == F) %&gt;% arrange(desc(class5)) %&gt;% mutate(prop = n / sum(.$n)*100) %&gt;% mutate(ypos = cumsum(prop) - 0.5*prop) %&gt;% mutate(lbl = round(prop)) # create data frame for FRI 5 classes dat_fri_c5 &lt;- poly_fri %&gt;% as.data.frame %&gt;% tabyl(class5) %&gt;% filter(is.na(class5) == F) %&gt;% arrange(desc(class5)) %&gt;% mutate(prop = n / sum(.$n)*100) %&gt;% mutate(ypos = cumsum(prop) - 0.5*prop) %&gt;% mutate(lbl = round(prop)) # plot p1 &lt;- ggplot(dat_grm_c5, aes(x = &quot;&quot;, y = prop, fill = class5)) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start = 0) + theme_void() + geom_text(aes(y = ypos, label = str_c(lbl, &quot;%&quot;)), size = 15) + theme(legend.title = element_text(size = 30), legend.text = element_text(size = 20), legend.key.width = unit(2, &#39;cm&#39;), plot.title = element_text(size=30)) + scale_fill_manual(values = c(&#39;#ccbb44&#39;, &#39;#228833&#39;, &#39;#4477aa&#39;, &#39;#ee6677&#39;, &#39;#aa3377&#39;)) + labs(fill = &quot;&quot;) + ggtitle(&quot;Imputed Five Functional \\nGroup Classification (GRM)&quot;) p2 &lt;- ggplot(dat_fri_c5, aes(x = &quot;&quot;, y = prop, fill = class5)) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start = 0) + theme_void() + geom_text(aes(y = ypos, label = str_c(lbl, &quot;%&quot;)), size = 15) + theme(legend.title = element_text(size = 30), legend.text = element_text(size = 20), legend.key.width = unit(2, &#39;cm&#39;), plot.title = element_text(size=30)) + scale_fill_manual(values = c(&#39;#ccbb44&#39;, &#39;#228833&#39;, &#39;#4477aa&#39;, &#39;#ee6677&#39;, &#39;#aa3377&#39;)) + labs(fill = &quot;&quot;) + ggtitle(&quot;Five Functional Group \\nClassification (FRI)&quot;) grid.arrange(p1, p2, ncol = 2) The distribution of imputed five classes of species is also very similar to the FRI distribution. The imputed values contain slightly more black spruce dominated stands (2% more than the FRI), hardwood stands (1% more than the FRI), and less jack pine dominated and mixed conifer stands. 5.16 Caveats We only used ALS variables derived directly from the point cloud, as in our performance analysis these variables performed slightly better than EFI attributes modelled using an area-based approach. That being said, the performance difference was minimal, as many variables are highly correlated and thus interchangable. In terms of number of X-variables used in imputation, our performance analysis showed a small difference between n = 5 and n = 7. We stuck with n = 7 that performed slightly better. N = 3 performed much poorer. We used k = 5 as it performed better than k = 3 and k = 1 yet still resulted in an adequate distribution of variables (higher values of k lead to less variability in the imputed attributes). Although we compared imputed and observed values across the FRI data, we were limited in our ability to quantify error in relation to ground truth. Little work has investigated the accuracy of FRI species and age attributes. Accuracy depends on the complexity of the forest ecosystem and interpretation procedures (Tompalski et al., 2021). Thus implications will vary across areas, and also based on harvesting and management priorities. Across the RMF we found similar distributions of age and species between imputed and observed values, even with the GRM data containing ~70% more polygons. Overall distributions are important for forest planning and can even out across the landscape even if individual forest stands do not match imputed and observed values (Thompson et al., 2007). 5.17 Future Work Future work should explore the performance of imputation across diverse landscapes and scales. In addition, ground plots could be used to assess the relationship between ground truth, FRI polygon attributes, and newly segmented GRM polygon attributes. 5.18 Summary This tutorial has demonstrated a novel approach to update forest stand polygons and populate them with important forest attributes derived from both expert interpretation of aerial photography and ALS point clouds. The imputation approach is open-source, reproducible, and scalable to meet the needs of operational and strategic planning at various levels. "],["references.html", "6 References", " 6 References Baatz, M., Schäpe, A., 2000. Multiresolution Segmentation: an optimization approach for high quality multi-scale image segmentation. Bilyk, A., Pulkki, R., Shahi, C., Larocque, G.R., 2021. Development of the ontario forest resources inventory: A historical review. Canadian Journal of Forest Research 51, 198–209. https://doi.org/10.1139/cjfr-2020-0234 Blaschke, T., 2010. Object based image analysis for remote sensing. ISPRS Journal of Photogrammetry and Remote Sensing 65, 2–16. https://doi.org/10.1016/j.isprsjprs.2009.06.004 Coops, N.C., Tompalski, P., Goodbody, T.R.H., Queinnec, M., Luther, J.E., Bolton, D.K., White, J.C., Wulder, M.A., van Lier, O.R., Hermosilla, T., 2021. Modelling lidar-derived estimates of forest attributes over space and time: A review of approaches and future trends. Remote Sens Environ 260, 112477. https://doi.org/10.1016/j.rse.2021.112477 Dechesne, C., Mallet, C., le Bris, A., Gouet-Brunet, V., 2017. Semantic segmentation of forest stands of pure species combining airborne lidar data and very high resolution multispectral imagery. ISPRS Journal of Photogrammetry and Remote Sensing 126, 129–145. https://doi.org/10.1016/j.isprsjprs.2017.02.011 Eskelson, B.N.I., Temesgen, H., Lemay, V., Barrett, T.M., Crookston, N.L., Hudak, A.T., 2009. The roles of nearest neighbor methods in imputing missing data in forest inventory and monitoring databases. Scand J For Res. https://doi.org/10.1080/02827580902870490 Pukkala, T., 2020. Delineating forest stands from grid data. For Ecosyst 7. https://doi.org/10.1186/s40663-020-00221-8 Queinnec, M., Coops, N.C., White, J.C., Griess, V.C., Schwartz, N.B., McCartney, G., 2022. Mapping Dominant Boreal Tree Species Groups by Combining Area-Based and Individual Tree Crown LiDAR Metrics with Sentinel-2 Data. Canadian Journal of Remote Sensing. https://doi.org/10.1080/07038992.2022.2130742 Thompson, I.D., Maher, S.C., Rouillard, D.P., Fryxell, J.M., Baker, J.A., 2007. Accuracy of forest inventory mapping: Some implications for boreal forest management. For Ecol Manage 252, 208–221. https://doi.org/10.1016/j.foreco.2007.06.033 Tompalski, P., White, J.C., Coops, N.C., Wulder, M.A., Leboeuf, A., Sinclair, I., Butson, C.R., Lemonde, M.O., 2021. Quantifying the precision of forest stand height and canopy cover estimates derived from air photo interpretation. Forestry 94, 611–629. https://doi.org/10.1093/forestry/cpab022 White, J.C., Coops, N.C., Wulder, M.A., Vastaranta, M., Hilker, T., Tompalski, P., 2016. Remote Sensing Technologies for Enhancing Forest Inventories: A Review. Canadian Journal of Remote Sensing 42, 619–641. https://doi.org/10.1080/07038992.2016.1207484 Woods, M., Pitt, D., Penner, M., Lim, K., Nesbitt, D., Etheridge, D., Treitz, P., 2011. Operational implementation of a LiDAR inventory in Boreal Ontario. Wulder, M.A., White, J.C., Hay, G.J., Castilla, G., 2008. Towards automated segmentation of forest inventory polygons on high spatial resolution satellite imagery. Forestry Chronicle 84, 221–230. https://doi.org/10.5558/tfc84221-2 "]]
