[["index.html", "Integration of Photo Interpreted and LiDAR Attributes into a Polygonal Forest Inventory Framework Overview Project Summary Project Partners", " Integration of Photo Interpreted and LiDAR Attributes into a Polygonal Forest Inventory Framework Ethan Berman Research Scientist Integrated Remote Sensing Studio University of British Columbia 2023-03-22 Overview Welcome! This website serves as a platform to introduce the Integration of Photo Interpreted and LiDAR Attributes into a Polygonal Forest Inventory Framework project and provide updates as progress is made. The project is led by Prof. Nicholas Coops and made possible through a Forestry Futures Trust Ontario grant. Project Summary The acquisition of Single Photon Lidar (SPL) over the forested area of Ontario is redefining how forest attributes are predicted and monitored throughout the Province. A key question remains however, of how to aggregate these area-based (raster) estimates of forest attributes into traditional strategic or tactical-level inventory polygons. This project is designed to address this need. Outcomes of the project will be open source segmentation and attribute prediction tools to develop a polygon based eFRI inclusive of both SPL and interpreted forest stand attributes as well as knowledge transfer activities and demonstration at a number of forest management units. Project Partners This project is principally a partnership between the University of British Columbia, GreenFirst Forest Products, the Ontario Ministry Natural Resources and Forestry, and Forestry Futures Trust Ontario. GreenFirst Forest Products (previously RYAM Forest Management) and the University of British Columbia have a history of successful collaboration including the use of LiDAR and aerial photogrammetry for spruce budworm assessment and regeneration mapping. This project will also build upon the very strong links UBC has with the Canadian Wood Fiber Centre (CWFC), which has thus far culminated in more than 20 peer-reviewed publications on CWFC-funded work, including the award winning best practices guide. In addition, UBC, GreenFirst Forest Products and the Ontario Ministry of Natural Resources and Forestry are involved in a large NSERC funded project, Silv@21, which is a collaborative research program across Canada focused on developing and applying new silvicultural approaches to forest management under changing forestry demand and markets, climate and diverse public expectations. The questions addressed throughout this project are beyond the scope of Silv@21 and therefore an additional effort was needed. Working with the province and industry with advanced remote sensing data sources such as SPL, ensures that the new eFRI provides detailed information on forest composition and structure to inform how much wood can be harvested sustainably across the province. By developing tools which allow for integration of raster-based forest inventory attributes from the SPL and utilizing the recognized benefits of the FRI polygonal inventory, Forestry Futures Trust Ontario has the opportunity to advance forest inventory within Ontario in a significant way. Issues around data management, modeling and fusion of these datasets together into one integrated accurate and operational inventory is of critical importance. This project provides open access tools and methodologies to allow this to be done. The partnerships assembled through private industry, government and universities to address the needs highlighted in this project are critically important and are in direct support of the new Ontario Forest Sector strategy and the Crown Forest Sustainability Act by providing foundational information for economically sustainable forest management planning and spatial planning in the Province. "],["people.html", "People University of British Columbia GreenFirst Forest Products Ontario Ministry of Natural Resources and Forestry", " People This project would not be possible without dedicated collaborators. University of British Columbia Prof. Nicholas Coops, project lead Professor Nicholas Coops holds a Canada Research Chair in Remote Sensing (Tier 1) at UBC. He has published &gt;460 total referenced peer-reviewed journal publications and is internationally recognised as a scientific leader in the field of remote sensing. He was the principal investigator of the AWARE project, a 5-year research project focused on developing LiDAR applications in Canada for forestry applications. He is a co-author of the Canadian Forest Service LiDAR best practice guide series, the most downloaded CFS information handbook ever, focused on LiDAR data processing. In 2020 he was the co-recipient of the Marcus Wallenberg prize for scientific achievements contributing to significantly broadening knowledge and technical development within the field of Forestry. Coops has lead a previous, successful Forestry Futures Trust Ontario grant investigating the application of SPL data for forest inventory and plot stratification in Ontario. Responsibilities As the principal investigator, Coops will manage the budget and supervise Ethan Berman, the research scientist working on the project. At the completion of the project Berman will ensure the software is suitable for open and public release, allowing the methods to be applied to other forest management areas in Ontario covered by SPL data and to be used by Provincial staff. Ethan Berman, research scientist Ethan Berman is a research scientist in the Integrated Remote Sensing Studio (IRSS) at UBC. He has a background in mathematics and remote sensing, and has worked extensively with large spatial datasets, developing novel approaches and building models to answer questions relating to forests, snow, vegetation, and climate change. Ethan received his MSc in 2019 under the supervision of Prof. Coops and before starting work on this project was a consultant for the United States Geological Survey, constructing, curating, and managing spatial datasets for a team of ecologists. Responsibilities Berman will be responsible for data management, algorithm development and testing, and delivery of open source software tools to the project partners. He will be supervised by Prof. Coops at UBC. GreenFirst Forest Products Chris McDonell, partner Chris McDonell is the Chief Forester for Ontario at GreenFirst Forest Products. He is accountable for relations between GreenFirst and First Nation and Metis communities in Ontario and Quebec. He is also the coordinator of forest certification standard implementation (FSC) and liaison with various public, private and community organizations. Grant McCartney, partner Grant McCartney was a spatial analyst and forest information systems coordinator with RYAM Forest Management (now GreenFirst Forest Products). He recently transitioned to a role at Forsite Consultants. He performed spatial analysis in support of forest management planning (FMP) and forestry operations on the Gordon Cosens, Romeo Malette and Martel – Magpie Forests in Northeastern Ontario, Canada and led the acquisition of remotely sensed data including Digital Aerial Photogrammetry (DAP) and LiDAR for RYAM. He has been an industry partner in numerous research efforts including: the Assessment of Wood Attributes for REmote sensing (AWARE) project, CWFC – FIP , past FFT - KKTD projects and the recently approved Silv@21. Responsibilities We will work with GreenFirst Forest Products at a number of their forest management areas where FRI data and EFI data exist. GreenFirst partners will provide plot data, SPL and any other relevant datasets over the areas to ensure accurate model development. They will also attend project meetings, assess model accuracy with GreenFirst staff and test developed software as required. Ontario Ministry of Natural Resources and Forestry Ian Sinclair, partner Ian Sinclair is a terrestrial landscape analyst with the Ontario Ministry of Natural Resources and Forestry, where he is responsible for the development of remotely sensed inventory and landscape level monitoring and reporting programs. His focus includes the development of LiDAR attributes and remotely sensed applications to support a continuous forest inventory program. Ian also coordinates acquisition of the Ministry’s SPL program and supports the operational aspects of the Vegetation Sampling Network Protocol for development of LiDAR based forest inventories. Geordie Robere-McGugan, partner Geordie Robere-McGugan is the Inventory Development Specialist at the Ontario Ministry of Natural Resources and Forestry and the lead on the modernization of the Forest Resources Inventory ensuring the inventory products meet the Provincial Policy Framework. Responsibilities Sinclair and Robere-McGugan have significant expertise in the FRI mapping process, SPL data capture and inventory development. Including government partners on the project team ensures that the outcomes will have immediate uptake by other users within the Ministry and there will be a successful technology transfer and knowledge exchange. Sinclair and Robere-McGugan will be available for project update meetings and to give advice to the research team. They will also be available to test versions of the developed open source code and help ensure documentation is easy to access and appropriate. "],["project-details.html", "Project Details Objective Themes Description Design Methodology Schedule Knowledge &amp; Technology Transfer", " Project Details Objective The objective of this project is to develop open source tools to integrate current EFI SPL-based forest inventory attributes with pre-existing Ontario-wide Forest Resources Inventory data (FRI) to map key forest attributes of interest (volume, height, basal area, density, species composition and group, age and micro-site productivity) in a polygonal format, over entire forest management areas in Ontario. Themes Enhance the production of the Enhanced Forest Resource Inventory (eFRI) Integrate and refine photo interpreted attributes and LiDAR Develop object-based image assessment techniques to use with LiDAR for species identification Deliver open source code and methods Description A key requirement of sustainable forest management is the establishment and maintenance of forest inventories to provide accurate and timely information on the state of the forest to support a variety of purposes and information needs. In Ontario, where extensive forest management practices dominate, forest inventories are derived from a multi-stage process that involves acquiring aerial photography, using the photos to delineate homogenous units or forest stands, and then interpreting attributes from the photography for those delineated stands, often with the aid of stereo vision. Due to the capability of airborne Single Photon LiDAR (SPL) to provide a detailed three-dimensional view of forest structure, the technology and related analysis approaches have transformed the derivation of forest inventories. Attributes such as volume, biomass, diameter, and basal area as well as information on canopy cover and its vertical distribution have all been shown to meet or exceed accuracy requirements in operational forest management with relative root mean square errors (%RMSE) typically lower than 20%. A key issue in utilising SPL data to derive forest inventory information is that a number of forest attributes required to provide the full complement of variables necessary for strategic-level forest inventories such as species group, composition, stand age, and site productivity, are not well predicted from SPL data alone, due to the lack of spectral and other information in the returned point cloud. A second issue is that the development of Enhanced Forest Inventories (EFI) using the area-based approaches (ABA) results in raster-based predictions of forest attributes. While these raters are produced at a fine grid cell (typically 20 x 20 m), most forest management decision making processes, such as harvest planning and scheduling, growth and yield estimation and longer term estate planning relies on polygonal data structures, such as those manually derived from photo interpreters. As a result, two gaps are evident when introducing SPL-based inventories into a forest management framework for strategic and tactical decision making. First, raster based EFI information needs to be transformed into polygonal structures and second, new approaches are needed to derive the attributes required for sustainable forest management decision making that are not readily predicted from SPL data alone. The goal of this project is to develop and apply an open source methodological framework that combines raster EFI predictions of stand structure, with environmental and satellite data to produce comprehensive polygonal forest inventory information in a spatially exhaustive way, over large forested areas of Ontario. These new layers will be polygon based, with the full set of required attributes, at a scale comparable to existing photo-interpreted inventory, to ensure they are compatible with subsequent forest growth and yield, harvest and estate planning software and activities. Design We propose three forest management units as focus sites for this research. The Romeo Malette Forest, which is an actively managed forest by GreenFirst Forest Products and is a typical example of a boreal forest management unit in this region. Forest harvesting practices result in an array of stand development stages which are associated with a range of vegetation structures. A second focus site will be forest management areas within the Great Lakes forest FMU, including French Severn forest or Algonquin Provincial Park. These forest areas have distinct and complementary ecologies, resulting in a range of species and structures not evident in the northern boreal. Likewise additional datasets to utilise in the model development will be different, resulting in different approaches than the methodology developed for boreal systems. If time and resources are available we would apply the approach in the second year at a third site to ensure the approaches has been applied to all three regions. This would provide the Ministry with worked examples to aid tech/knowledge transfer at the regional level and testing operational scalability across Ontario. Methodology First, image segmentation (or object based image analysis) will be performed on key EFI-derived raster attributes of stand (Lorey’s) height and canopy cover to derive structurally-homogeneous micro-stand objects. We will use newly developed open source segmentation tools, specifically developed for forestry LiDAR data to produce wall-to-wall polygon representations of EFI raster layers. Second, the boundaries of these micro-stand objects will be used to extract information from other EFI layers (volume, basal area, crown closure) transforming them from rasters into attributed micro-stand polygons. Microstands will then be cleaned and merged based on size constraints and differences within and between segments (such as between vs within variations in volume and basal area). We will utilise extensive information already produced by the Ontario MNR on Ontario Forest Resources Inventory Photo Interpretation Specifications which state key area, perimeter and quality control standards for polygon delineation to meet Ontario forestry needs. These standards will be used to ensure that the derived polygons from the raster EFI’s will closely resemble the current FRI polygons in terms of area, shape and perimeter distributions. In the third step we will develop an imputation-based approach to derive the remaining attributes not readily predicted from LIDAR including age, species group / species composition or forest unit, and site index. We will utilise the existing Ontario FRI polygons within the forest management area. For each FRI polygon we will extract positional (latitude / longitude), terrain, and climatic information from auxiliary 30 m environmental data. We will also access considerable work completed and underway mapping forest age from historical Landsat data and spatial databases of fire, harvesting, and road disturbances compiled as part of the Boreal Disturbance Database. If available, this type of information will also be compiled and attributed into each delineated polygon. We will then develop an imputation reference dataset which links the four desired attributes to both the structural attributes within the FRI (volume, density, basal area and height), as well as the location, terrain and climate data. This will produce a reference library of every combination of structural and compositional conditions. In the final step, we will impute the additional forest inventory attributes for each SPL derived micro-stand including age, species group / composition, and site index combination for that given stand based on its nearest neighbours in attribute space. We will also work with other researchers funded by KTTD investigating the use of SPL data for additional attributes such as soils prediction to ensure consistency in the use of environmental attributes. We will assess accuracy in a number of ways. For delineation of the micro-stands themselves, we will use GIS spatial overlap algorithms to assess degree of spatial coherence between manually-delineated polygons vs the automatically delineated stands micro-stands from the SPL. For imputation, we will assess agreement using independent validation samples of a hold back of FRI polygons and compare the imputation results with the manually interpreted species, age and site index assessments. Schedule Project Dates June – December 2021: Compile all available SPL data and plot data. Build necessary EFI at each management area. Apply segmentation approaches on raster predictions. MILESTONE: Compiled SPL and EFI datasets over study areas. January – May 2022: Refinement of polygon size and shape based on Provincial input. Development of models for imputation of non-structure variables like species. MILESTONE: Segmentation / object based Algorithm refined, Species / composition databases developed. June – December 2022: Full model development of non-structure attributes. Imputation approach applied over all sites. Field program undertaken at key sites to ensure accuracy of predictions, visits to unusual stand conditions to verify predictions. MILESTONE: Full algorithm testing, Accuracy assessment. January – May 2023: Workshops, code demos, open source code packaging / delivery to all partners. Final validation, Final inventory polygon coverages provided to partners. MILESTONE: Open source code, workshops, peer reviewed papers. Deliverables Key deliverables for this project are: 15 December 2021: Digital Layers Compiled SPL and EFI datasets over study areas. Provided to ministry and industry staff. 15 May 2022: Object based segmentation approach with validation. Code available for broader scale testing and applications. 15 December 2022: Imputation draft paper developed. Imputation code developed ready for testing. 15 May 2023: Open source code release. Workshop for industry and government participants. Peer reviewed papers on approach. Knowledge &amp; Technology Transfer This project is designed to support research, development and technology transfer in the use of transformation technologies such as innovative remote sensing and environmental datasets such as SPL for forest management across Canada’s forest sector. Specifically this project is designed to support the Provincial government of Ontario to improve the accuracy of the forest inventory through the innovative use of SPL data, as well as exploiting the available information on species and structure in the current FRI. By undertaking these activities we contribute to the ongoing transformation of the forest sector through the development and adoption of innovative science-based solutions in particular by linking to the new Ontario governments Forest Sector Strategy. As the application of SPL and other 3-dimensional data to forest inventories becomes more common, the need for joint projects with industry is required to communicate these successes and caveats of these technologies and their appropriate application across Canada. The ultimate outcomes of this project will be a workforce that is more informed on the use of remote sensing technologies for next generation forest inventory applications. We will work with Ontario MNR FRI staff, and GreenFirst Forest Products managers on a number of key methods of outcome dissemination. These will include: Peer-reviewed scientific publications. A series of online workshops on the use of methods for generating the micro-stands and the additional attribution approaches. These will be regional, as needed, to ensure the developed approaches are consistent with regionally specific datasets and needs. Workshops will also be designed to focus on industry forest practitioners, as needed. A technical memo describing the methods developed for key technical staff as it is these staff that are applying these tools themselves as well as for consulting firms which can also be hired by smaller SFLs to implement solutions. Open access to R and other developed programming scripts to undertake the modeling and comparisons. More broadly key outputs of this project will be an increased awareness and capacity building of operational forest managers on the use of these technologies for next generation forest inventory which will lead to an increased ability to dissolve current boundaries between operational (tactical) and strategic (forest level) inventories. In addition, more thorough adoption of SPL technologies as a critical component of forest inventories will benefit other Canadian industries such as survey providers and technology developers by increasing the market and consolidating their international competitiveness, key goals of the Ontario Forest Sector strategy. "],["segmentation-example.html", "Segmentation Example Intro Create Multi-band Raster Run Mean Shift Segmentation Compare Datasets", " Segmentation Example Intro This page provides an example of the image segmentation approach used to derive polygonal forest stands from LiDAR-based raster data. The example data provided is from the Romeo Malette Forest (RMF). First, a three band raster at 20 m spatial resolution is generated from Enhanced Forest Inventory (EFI) metrics. The three bands comprise Lorey’s height, canopy cover (% cover above 2m) and coefficient of variation (standard deviation of LiDAR return heights divided by mean). These bands are re-scaled to comprise values from 0-100, setting the minimum and maximum values to be the 1st and 99th percentile of data observations. Large roads and waterbodies are also masked to prevent them from being included in forest stand polygons. Second, the mean shift algorithm is used to derive forest stand polygons from the input raster. The algorithm used is the LargeScaleMeanShift function from Orfeo ToolBox. Mean Shift is a robust and efficient segmentation algorithm that allows users to provide several parameters to ensure the output polygons meet certain specifications (input parameters described in above link). Although this example only runs the segmentation over a small part of the RMF, segmentation of the entire RMF only takes around 1.5 hrs on a single desktop machine. Third, the output segmentation is compared to polygons derived from manual interpretation. Create Multi-band Raster # load packages library(terra) library(meanShiftR) library(tidyverse) # load file names of SPL rasters to stack lor &lt;- &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/ABA layers SPL 2018/RMF_20m_T130cm_lor.tif&#39; cc &lt;- &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_2m_cov.tif&#39; cv &lt;- &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_cv.tif&#39; # stack rasters spl &lt;- rast(c(lor, cc, cv)) # load RMF shapefile along with sample to be used for example # reproject to match raster rmf &lt;- vect(&#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Land ownership/RMF_Ownership.shp&#39;) %&gt;% project(., spl) rmf_samp &lt;- vect(&#39;D:/ontario_inventory/romeo/RMF_Sample.shp&#39;) %&gt;% project(., spl) # plot RMF overlaid with example plot(rmf, col = &#39;grey&#39;, main = &#39;Romeo Malette Forest with Sample Area in Red&#39;) plot(rmf_samp, col = &#39;red&#39;, add = T) # crop raster stack to sample area spl &lt;- crop(spl, rmf_samp) # apply smoothing function on 5 cell square spl[[1]] &lt;- focal(spl[[1]], w=5, fun=&quot;mean&quot;) spl[[2]] &lt;- focal(spl[[2]], w=5, fun=&quot;mean&quot;) spl[[3]] &lt;- focal(spl[[3]], w=5, fun=&quot;mean&quot;) # load roads and waterbodies roads &lt;- vect(&#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Roads/RMF_roads.shp&#39;) %&gt;% project(., spl) waterb &lt;- vect(&#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Lakes and rivers/RMF_waterbodies.shp&#39;) %&gt;% project(., spl) # subset roads to only mask the main types used by interpreter # RDTYPE = H, P, B roads &lt;- roads[roads$RDTYPE %in% c(&#39;H&#39;, &#39;P&#39;, &#39;B&#39;),] # mask road and water body pixels to NA spl &lt;- spl %&gt;% mask(., roads, inverse = T) %&gt;% mask(., waterb, inverse = T) # if any band is missing values set all to NA spl[is.na(spl[[1]])] &lt;- NA spl[is.na(spl[[2]])] &lt;- NA spl[is.na(spl[[3]])] &lt;- NA # create function to rescale values from 0 to 100 using 1 and 99 percentile scale_100 &lt;- function(x){ # calculate 1st and 99th percentile of input raster perc &lt;- values(x, mat=F) %&gt;% quantile(., probs=c(0.01, 0.99), na.rm=T) # rescale raster using 1st and 99th % x &lt;- (x-perc[1])/(perc[2] - perc[1]) * 100 #reset values below 0 and above 100 x[x &lt; 0] &lt;- 0 x[x &gt; 100] &lt;- 100 return(x) } # rescale rasters from 0 to 100 spl[[1]] &lt;- scale_100(spl[[1]]) spl[[2]] &lt;- scale_100(spl[[2]]) spl[[3]] &lt;- scale_100(spl[[3]]) # plot raster layers plot(spl[[1]], main = &quot;Lorey&#39;s Height (scaled 0-100)&quot;) plot(spl[[2]], main = &quot;Canopy Cover (scaled 0-100)&quot;) plot(spl[[3]], main = &quot;Coefficient of Variation (scaled 0-100)&quot;) # write raster to tif writeRaster(spl, filename=&#39;D:/ontario_inventory/segmentation_ex/spl_stack.tif&#39;, overwrite=T) Run Mean Shift Segmentation # set working directory where temp files will be output setwd(&#39;D:/temp&#39;) # create function to run mean shift meanshift_otb &lt;- function(otb_path = &quot;&quot;, raster_in = &quot;&quot;, out_path = &quot;&quot;, name =&quot;&quot;, spatialr = &quot;10&quot;, ranger = &quot;10&quot;, minsize = &quot;100&quot;, tilesizex = &quot;500&quot;, tilesizey = &quot;500&quot;, outmode = &quot;vector&quot;, cleanup = &quot;true&quot;, ram = &quot;256&quot;){ # Set configuration conf &lt;- paste(&quot;-in&quot;, raster_in, &quot;-spatialr&quot;, spatialr, &quot;-ranger&quot;, ranger, &quot;-minsize&quot;, minsize, &quot;-tilesizex&quot;, tilesizex, &quot;-tilesizey&quot;, tilesizey, &quot;-mode&quot;, outmode, &quot;-mode.vector.out&quot;, paste(out_path, &quot;/&quot;, name, &quot;.shp&quot;, sep=&quot;&quot;), &quot;-cleanup&quot;, cleanup,&quot;-ram&quot;, ram) # apply function in command line system(paste(otb_path, &quot;/otbcli_LargeScaleMeanShift&quot;, &quot; &quot;, conf, sep=&quot;&quot;), intern=T) # save configuration for further use write.table(x = conf,file = paste(out_path,&quot;/&quot;,name,&quot;_conf.txt&quot;,sep=&quot;&quot;),row.names = F, col.names = F) } # run mean shift meanshift_otb(otb_path = &quot;C:/OTB/bin&quot;, raster_in = &#39;D:/ontario_inventory/segmentation_ex/spl_stack.tif&#39;, out_path = &quot;D:/ontario_inventory/segmentation_ex&quot;, name = &quot;ms_10_10_100&quot;, spatialr = &quot;10&quot;, ranger = &quot;10&quot;, minsize = &quot;100&quot;, ram = &quot;1024&quot;) # since mean shift segments missing values into polygons of 1 pixel, aggregate into single polygon # segmented dataset p &lt;- vect(&#39;D:/ontario_inventory/segmentation_ex/ms_10_10_100.shp&#39;) # subset by polygons that only have one pixel (NA) and polygons that have more p_na &lt;- p[p$nbPixels==1,] p_real &lt;- p[p$nbPixels&gt;1,] # dissolve polygons that only have 1 pixels p2 &lt;- aggregate(p_na, by=&#39;nbPixels&#39;) # add back into single file p3 &lt;- rbind(p_real, p2) # write to file writeVector(p3, &#39;D:/ontario_inventory/segmentation_ex/ms_10_10_100_agg_na.shp&#39;, overwrite = T) Compare Datasets # load base imagery base &lt;- rast(&#39;D:/ontario_inventory/romeo/RMF_Sample_Base.tif&#39;) # load manually interpreted polygons and change to raster proj man_poly &lt;- vect(&#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Polygons Inventory/RMF_PolygonForest.shp&#39;) %&gt;% project(., base) %&gt;% crop(., base) # load mean shift derived polygons and change to raster proj ms_poly &lt;- vect(&#39;D:/ontario_inventory/segmentation_ex/ms_10_10_100_agg_na.shp&#39;) %&gt;% project(., base) %&gt;% crop(., base) # plot base imagery with interpreter polygons plot(man_poly, main = &#39;Polygonal Forest Stands Derived Manually&#39;) plotRGB(base, stretch = &#39;lin&#39;, add = T) plot(man_poly, border = &#39;mediumvioletred&#39;, add = T) The above plot shows the manually derived forest stand polygons in part of the sample area, overlaid on recent true color imagery. Note the clean edges around water bodies, rivers/streams, and road features. # plot base imagery with mean shift polygons plot(ms_poly, main = &#39;Polygonal Forest Stands Derived from EFI Layers using Mean Shift&#39;) plotRGB(base, stretch = &#39;lin&#39;, add = T) plot(ms_poly, border = &#39;red2&#39;, add = T) The above plot shows the mean shift derived forest stand polygons in part of the sample area. Note that the automated segmentation also has clean edges around water bodies and road features, since these features were masked in the first step. Rivers/streams were not masked due to no clear pattern dictating which features manual interpreters included/excluded. The workflow gives the user control over which features in the landscape should be excluded from the segmentation process. Also note that the polygons never reach the same maximum size as the polygons derived manually. The algorithm takes minimum polygon size as an input parameter, but in general does not cluster the landscape into as large polygons. We will continue to tweak the segmentation algorithm until we are satisfied with the polygonal output. We have also begun working on the imputation step of the project and will provide further results as they become available. We look forward to receiving feedback! "],["segmentation-update.html", "Segmentation Update Intro 1. Create Multi-band Raster 2. Run GRM Segmentation 3a. Calculate Summary Statistics 3b. Compare GRM Segmentation Output to FRI", " Segmentation Update Intro This page provides an update of the segmentation approach used to derive polygons from Airborne Laser Scanning (ALS) gridded summary metrics. Based on feedback from our meetings I have now tested three segmentation algorithms to find the best approach to balance a) the visual component of the segmentation (e.g. shape, overlapping parallel lines) with b) the spectral component (outputting polygons with homogeneous values different from nearby polygons). The three segmentation algorithms I tested are: 1. Large Scale Mean Shift (showcased in the initial segmentation example) 2. Generic Region Merging 3. Simple Linear Iterative Clustering This example will run a segmentation of a small area in the Romeo Malette Forest using the Generic Region Merging algorithm, which I found to be the best candidate algorithm for this application. GRM is an open-source region merging algorithm available from Orfeo Toolbox that uses the same segmentation criterion (called the Baatz &amp; Schape criterion) as the subscription based Multi-resolution Segmentation (MRS) from eCognition software, which is the most popular segmentation method for remote sensing applications. I found GRM to be the most appropriate algorithm to derive meaningful and consistent forest stands, due to its ability to easily balance segment shape with spectral homogeneity, an important trade-off to ensure homogeneous forest stands while also maintaining spatial standards needed for forest operations. First, a three band raster at 20 m spatial resolution is generated from gridded ALS metrics. The three bands comprise P95 height (95th percentile of vegetation return heights above 1.3 m), canopy cover (% cover above 2m) and coefficient of variation (standard deviation of LiDAR return heights divided by mean). I initially used Lorey’s Height instead of P95 height, but am now using P95 height because the two variables are highly correlated (Pearson’s correlation = 0.91) and P95 height provides wall-to-wall coverage within ALS transects whereas Lorey’s Height is modelled for a specific area and only valid within forested landcover types. These bands are re-scaled to comprise values from 0-100, setting the minimum and maximum values to be the 1st and 99th percentile of data observations. Large roads and waterbodies are also masked to prevent them from being included in forest stand polygons. These polygons are re-added to the dataset after segmentation. Second, the GRM algorithm is used to derive forest stand polygons from the input raster. I am using the algorithm parameters I found to work best after conducting an iterative sensitivity analysis, which is described in the corresponding paper. Third, the segmentation output is compared to polygons derived from manual interpretation, both visually and with a full suite of summary and performance metrics including: Minimum, maximum, mean, and median polygon size (expressed in number of hectares) Total number of polygons Mean, standard error, and standard deviation of polygon area, perimeter, and shape index R2 to assess the performance of segmentation outputs in explaining variation in the input variables: P95 height, canopy cover, and coefficient of variation 1. Create Multi-band Raster # load packages library(terra) library(tidyverse) library(janitor) library(sf) library(exactextractr) library(lwgeom) library(magrittr) library(gridExtra) ################################################### ### LOAD MULTI BAND ALS RASTER FOR SEGMENTATION ### ################################################### # load file names of SPL rasters to stack p95 &lt;- &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_p95.tif&#39; cc &lt;- &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_2m_cov.tif&#39; cv &lt;- &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_cv.tif&#39; # stack rasters spl &lt;- rast(c(p95, cc, cv)) # load RMF shapefile along with sample to be used for example # reproject to match raster rmf &lt;- vect(&#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Land ownership/RMF_Ownership.shp&#39;) %&gt;% project(., spl) rmf_samp &lt;- vect(&#39;D:/ontario_inventory/romeo/RMF_Sample.shp&#39;) %&gt;% project(., spl) # plot RMF overlaid with example plot(rmf, col = &#39;grey&#39;) plot(rmf_samp, col = &#39;red&#39;, add = T, main = &#39;Romeo Malette Forest with Sample Area in Red&#39;, cex.main = 3) # crop raster stack to sample area spl &lt;- crop(spl, rmf_samp) # apply smoothing function on 5 cell square spl[[1]] &lt;- focal(spl[[1]], w=5, fun=&quot;mean&quot;) spl[[2]] &lt;- focal(spl[[2]], w=5, fun=&quot;mean&quot;) spl[[3]] &lt;- focal(spl[[3]], w=5, fun=&quot;mean&quot;) #################################### ### MASK ROAD AND WATER POLYGONS ### #################################### # create spl template with all values equal to 1 spl_temp &lt;- spl[[1]] spl_temp[] &lt;- 1 # load photo interpreted polygons poly &lt;- vect(&#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Polygons Inventory/RMF_PolygonForest.shp&#39;) # reproject whole FRI to match lidar poly &lt;- project(poly, spl) # crop FRI to sample area poly &lt;- crop(poly, rmf_samp) # subset polygons that are WAT OR UCL poly_sub &lt;- poly[poly$POLYTYPE %in% c(&#39;WAT&#39;, &#39;UCL&#39;)] # loop through water and unclassified polygons, mask raster, and vectorize for(i in seq_along(poly_sub)){ pt &lt;- poly_sub$POLYTYPE[i] if(i == 1){ spl_pt &lt;- spl_temp %&gt;% crop(poly_sub[i], snap = &#39;out&#39;) %&gt;% mask(poly_sub[i], touches = T) npix &lt;- sum(values(spl_pt), na.rm = T) spl_pt &lt;- as.polygons(spl_pt) names(spl_pt) &lt;- &#39;POLYTYPE&#39; spl_pt$POLYTYPE &lt;- pt spl_pt$nbPixels &lt;- npix } else{ spl_hold &lt;- spl_temp %&gt;% crop(poly_sub[i], snap = &#39;out&#39;) %&gt;% mask(poly_sub[i], touches = T) npix &lt;- sum(values(spl_hold), na.rm = T) spl_hold &lt;- as.polygons(spl_hold) names(spl_hold) &lt;- &#39;POLYTYPE&#39; spl_hold$POLYTYPE &lt;- pt spl_hold$nbPixels &lt;- npix spl_pt &lt;- rbind(spl_pt, spl_hold) } } # mask lidar outside of FRI spl &lt;- mask(spl, poly, inverse = F, touches = T) # mask WAT and UCL polygons spl &lt;- mask(spl, poly_sub, inverse = T, touches = T) ########################################## ### DEAL WITH MISSING DATA AND RESCALE ### ########################################## # if any band is missing values set all to NA spl[is.na(spl[[1]])] &lt;- NA spl[is.na(spl[[2]])] &lt;- NA spl[is.na(spl[[3]])] &lt;- NA # create function to rescale values from 0 to 100 using 1 and 99 percentile scale_100 &lt;- function(x){ # calculate 1st and 99th percentile of input raster perc &lt;- values(x, mat=F) %&gt;% quantile(., probs=c(0.01, 0.99), na.rm=T) # rescale raster using 1st and 99th % x &lt;- (x-perc[1])/(perc[2] - perc[1]) * 100 #reset values below 0 and above 100 x[x &lt; 0] &lt;- 0 x[x &gt; 100] &lt;- 100 return(x) } # rescale rasters from 0 to 100 spl[[1]] &lt;- scale_100(spl[[1]]) spl[[2]] &lt;- scale_100(spl[[2]]) spl[[3]] &lt;- scale_100(spl[[3]]) # plot raster layers plot(spl[[1]]) title(main = &quot;P95 Height (scaled 0-100)&quot;, cex.main = 3, line = -0.0001) plot(spl[[2]]) title(main = &quot;Canopy Cover (scaled 0-100)&quot;, cex.main = 3, line = -0.0001) plot(spl[[3]]) title(main = &quot;Coefficient of Variation (scaled 0-100)&quot;, cex.main = 3, line = -0.0001) # write raster to tif writeRaster(spl, filename=&#39;D:/ontario_inventory/segmentation_ex/spl_stack_grm.tif&#39;, overwrite=T) # write masked water and unclassified polygons to shp writeVector(spl_pt, &#39;D:/ontario_inventory/segmentation_ex/wat_ucl_polygons.shp&#39;, overwrite = T) 2. Run GRM Segmentation ######################### ### RUN GRM ALGORITHM ### ######################### # set working directory where temp files will be output setwd(&#39;D:/temp&#39;) # create function to run generic region merging grm_otb &lt;- function(otb_path = &quot;&quot;, raster_in = &quot;&quot;, out_path = &quot;&quot;, name = &quot;&quot;, method = &quot;bs&quot;, thresh = &quot;&quot;, spec = &quot;0.5&quot;, spat = &quot;0.5&quot;){ # Set configuration conf &lt;- paste(&quot;-in&quot;, raster_in, &quot;-out&quot;, paste(out_path, &quot;/&quot;, name, &quot;.tif&quot;, sep=&quot;&quot;), &quot;-criterion&quot;, method, &quot;-threshold&quot;, thresh, &quot;-cw&quot;, spec, &quot;-sw&quot;, spat) # apply function in command line system(paste(otb_path, &quot;/otbcli_GenericRegionMerging&quot;, &quot; &quot;, conf, sep=&quot;&quot;)) # save configuration for further use write.table(x = conf,file = paste(out_path,&quot;/&quot;,name,&quot;_conf.txt&quot;,sep=&quot;&quot;),row.names = F, col.names = F) } # run grm grm_otb(otb_path = &quot;C:/OTB/bin&quot;, raster_in = &quot;D:/ontario_inventory/segmentation_ex/spl_stack_grm.tif&quot;, out_path = &quot;D:/ontario_inventory/segmentation_ex&quot;, name = &quot;grm_ex&quot;, thresh = &quot;10&quot;, spec = &quot;0.1&quot;, spat = &quot;0.5&quot;) ########################### ### MASK MISSING VALUES ### ########################### # load grm segments raster p &lt;- rast(&quot;D:/ontario_inventory/segmentation_ex/grm_ex.tif&quot;) # load single band of raster input for segmentation to use as mask mask &lt;- rast(&quot;D:/ontario_inventory/segmentation_ex/spl_stack_grm.tif&quot;) %&gt;% .[[1]] # mask grm segments raster p &lt;- mask(p, mask) # re-write masked grm raster writeRaster(p, &quot;D:/ontario_inventory/segmentation_ex/grm_ex.tif&quot;, overwrite = T) ################################################## ### CONVERT TO POLYGONS AND CALC POLYGON SIZES ### ################################################## # convert to vector based on cell value vec &lt;- as.polygons(p) # create table of number of pixels in each polygon num &lt;- as.vector(values(p)) num_pix &lt;- tabyl(num) # drop na row num_pix &lt;- na.omit(num_pix) # get pixel ids from vector vec_dat &lt;- tibble(id = values(vec)[,1]) colnames(vec_dat) &lt;- &#39;id&#39; # loop through values and add to vector data vec_dat$nbPixels &lt;- NA for(i in 1:NROW(vec_dat)){ vec_dat$nbPixels[i] &lt;- num_pix$n[num_pix$num == vec_dat$id[i]] } # remove current column of data and add id # add nbPixels to vector vec &lt;- vec[,-1] vec$id &lt;- vec_dat$id vec$nbPixels &lt;- vec_dat$nbPixels 3a. Calculate Summary Statistics ############################################## ### RE-ADD WATER AND UNCLASSIFIED POLYGONS ### ############################################## # load polygon dataset as p p &lt;- vec # reproject segmented polygons to ensure same crs as wat and ucl polygons p &lt;- project(p, spl_pt) # add WAT/UCL POLYTYPE polygons back in p &lt;- rbind(p, spl_pt) ####################################################### ### EXTRACT ALS METRICS AND CALCULATE SUMMARY STATS ### ####################################################### # convert to sf p_sf &lt;- st_as_sf(p) # calculate perimeter p$perim &lt;- st_perimeter(p_sf) %&gt;% as.numeric # calculate area p$area &lt;- st_area(p_sf) %&gt;% as.numeric # calculate mean shape index p$msi &lt;- p$perim/sqrt(pi * p$area) # write to file to backup writeVector(p, &quot;D:/ontario_inventory/segmentation_ex/grm_ex.shp&quot;, overwrite = T) # subset non masked WAT and UCL polygons p2_sf &lt;- p[is.na(p$POLYTYPE)] %&gt;% st_as_sf p2 &lt;- p[is.na(p$POLYTYPE)] %&gt;% as.data.frame # load original raster input file ras &lt;- rast(&quot;D:/ontario_inventory/segmentation_ex/spl_stack_grm.tif&quot;) # rename bands names(ras) &lt;- c(&#39;p95&#39;, &#39;cc&#39;, &#39;cv&#39;) # extract pixel values pvals &lt;- exact_extract(ras, p2_sf) # calculate SSE sse &lt;- sapply(pvals, FUN = function(x){ p95_mean &lt;- mean(x$p95, na.rm = T) cc_mean &lt;- mean(x$cc, na.rm = T) cv_mean &lt;- mean(x$cv, na.rm = T) return(c(sum((x$p95 - p95_mean)^2, na.rm = T), sum((x$cc - cc_mean)^2, na.rm = T), sum((x$cv - cv_mean)^2, na.rm = T))) }) # transpose sse &lt;- t(sse) # calculate final sums sse &lt;- colSums(sse) # unlist values pvals2 &lt;- do.call(rbind, pvals) # calculate global mean values p95_mean &lt;- mean(pvals2$p95, na.rm = T) cc_mean &lt;- mean(pvals2$cc, na.rm = T) cv_mean &lt;- mean(pvals2$cv, na.rm = T) rm(pvals2) # calculate SST sst &lt;- sapply(pvals, FUN = function(x){ return(c(sum((x$p95 - p95_mean)^2, na.rm = T), sum((x$cc - cc_mean)^2, na.rm = T), sum((x$cv - cv_mean)^2, na.rm = T))) }) # transpose sst &lt;- t(sst) # calculate final sums sst &lt;- colSums(sst) # calculate r2 values r2_p95 &lt;- 1 - (sse[1]/sst[1]) %&gt;% round(3) r2_cc &lt;- 1 - (sse[2]/sst[2]) %&gt;% round(3) r2_cv &lt;- 1 - (sse[3]/sst[3]) %&gt;% round(3) r2_all &lt;- (sum(r2_p95, r2_cc, r2_cv) / 3) %&gt;% round(3) # create standard error function se &lt;- function(x){sd(x) / sqrt(length(x))} # create dataframe of all summary stats df &lt;- data.frame( alg = &#39;GRM&#39;, min_pix = (min(p2$nbPixels)), max_pix = (max(p2$nbPixels)), mean_pix = (mean(p2$nbPixels)), med_pix = (median(p2$nbPixels)), num_poly = NROW(p2), mean_area = mean(p2$area), se_area = se(p2$area), sd_area = sd(p2$area), mean_perim = mean(p2$perim), se_perim = se(p2$perim), sd_perim = sd(p2$perim), mean_msi = mean(p2$msi), se_msi = se(p2$msi), sd_msi = sd(p2$msi), r2_p95 &lt;- r2_p95, r2_cc &lt;- r2_cc, r2_cv &lt;- r2_cv, r2_all &lt;- r2_all ) # round numeric columns df %&lt;&gt;% mutate_at(c(&#39;min_pix&#39;, &#39;max_pix&#39;, &#39;mean_pix&#39;, &#39;med_pix&#39;, &#39;mean_area&#39;, &#39;se_area&#39;, &#39;sd_area&#39;, &#39;mean_perim&#39;, &#39;se_perim&#39;, &#39;sd_perim&#39;), function(x) round(x, 2)) %&gt;% mutate_at(c(&#39;mean_msi&#39;, &#39;se_msi&#39;, &#39;sd_msi&#39;), function(x) round(x, 4)) ####################################### ### CALCULATE SUMMARY STATS FOR FRI ### ####################################### # load interpreter derived polygons to extract statistics p &lt;- poly # convert to sf p_sf &lt;- st_as_sf(p) # calculate perimeter p$perim &lt;- st_perimeter(p_sf) %&gt;% as.numeric # calculate area p$area &lt;- st_area(p_sf) %&gt;% as.numeric # calculate msi p$msi &lt;- p$perim/sqrt(pi * p$area) # calculate nbPixels p$nbPixels &lt;- p$area / 400 # write to file to backup writeVector(p, &quot;D:/ontario_inventory/segmentation_ex/fri_ex.shp&quot;, overwrite = T) # subset all non water/ucl polygons p2_sf &lt;- p[!(p$POLYTYPE %in% c(&#39;WAT&#39;, &#39;UCL&#39;))] %&gt;% st_as_sf p2 &lt;- p[!(p$POLYTYPE %in% c(&#39;WAT&#39;, &#39;UCL&#39;))] %&gt;% as.data.frame # extract pixel values pvals &lt;- exact_extract(ras, p2_sf) # calculate SSE sse &lt;- sapply(pvals, FUN = function(x){ # subset values based on coverage fraction x %&lt;&gt;% filter(coverage_fraction &gt;= 0.5) p95_mean &lt;- mean(x$p95, na.rm = T) cc_mean &lt;- mean(x$cc, na.rm = T) cv_mean &lt;- mean(x$cv, na.rm = T) return(c(sum((x$p95 - p95_mean)^2, na.rm = T), sum((x$cc - cc_mean)^2, na.rm = T), sum((x$cv - cv_mean)^2, na.rm = T))) }) # transpose sse &lt;- t(sse) # calculate final sums sse &lt;- colSums(sse) # unlist values pvals2 &lt;- do.call(rbind, pvals) # subset values based on coverage fraction pvals2 %&lt;&gt;% filter(coverage_fraction &gt;= 0.5) # calculate global mean values p95_mean &lt;- mean(pvals2$p95, na.rm = T) cc_mean &lt;- mean(pvals2$cc, na.rm = T) cv_mean &lt;- mean(pvals2$cv, na.rm = T) rm(pvals2) # calculate SST sst &lt;- sapply(pvals, FUN = function(x){ # subset values based on coverage fraction x %&lt;&gt;% filter(coverage_fraction &gt;= 0.5) return(c(sum((x$p95 - p95_mean)^2, na.rm = T), sum((x$cc - cc_mean)^2, na.rm = T), sum((x$cv - cv_mean)^2, na.rm = T))) }) # transpose sst &lt;- t(sst) # calculate final sums sst &lt;- colSums(sst) # calculate r2 values r2_p95 &lt;- 1 - (sse[1]/sst[1]) %&gt;% round(3) r2_cc &lt;- 1 - (sse[2]/sst[2]) %&gt;% round(3) r2_cv &lt;- 1 - (sse[3]/sst[3]) %&gt;% round(3) r2_all &lt;- (sum(r2_p95, r2_cc, r2_cv) / 3) %&gt;% round(3) # create dataframe with values wanted df_fri &lt;- data.frame(alg = &#39;FRI&#39;, min_pix = (min(p2$area / 400)), max_pix = (max(p2$area / 400)), mean_pix = (mean(p2$area / 400)), med_pix = (median(p2$area / 400)), num_poly = NROW(p2), mean_area = mean(p2$area), se_area = se(p2$area), sd_area = sd(p2$area), mean_perim = mean(p2$perim), se_perim = se(p2$perim), sd_perim = sd(p2$perim), mean_msi = mean(p2$msi), se_msi = se(p2$msi), sd_msi = sd(p2$msi), r2_p95 &lt;- r2_p95, r2_cc &lt;- r2_cc, r2_cv &lt;- r2_cv, r2_all &lt;- r2_all) # round numeric columns df_fri %&lt;&gt;% mutate_at(c(&#39;min_pix&#39;, &#39;max_pix&#39;, &#39;mean_pix&#39;, &#39;med_pix&#39;), function(x) round(x)) %&gt;% mutate_at(c(&#39;mean_area&#39;, &#39;se_area&#39;, &#39;sd_area&#39;, &#39;mean_perim&#39;, &#39;se_perim&#39;, &#39;sd_perim&#39;), function(x) round(x, 2)) %&gt;% mutate_at(c(&#39;mean_msi&#39;, &#39;se_msi&#39;, &#39;sd_msi&#39;), function(x) round(x, 4)) # bind df df &lt;- rbind(df, df_fri) # write summary stats to csv as backup write.csv(df, file = &#39;D:/ontario_inventory/segmentation_ex/grm_ex_summary_stats.csv&#39;, row.names = F) 3b. Compare GRM Segmentation Output to FRI # load base imagery base &lt;- rast(&#39;D:/ontario_inventory/romeo/RMF_Sample_Base.tif&#39;) # load FRI and change to raster proj fri_poly &lt;- vect(&quot;D:/ontario_inventory/segmentation_ex/fri_ex.shp&quot;) %&gt;% project(., base) %&gt;% crop(., base) # load GRM polygons and change to raster proj grm_poly &lt;- vect(&quot;D:/ontario_inventory/segmentation_ex/grm_ex.shp&quot;) %&gt;% project(., base) %&gt;% crop(., base) # plot base imagery with FRI polygons plot(fri_poly) plotRGB(base, stretch = &#39;lin&#39;, add = T) plot(fri_poly, border = &#39;mediumvioletred&#39;, lwd = 2, add = T) title(main = &#39;FRI Polygons Overlaid on True Color Image&#39;, line = -2, cex.main = 3) The above plot shows the FRI polygons in part of the sample area, overlaid on recent true color imagery. Note the clean edges around water bodies, rivers/streams, and road features. # plot base imagery with GRM polygons plot(grm_poly) plotRGB(base, stretch = &#39;lin&#39;, add = T) plot(grm_poly, border = &#39;red2&#39;, lwd = 2, add = T) title(main = &#39;GRM Polygons Overlaid on True Color Image&#39;, line = -2, cex.main = 3) The above plot shows the GRM derived polygons in part of the sample area. Note that the automated segmentation also has clean edges around water and road features, since these features were masked in the first step, and the polygons re-added after segmentation. The GRM polygons are also much more compact and uniform than the FRI polygons, and generally do not have issues with the line-work (overlapping and parallel lines), which was an important point mentioned in our meetings. #################################### ### DISTRIBUTION OF POLYGON SIZE ### #################################### # remove wat and ucl polygons and convert to df fri_poly &lt;- fri_poly[!(fri_poly$POLYTYPE %in% c(&#39;WAT&#39;, &#39;UCL&#39;))] %&gt;% as.data.frame grm_poly &lt;- grm_poly[is.na(grm_poly$POLYTYPE)] %&gt;% as.data.frame # plot polygon size FRI in Ha g1 &lt;- ggplot(data.frame(nbPixels = fri_poly$nbPixels/25), aes(x = nbPixels)) + geom_density(fill = &#39;grey&#39;) + xlim(c(0, 500/25)) + ylim(c(0, 0.17)) + geom_vline(aes(xintercept = median(nbPixels)), linetype = &quot;dashed&quot;, size = 0.6) + theme_bw() + xlab(&#39;Number of Hectares&#39;) + ylab(&#39;Density&#39;) + ggtitle(&#39;FRI Distribution of Polygon Size&#39;) + theme(text = element_text(size = 25)) # plot polygon size GRM in Ha g2 &lt;- ggplot(data.frame(nbPixels = grm_poly$nbPixels/25), aes(x = nbPixels)) + geom_density(fill = &#39;grey&#39;) + xlim(c(0, 500/25)) + ylim(c(0, 0.17)) + geom_vline(aes(xintercept = median(nbPixels)), linetype = &quot;dashed&quot;, size = 0.6) + theme_bw() + xlab(&#39;Number of Hectares&#39;) + ylab(&#39;Density&#39;) + ggtitle(&#39;GRM Distribution of Polygon Size&#39;) + theme(text = element_text(size = 25)) # plot together grid.arrange(g1, g2) Above is the distribution of polygon size within the sample area. The dotted line denotes the median values, which are very close between the FRI and GRM polygons. Both distributions have a long tail toward large polygons, but the GRM polygons have a more uniform size and less large polygons. Water and Unclassified polygons are not included. ################################### ### DISTRIBUTION OF SHAPE INDEX ### ################################### # plot shape index FRI g1 &lt;- ggplot(data.frame(msi = as.numeric(fri_poly$msi)), aes(x = msi)) + geom_density(fill = &#39;grey&#39;) + xlim(c(0, 8)) + ylim(c(0, 1.5)) + geom_vline(aes(xintercept = median(msi)), linetype = &quot;dashed&quot;, size = 0.6) + theme_bw() + xlab(&#39;Shape Index&#39;) + ylab(&#39;Density&#39;) + ggtitle(&#39;FRI Distribution of Shape Index&#39;) + theme(text = element_text(size = 25)) # plot shape index GRM g2 &lt;- ggplot(data.frame(msi = as.numeric(grm_poly$msi)), aes(x = msi)) + geom_density(fill = &#39;grey&#39;) + xlim(c(0, 8)) + ylim(c(0, 1.5)) + geom_vline(aes(xintercept = median(msi)), linetype = &quot;dashed&quot;, size = 0.6) + theme_bw() + xlab(&#39;Shape Index&#39;) + ylab(&#39;Density&#39;) + ggtitle(&#39;GRM Distribution of Shape Index&#39;) + theme(text = element_text(size = 25)) # plot together grid.arrange(g1, g2) The above plots show Shape Index for FRI and GRM polygons in the sample area (Water and Unclassified polygons are not included). The dotted line is the median value. Shape Index has a value of 1 for circular objects and increases without limits with increased complexity and irregularity. The GRM polygons have a much more compact shape (lower Shape Index) as well as a more uniform and tight distribution. These characteristics are visible in the above plots of the polygons. ########################################### ### Summary Stats and Additional Tables ### ########################################### # Size t1 &lt;- as.tibble(df[, 1:6]) # change to ha t1[, 2:5] &lt;- round(t1[, 2:5] / 25, 2) names(t1) &lt;- c(&#39;Dataset&#39;, &quot;Min Ha&quot;, &#39;Max Ha&#39;, &quot;Mean Ha&quot;, &quot;Median Ha&quot;, &quot;Number of Polygons&quot;) knitr::kable(t1, caption = &quot;Table 1: Polygon Size Stats&quot;, label = NA) Table 1: Polygon Size Stats Dataset Min Ha Max Ha Mean Ha Median Ha Number of Polygons GRM 0.04 42.84 4.94 4.12 25774 FRI 0.00 184.00 7.56 3.92 18202 Table 1 shows Polygon size stats. The median values are very close, but the mean GRM polygon size is smaller, as is the maximum polygon size. Thus, the GRM output has more polygons overall than the FRI. # Area and Perim t2 &lt;- as.tibble(df[, c(1, 7:12)]) names(t2) &lt;- c(&quot;Dataset&quot;, &quot;Mean Area (m2)&quot;, &quot;SE Area&quot;, &quot;SD Area&quot;, &quot;Mean Perimeter (m)&quot;, &quot;SE Perimeter&quot;, &quot;SD Perimeter&quot;) knitr::kable(t2, caption = &quot;Table 2: Polygon Area and Perimeter Stats&quot;, label = NA) Table 2: Polygon Area and Perimeter Stats Dataset Mean Area (m2) SE Area SD Area Mean Perimeter (m) SE Perimeter SD Perimeter GRM 49366.70 209.81 33682.75 1133.89 2.30 368.70 FRI 75643.27 830.58 112057.39 1664.71 11.87 1600.98 Table 2 shows area and perimeter stats. The mean area and perimeter of GRM polygons are smaller than the FRI. The variation is also smaller, since the GRM polygons are more uniform. # Mean Shape Index t3 &lt;- as.tibble(df[, c(1, 13:15)]) names(t3) &lt;- c(&quot;Dataset&quot;, &quot;Mean Shape Index&quot;, &quot;SE Shape Index&quot;, &quot;SD Shape Index&quot;) knitr::kable(t3, caption = &quot;Table 3: Polygon Shape Index Stats&quot;, label = NA) Table 3: Polygon Shape Index Stats Dataset Mean Shape Index SE Shape Index SD Shape Index GRM 3.0626 0.0028 0.4501 FRI 3.8114 0.0097 1.3145 Table 3 presents Mean Shape Index, an indication of the regularity of polygon shape, with values of 1 representing perfect circles and values increasing without maximum with more complexity. We can see that GRM polygons are less complex (more compact) than FRI polygons and that shape varies less in the GRM output. # R2 t4 &lt;- as.tibble(df[,c(1, 16:19)]) names(t4) &lt;- c(&#39;Dataset&#39;, &#39;R2 P95&#39;, &#39;R2 Can Cov&#39;, &#39;R2 Coeff Var&#39;, &#39;R2 Overall&#39;) knitr::kable(t4, caption = &quot;Table 4: Polygon R2 Stats&quot;, label = NA) Table 4: Polygon R2 Stats Dataset R2 P95 R2 Can Cov R2 Coeff Var R2 Overall GRM 0.894 0.932 0.847 0.891 FRI 0.819 0.861 0.701 0.794 The last table shows R2 values, which give an indication of how well the segmentation output is explaining the variation among ALS input variables. We can see that the GRM output has a high R2 value of 0.89 (overall R2), which is a strong indicator that the segmentation algorithm is effectively explaining structural differences between polygons. The manually segmented FRI polygons also have a very good R2 value, indicating the skill of interpreters in delineating forest structure. "],["imputation.html", "Imputation Intro 1. Extract LiDAR attributes in FRI polygons and GRM polygons 2. Screen FRI polygons to curate an optimal dataset to use for imputation 3. Run imputation on FRI polygons ONLY to assess imputation performance 4. Run imputation between FRI and GRM polygons to derive estimates of age and species composition 5. Conclusion", " Imputation Intro This page provides an example of the imputation approach used to estimate age and species composition in newly-generated forest stands. The imputation is based on a k-nearest neighbor (kNN) algorithm. X-variables (variables that are contiguous across reference and target polygons) are identified and for each Generic Region Merging (GRM) generated forest polygon (the target polygon), the k-nearest neighbor FRI polygons (the reference polygons) that minimize the Euclidean distance of the X-variables are identified and used to impute age and species composition into the target polygon. If k = 1, the age and species variables are imputed directly from the best matching reference polygon. If k &gt; 1, age is imputed as the mean age of kNN polygons and species variables are imputed as the mode across kNN polygons. The basic workflow is as follows: Run GRM Segmentation to derive new polygon dataset (see previous posts) Extract LiDAR attributes in FRI polygons and GRM segmented polygons Screen FRI polygons to curate an optimal dataset to use for imputation Run imputation on FRI polygons ONLY to assess imputation performance Run imputation between FRI and GRM polygons to impute estimates of age and species composition 1. Extract LiDAR attributes in FRI polygons and GRM polygons Before running imputation we need to extract LiDAR and Sentinel 2 attributes in all FRI and GRM polygons. For this example, we only extract attributes used in the optimal imputation model (listed below) as well as the attributes needed for polygon data screening in step 2 of this walk-through. The value extracted for each polygon is the median cell value, weighted by the fraction of each cell that is covered by the polygon. The variables used in the imputation algorithm are as follows, and were selected with guidance from previous works as well as a performance analysis (the details of which will be disclosed in a forthcoming journal article): avg: mean returns height &gt; 1.3 m classified as vegetation sd: standard deviation of returns height &gt; 1.3 m classified as vegetation rumple: ratio of canopy outer surface area to ground surface area zpcum8: cumulative percentage of LiDAR returns found in 80% percentile of LiDAR height x and y: coordinates of polygon centroid (in UTM coordinates) red_edge_2: Sentinel 2 surface reflectance band 6 (740 nm) The following two variables are needed for polygon data screening: p95: 95th percentile of LiDAR height returns &gt; 1.3 m classified as vegetation cc: percent of first returns &gt; 2 meters (canopy cover) 1a. Extract attributes in FRI polygons # load packages library(terra) library(tidyverse) library(exactextractr) library(sf) library(magrittr) library(gridExtra) # load FRI polygons poly &lt;- vect(&#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Polygons Inventory/RMF_PolygonForest.shp&#39;) # convert to df dat_fri &lt;- as.data.frame(poly) # cbind centroids to dat dat_fri &lt;- cbind(dat_fri, centroids(poly) %&gt;% crds) # load LiDAR datasets we need to extract over polygons # create named vector with variable names and data links lidar &lt;- c(&#39;p95&#39; = &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_p95.tif&#39;, &#39;sd&#39; = &#39;D:/ontario_inventory/romeo/SPL metrics/Z_METRICS_MOSAIC/individual/RMF_Z_METRICS_MOSAIC_zsd.tif&#39;, &#39;rumple&#39; = &#39;D:/ontario_inventory/romeo/SPL metrics/Z_METRICS_MOSAIC/individual/RMF_RUMPLE_MOSAIC_r_rumple.tif&#39;, &#39;zpcum8&#39; = &#39;D:/ontario_inventory/romeo/SPL metrics/Z_METRICS_MOSAIC/individual/RMF_Z_METRICS_MOSAIC_zpcum8.tif&#39;, &#39;avg&#39; = &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_avg.tif&#39;, &#39;red_edge_2&#39; = &#39;D:/ontario_inventory/romeo/Sentinel/red_edge_2.tif&#39;, &#39;cc&#39; = &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_2m_cov.tif&#39;) # loop through LiDAR attributes to extract values for (i in seq_along(lidar)) { # load LiDAR rasters as raster stack lidar_ras &lt;- rast(lidar[i]) # project poly to crs of raster poly_ras &lt;- project(poly, lidar_ras) # convert to sf poly_ras &lt;- st_as_sf(poly_ras) #extract median values vec &lt;- exact_extract(lidar_ras, poly_ras, &#39;median&#39;) # aggregate into data frame if(i == 1){ vec_df &lt;- as.data.frame(vec) } else{ vec_df &lt;- cbind(vec_df, as.data.frame(vec)) } } # change column names of extracted attribute data frame colnames(vec_df) &lt;- names(lidar) # add LiDAR attributes to FRI polygon data frame dat_fri &lt;- cbind(dat_fri, vec_df) # add 2018 age values dat_fri$AGE2018 &lt;- 2018 - dat_fri$YRORG # save extracted dataframe for fast rebooting save(dat_fri, file = &#39;D:/ontario_inventory/imputation/example/dat_fri_extr.RData&#39;) 1b. Extract attributes in GRM segmented polygons # load GRM segmented polygons poly &lt;- vect(&#39;D:/ontario_inventory/segmentation/grm/shp/grm_10_01_05.shp&#39;) # convert to df dat_grm &lt;- as.data.frame(poly) # cbind centroids to dat dat_grm &lt;- cbind(dat_grm, centroids(poly) %&gt;% crds) # remove p95 and cc as we&#39;ll re-calculate here for consistency dat_grm %&lt;&gt;% select(-c(p95, cc)) # load LiDAR datasets we need to extract over polygons # create named vector with variable names and data links lidar &lt;- c(&#39;p95&#39; = &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_p95.tif&#39;, &#39;sd&#39; = &#39;D:/ontario_inventory/romeo/SPL metrics/Z_METRICS_MOSAIC/individual/RMF_Z_METRICS_MOSAIC_zsd.tif&#39;, &#39;rumple&#39; = &#39;D:/ontario_inventory/romeo/SPL metrics/Z_METRICS_MOSAIC/individual/RMF_RUMPLE_MOSAIC_r_rumple.tif&#39;, &#39;zpcum8&#39; = &#39;D:/ontario_inventory/romeo/SPL metrics/Z_METRICS_MOSAIC/individual/RMF_Z_METRICS_MOSAIC_zpcum8.tif&#39;, &#39;avg&#39; = &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_avg.tif&#39;, &#39;red_edge_2&#39; = &#39;D:/ontario_inventory/romeo/Sentinel/red_edge_2.tif&#39;, &#39;cc&#39; = &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_2m_cov.tif&#39;) # loop through LiDAR attributes to extract values for (i in seq_along(lidar)) { # load LiDAR rasters as raster stack lidar_ras &lt;- rast(lidar[i]) # project poly to crs of raster poly_ras &lt;- project(poly, lidar_ras) # convert to sf poly_ras &lt;- st_as_sf(poly_ras) #extract median values vec &lt;- exact_extract(lidar_ras, poly_ras, &#39;median&#39;) # aggregate into data frame if(i == 1){ vec_df &lt;- as.data.frame(vec) } else{ vec_df &lt;- cbind(vec_df, as.data.frame(vec)) } } # change column names of extracted attribute data frame colnames(vec_df) &lt;- names(lidar) # add LiDAR attributes to FRI polygon data frame dat_grm &lt;- cbind(dat_grm, vec_df) # save extracted data frame for fast rebooting save(dat_grm, file = &#39;D:/ontario_inventory/imputation/example/grm_10_01_05_extr.RData&#39;) # clear workspace rm(list=ls()) 2. Screen FRI polygons to curate an optimal dataset to use for imputation In order to ensure the best possible imputation results, it is important to screen the FRI dataset and remove polygons that do not fit certain data quality criteria, or are not representative of forest stands. The current criteria being used are: POLYTYPE == ‘FOR’ polygon &gt;= 50% forested landcover p95 &gt;= 5 meters (broad definition of ‘forest’) Canopy cover &gt;= 50% # load FRI polygons poly &lt;- vect(&#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Polygons Inventory/RMF_PolygonForest.shp&#39;) # load FRI polygon data frame load(&#39;D:/ontario_inventory/imputation/example/dat_fri_extr.RData&#39;) ################################## ### SCREEN FOR POLYTYPE == FOR ### ################################## # filter POLYTYPE == &#39;FOR&#39; dat_fri &lt;- filter(dat_fri, POLYTYPE == &#39;FOR&#39;) poly_fri &lt;- poly[poly$POLYTYPE == &#39;FOR&#39;] #################################################### ### SCREEN FOR POLYGON &gt;= 50% FORESTED LANDCOVER ### #################################################### # load VLCE 2.0 landcover dataset from 2018 lc &lt;- rast(&#39;D:/ontario_inventory/VLCE/CA_forest_VLCE2_2018_CLIPPED.tif&#39;) # project poly to crs of raster poly_lc &lt;- project(poly_fri, lc) # convert to sf poly_lcsf &lt;- st_as_sf(poly_lc) # extract landcover values lc_poly &lt;- exact_extract(lc, poly_lcsf) # set landcover class key with single forested class lc_key_for &lt;- c(`20` = &#39;Water&#39;, `31` = &#39;Snow/Ice&#39;, `32` = &#39;Rock/Rubble&#39;, `33` = &#39;Exposed/Barren Land&#39;, `40` = &#39;Bryoids&#39;, `50` = &#39;Shrubland&#39;, `80` = &#39;Wetland&#39;, `81` = &#39;Forest&#39;, `100` = &#39;Herbs&#39;, `210` = &#39;Forest&#39;, `220` = &#39;Forest&#39;, `230` = &#39;Forest&#39;) # find polygons with forest at least 50% of pixels # apply over list lc_dom_for &lt;- sapply(lc_poly, function(x){ x$value &lt;- recode(x$value, !!!lc_key_for) x &lt;- x %&gt;% group_by(value) %&gt;% summarize(sum = sum(coverage_fraction)) m &lt;- x$value[which(x$sum == max(x$sum))] if((length(m) == 1) &amp; (m == &#39;Forest&#39;)[1]){ if(x$sum[x$value == m]/sum(x$sum) &gt;= 0.5){ return(&#39;Yes&#39;) }else{return(&#39;No&#39;)} }else{return(&#39;No&#39;)} }) # add into FRI data frame dat_fri &lt;- dat_fri %&gt;% add_column(dom_for = lc_dom_for) # subset FRI data frame based on whether polygon dominated by forest dat_dom_for &lt;- dat_fri[dat_fri$dom_for == &#39;Yes&#39;,] ################################## ### SCREEN FOR P95 &gt;= 5 METERS ### ################################## # subset FRI data frame dat_p95 &lt;- dat_fri[dat_fri$p95 &gt;= 5,] ############################ ### SCREEN FOR CC &gt;= 50% ### ############################ # subset FRI data frame dat_cc &lt;- dat_fri[dat_fri$cc &gt;= 50,] ############################################ ### COMBINE INTERSECTION OF DATA SCREENS ### ############################################ # first combine only by POLYID dat_fri_scr &lt;- intersect(dat_dom_for %&gt;% subset(select = POLYID), dat_p95 %&gt;% subset(select = POLYID)) %&gt;% intersect(., dat_cc %&gt;% subset(select = POLYID)) # load full data frame attributes dat_fri_scr &lt;- dat_fri[dat_fri$POLYID %in% dat_fri_scr$POLYID,] # save extracted data frame for fast rebooting save(dat_fri_scr, file = &#39;D:/ontario_inventory/imputation/example/dat_fri_scr.RData&#39;) # clear workspace rm(list=ls()) 3. Run imputation on FRI polygons ONLY to assess imputation performance The goal of this imputation procedure is to estimate age and species composition in GRM segmented forest polygons. But it is also important to assess the performance of the algorithm. To do this, we can conduct the imputation over the FRI dataset ONLY. For each FRI polygon, we find the k-nearest neighbors, and calculate the age and species composition attributes to impute. We can then compare the observed age and species composition of the polygon to the imputed values. Note we have to do this calculation on the FRI dataset alone because we do not have observed age and species composition values for the GRM segmented polygons. Also note that ALL attributes are imputed from the same k-nearest neighbors. The algorithm is not run for individual attributes. For age, we report root mean squared difference (RMSD), mean absolute error (MAE) and mean bias error (MBE). RMSD is used for assess results for model comparison. MAE gives an average error of imputed age in years. MBE gives an indication of whether we are imputing younger or older ages on average. For species composition, we report the percent of observed and imputed values that match (accuracy). Species composition is broken down into several distinct attributes: Working group First leading species (from FRI SPCOMP attribute) Second leading species (from FRI SPCOMP attribute) Three functional group classification (softwood, mixedwood, hardwood) derived from SPCOMP Five functional group classification (jack pine dominated, black spruce dominated, mixed conifer, mixedwood, hardwood) derived from SPCOMP We also calculate performance metrics on the X-variables used in imputation: relative RMSD (RRMSD) and relative MBE (RMBE), calculated by dividing RMSD and RMBE by the mean value of each variable and multiplying by 100. RRMSD and RMBE give a percent error that can be compared across variables and when variables have difficult to interpret units, such as several of the ALS metrics used in imputation. 3a. Create functions needed ###################################################### ### FUNCTIONS TO RUN K NEAREST NEIGHBOR IMPUTATION ### ###################################################### # load packages library(RANN) library(reshape2) # create mode function getmode &lt;- function(v) { uniqv &lt;- unique(v) uniqv[which.max(tabulate(match(v, uniqv)))] } # create rmsd function rmsd &lt;- function(obs, est){ sqrt(mean((est - obs) ^ 2)) } # create rrmsd function rrmsd &lt;- function(obs, est){ sqrt(mean((est - obs) ^ 2)) / mean(obs) * 100 } # create mae function mae &lt;- function(obs, est){ mean(abs(est - obs)) } # create mbe function mbe &lt;- function(obs, est){ mean(est - obs) } # create rmbe function rmbe &lt;- function(obs, est){ mean(est - obs) / mean(obs) * 100 } # create knn function run_knn_fri &lt;- function(dat, vars, k) { # subset data dat_nn &lt;- dat %&gt;% select(all_of(vars)) # scale for nn computation dat_nn_scaled &lt;- dat_nn %&gt;% scale # run nearest neighbor nn &lt;- nn2(dat_nn_scaled, dat_nn_scaled, k = k + 1) # get nn indices nni &lt;- nn[[1]][, 2:(k + 1)] # add vars to tibble # take mean/mode if k &gt; 1 if(k &gt; 1){ for(i in seq_along(vars)){ if(i == 1){ nn_tab &lt;- tibble(!!vars[i] := dat_nn[,i], !!str_c(vars[i], &#39;_nn&#39;) := apply(nni, MARGIN = 1, FUN = function(x){ mean(dat_nn[x, i]) })) }else{ nn_tab %&lt;&gt;% mutate(!!vars[i] := dat_nn[,i], !!str_c(vars[i], &#39;_nn&#39;) := apply(nni, MARGIN = 1, FUN = function(x){ mean(dat_nn[x, i]) })) } } # add target vars to tibble nn_tab %&lt;&gt;% mutate(age = dat$AGE2018, wg = dat$WG, sp1 = dat$SP1, sp2 = dat$SP2, group5 = dat$SpeciesGroup2, group3 = dat$SpeciesGroup3, age_nn = apply(nni, MARGIN = 1, FUN = function(x){ mean(dat$AGE2018[x]) }), wg_nn = apply(nni, MARGIN = 1, FUN = function(x){ getmode(dat$WG[x]) }), sp1_nn = apply(nni, MARGIN = 1, FUN = function(x){ getmode(dat$SP1[x]) }), sp2_nn = apply(nni, MARGIN = 1, FUN = function(x){ getmode(dat$SP2[x]) }), group5_nn = apply(nni, MARGIN = 1, FUN = function(x){ getmode(dat$SpeciesGroup2[x]) }), group3_nn = apply(nni, MARGIN = 1, FUN = function(x){ getmode(dat$SpeciesGroup3[x]) })) } # take direct nn if k == 1 if(k == 1){ for(i in seq_along(vars)){ if(i == 1){ nn_tab &lt;- tibble(!!vars[i] := dat_nn[,i], !!str_c(vars[i], &#39;_nn&#39;) := dat_nn[nn[[1]][,2],i]) }else{ nn_tab %&lt;&gt;% mutate(!!vars[i] := dat_nn[,i], !!str_c(vars[i], &#39;_nn&#39;) := dat_nn[nn[[1]][,2],i]) } } # add target vars to tibble nn_tab %&lt;&gt;% mutate(age = dat$AGE2018, wg = dat$WG, sp1 = dat$SP1, sp2 = dat$SP2, group5 = dat$SpeciesGroup2, group3 = dat$SpeciesGroup3, age_nn = dat$AGE2018[nn[[1]][,2]], wg_nn = dat$WG[nn[[1]][,2]], sp1_nn = dat$SP1[nn[[1]][,2]], sp2_nn = dat$SP2[nn[[1]][,2]], group5_nn = dat$SpeciesGroup2[nn[[1]][,2]], group3_nn = dat$SpeciesGroup3[nn[[1]][,2]]) } # calculate fit metrics for vars for(i in seq_along(vars)){ if(i == 1){ perform_df &lt;- tibble(variable = vars[i], metric = c(&#39;rrmsd (%)&#39;, &#39;rmbe (%)&#39;), value = c(rrmsd(pull(nn_tab, vars[i]), pull(nn_tab, str_c(vars[i], &#39;_nn&#39;))), rmbe(pull(nn_tab, vars[i]), pull(nn_tab, str_c(vars[i], &#39;_nn&#39;))))) }else{ perform_df %&lt;&gt;% add_row(variable = vars[i], metric = c(&#39;rrmsd (%)&#39;, &#39;rmbe (%)&#39;), value = c(rrmsd(pull(nn_tab, vars[i]), pull(nn_tab, str_c(vars[i], &#39;_nn&#39;))), rmbe(pull(nn_tab, vars[i]), pull(nn_tab, str_c(vars[i], &#39;_nn&#39;))))) } } # calculate metrics for age perform_df %&lt;&gt;% add_row(variable = &#39;age&#39;, metric = c(&#39;rmsd (yrs)&#39;, &#39;mbe (yrs)&#39;, &#39;mae (yrs)&#39;), value = c(rmsd(nn_tab$age, nn_tab$age_nn), mbe(nn_tab$age, nn_tab$age_nn), mae(nn_tab$age, nn_tab$age_nn))) # calculate wg accuracy # create df of WG wg &lt;- data.frame(obs = nn_tab$wg, est = nn_tab$wg_nn) # create column of match or not wg$match &lt;- wg$obs == wg$est # add total percent of matching WG to perform_df perform_df %&lt;&gt;% add_row(variable = &#39;working group&#39;, metric = &#39;accuracy (%)&#39;, value = NROW(wg[wg$match == T,]) / NROW(wg) * 100) # calculate SP1 accuracy # create df of SP1 sp1 &lt;- data.frame(obs = nn_tab$sp1, est = nn_tab$sp1_nn) # create column of match or not sp1$match &lt;- sp1$obs == sp1$est # add total percent of matching SP1 to perform_df perform_df %&lt;&gt;% add_row(variable = &#39;leading species&#39;, metric = &#39;accuracy (%)&#39;, value = NROW(sp1[sp1$match == T,]) / NROW(sp1) * 100) # calculate SP2 accuracy # create df of SP2 sp2 &lt;- data.frame(obs = nn_tab$sp2, est = nn_tab$sp2_nn) # create column of match or not sp2$match &lt;- sp2$obs == sp2$est # add total percent of matching SP2 to perform_df perform_df %&lt;&gt;% add_row(variable = &#39;second species&#39;, metric = &#39;accuracy (%)&#39;, value = NROW(sp2[sp2$match == T,]) / NROW(sp2) * 100) # calculate GROUP3 accuracy # create df of GROUP3 group3 &lt;- data.frame(obs = nn_tab$group3, est = nn_tab$group3_nn) # create column of match or not group3$match &lt;- group3$obs == group3$est # add total percent of matching group3 to perform_df perform_df %&lt;&gt;% add_row(variable = &#39;three func group class&#39;, metric = &#39;accuracy (%)&#39;, value = NROW(group3[group3$match == T,]) / NROW(group3) * 100) # calculate GROUP5 accuracy # create df of GROUP5 group5 &lt;- data.frame(obs = nn_tab$group5, est = nn_tab$group5_nn) # create column of match or not group5$match &lt;- group5$obs == group5$est # add total percent of matching group5 to perform_df perform_df %&lt;&gt;% add_row(variable = &#39;five func group class&#39;, metric = &#39;accuracy (%)&#39;, value = NROW(group5[group5$match == T,]) / NROW(group5) * 100) # return df return(perform_df) } 3b. Run the imputation and assess performance ########################### ### LOAD FRI DATA FRAME ### ########################### # load from part 2 above load(&#39;D:/ontario_inventory/imputation/example/dat_fri_scr.RData&#39;) # subset only the attributes we need dat_fri_scr %&lt;&gt;% select(POLYID, AGE2018, SPCOMP, WG, avg, sd, rumple, zpcum8, x, y, red_edge_2) # remove any polygons with missing values dat_fri_scr &lt;- na.omit(dat_fri_scr) ########################################### ### CALCULATE AND ADD SPCOMP ATTRIBUTES ### ########################################### # parse SPCOMP strings sp &lt;- str_split(dat_fri_scr$SPCOMP, pattern = &quot;\\\\s{2}&quot;) # add first species to dat dat_fri_scr$SP1 &lt;- sapply(sp, FUN = function(x){ str &lt;- x[1] str &lt;- str_sub(str, start = 1, end = 2) return(str) }) # add first species percent to dat dat_fri_scr$SP1P &lt;- sapply(sp, FUN = function(x){ str &lt;- x[2] if(is.na(str)){ str &lt;- 100 } else{ str &lt;- str_sub(str, start = 1, end = 2) } return(str) }) # add second species to dat dat_fri_scr$SP2 &lt;- sapply(sp, FUN = function(x){ str &lt;- x[2] if(is.na(str) == F){ str &lt;- str_sub(str, start = 3, end = 4) } return(str) }) # add second species percent to dat dat_fri_scr$SP2P &lt;- sapply(sp, FUN = function(x){ str &lt;- x[3] if(is.na(str) == F){ str &lt;- str_sub(str, start = 1, end = 2) } return(str) }) # change second species missing values dat_fri_scr$SP2[is.na(dat_fri_scr$SP2)] &lt;- &#39;MIS&#39; dat_fri_scr$SP2P[is.na(dat_fri_scr$SP2P)] &lt;- 0 # load species group data -- calculated in separate code (can provide details) sp_group &lt;- read.csv( &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Polygons Inventory/RMF_PolygonForest_SPGROUP.shp&#39; ) # change POLYID to numeric dat_fri_scr %&lt;&gt;% mutate(POLYID = as.numeric(POLYID)) sp_group %&lt;&gt;% mutate(POLYID = as.numeric(POLYID)) # join to dat dat_fri_scr &lt;- left_join(dat_fri_scr, sp_group %&gt;% select(POLYID, SpeciesGroup2, SpeciesGroup3), by = &#39;POLYID&#39;) ############################################# ### RUN NEAREST NEIGHBOR IMPUTATION K = 5 ### ############################################# # create vector of X-variables for imputation vars &lt;- c(&#39;avg&#39;, &#39;sd&#39;, &#39;rumple&#39;, &#39;zpcum8&#39;, &#39;x&#39;, &#39;y&#39;, &#39;red_edge_2&#39;) # run_knn function already created perf &lt;- run_knn_fri(dat_fri_scr, vars, k = 5) # round values perf %&lt;&gt;% mutate(value = round(value, 2)) # factor variable and metric categories to order perf %&lt;&gt;% mutate(variable = factor(variable, levels = c(&#39;age&#39;, &#39;working group&#39;,&#39;leading species&#39;, &#39;second species&#39;, &#39;three func group class&#39;, &#39;five func group class&#39;, vars))) %&gt;% mutate(metric = factor(metric, levels = c(&#39;rmsd (yrs)&#39;, &#39;mbe (yrs)&#39;, &#39;mae (yrs)&#39;, &#39;accuracy (%)&#39;, &#39;rrmsd (%)&#39;, &#39;rmbe (%)&#39;))) # cast df perf_cast &lt;- dcast(perf, variable ~ metric) # remove x and y perf_cast %&lt;&gt;% filter(!(variable %in% c(&#39;x&#39;, &#39;y&#39;))) # set NA to blank perf_cast[is.na(perf_cast)] &lt;- &#39;&#39; # display results knitr::kable(perf_cast, caption = &quot;Imputation Performance of FRI Forest Stand Polygons&quot;, label = NA) Imputation Performance of FRI Forest Stand Polygons variable rmsd (yrs) mbe (yrs) mae (yrs) accuracy (%) rrmsd (%) rmbe (%) age 23.31 -0.14 16.06 working group 66.77 leading species 65.46 second species 34.05 three func group class 72.34 five func group class 60.73 avg 3.6 0 sd 3.88 -0.33 rumple 2.54 0.01 zpcum8 0.9 0.13 red_edge_2 2.15 -0.1 The mean bias error (MBE) of age is -0.14 years, indicating the imputed estimates of age are not skewed toward younger or older values. The mean absolute error (MAE) of age is 16.06 years, which is the average difference between the observed and imputed value. Accuracy of leading species classification is 65.46%, and a much lower 34.05% for second leading species. Three and five functional group classification have respective accuracies of 72.34% and 60.73%. Relative root mean squared difference (RRMSD) of the imputation attributes (avg, sd, rumple, zpcum8, and red_edge_2) is below 4% for all attributes. These are low values, which demonstrate that the imputation algorithm is finding optimal matches within the database of available FRI polygons. Relative mean bias error (RMBE) of the imputation attributes is close to 0%, meaning that the nearest neighbor selections are not skewed toward positive or negative values of these attributes. RRMSD/RMBE are not calculated for x and y because the coordinates do not represent a value scale. 4. Run imputation between FRI and GRM polygons to derive estimates of age and species composition The last step is to run the imputation between the screened FRI polygons and the GRM segmented polygons. For each GRM segmented polygon, the k-nearest neighbors in the FRI data are found and used to impute age and species composition. Note we do not conduct imputation on all the GRM segmented polygons, but only the polygons that have &gt;= 50% forested landcover, p95 &gt;= 5 meters, and canopy cover &gt;= 50% (same criteria as FRI data screening conducted above in step 2). Although we cannot directly assess the error of age and species composition when imputing into the GRM segmented polygons, we can still assess the fit of the variables used in the imputation. We can also review maps and distributions comparing FRI age/species composition against the same attributes imputed into GRM segmented polygons. # load additional packages library(viridis) library(scales) library(janitor) # load GRM polygon data frame load(&#39;D:/ontario_inventory/imputation/example/grm_10_01_05_extr.RData&#39;) # screen for forested polygons dat_grm_scr &lt;- dat_grm %&gt;% filter(dom_for == &#39;Yes&#39;, p95 &gt;= 5, cc &gt;= 50) # create data frame for grm and fri metrics used in imputation dat_grm_imp &lt;- dat_grm_scr %&gt;% select(id, avg, sd, rumple, zpcum8, x, y, red_edge_2) %&gt;% na.omit dat_fri_imp &lt;- dat_fri_scr %&gt;% select(avg, sd, rumple, zpcum8, x, y, red_edge_2) %&gt;% na.omit # need to combine and scale all values together then separate again dat_comb_scaled &lt;- rbind(dat_grm_imp %&gt;% select(-id), dat_fri_imp) %&gt;% scale dat_grm_scaled &lt;- dat_comb_scaled[1:NROW(dat_grm_imp),] dat_fri_scaled &lt;- dat_comb_scaled[(NROW(dat_grm_imp)+1):(NROW(dat_grm_imp)+NROW(dat_fri_imp)),] # run nearest neighbor imputation k = 5 nn &lt;- nn2(dat_fri_scaled, dat_grm_scaled, k = 5) # get nn indices nni &lt;- nn[[1]] # add imputed attributes into GRM imputation data frame for(i in seq_along(vars)){ dat_grm_imp %&lt;&gt;% add_column( !!str_c(vars[i], &#39;_imp&#39;) := apply( nni, MARGIN = 1, FUN = function(x){ mean(dat_fri_imp[x, vars[i]]) } )) } # create vector of target variables tar_vars &lt;- c(&#39;AGE2018&#39;, &#39;WG&#39;, &#39;SP1&#39;, &#39;SP2&#39;, &#39;SpeciesGroup3&#39;, &#39;SpeciesGroup2&#39;) # add age and species variables to GRM data frame for(i in seq_along(tar_vars)){ if(i == &#39;AGE2018&#39;){ dat_grm_imp %&lt;&gt;% add_column( !!tar_vars[i] := apply( nni, MARGIN = 1, FUN = function(x){ mean(dat_fri_scr[x, tar_vars[i]]) } )) } else{ dat_grm_imp %&lt;&gt;% add_column( !!tar_vars[i] := apply( nni, MARGIN = 1, FUN = function(x){ getmode(dat_fri_scr[x, tar_vars[i]]) } )) } } # update colnames dat_grm_imp %&lt;&gt;% rename(age = AGE2018, class3 = SpeciesGroup3, class5 = SpeciesGroup2) # add values back into main GRM data frame (missing values for polygons not # included in the imputation) dat_grm &lt;- left_join(dat_grm, dat_grm_imp) # calculate performance across imputation attributes for(i in seq_along(vars)){ if(i == 1){ perf &lt;- tibble(variable = vars[i], metric = c(&#39;rrmsd (%)&#39;, &#39;rmbe (%)&#39;), value = c(rrmsd(dat_grm_imp[, vars[i]], dat_grm_imp[, str_c(vars[i], &#39;_imp&#39;)]), rmbe(dat_grm_imp[, vars[i]], dat_grm_imp[, str_c(vars[i], &#39;_imp&#39;)]))) }else{ perf %&lt;&gt;% add_row(variable = vars[i], metric = c(&#39;rrmsd (%)&#39;, &#39;rmbe (%)&#39;), value = c(rrmsd(dat_grm_imp[, vars[i]], dat_grm_imp[, str_c(vars[i], &#39;_imp&#39;)]), rmbe(dat_grm_imp[, vars[i]], dat_grm_imp[, str_c(vars[i], &#39;_imp&#39;)]))) } } # round to two decimal places perf %&lt;&gt;% mutate(value = round(value, 2)) # factor so table displays nicely perf %&lt;&gt;% mutate(variable = factor(variable, levels = vars)) %&gt;% mutate(metric = factor(metric, levels = c(&#39;rrmsd (%)&#39;, &#39;rmbe (%)&#39;))) # cast df perf_cast &lt;- dcast(perf, variable ~ metric) # remove x and y perf_cast %&lt;&gt;% filter(!(variable %in% c(&#39;x&#39;, &#39;y&#39;))) # display results of imputation rmsd knitr::kable(perf_cast, caption = &quot;Imputation Performance between FRI and GRM Forest Stand Polygons&quot;, label = NA) Imputation Performance between FRI and GRM Forest Stand Polygons variable rrmsd (%) rmbe (%) avg 3.82 0.05 sd 4.05 -0.46 rumple 2.65 0.11 zpcum8 0.86 0.12 red_edge_2 2.09 -0.12 The RRMSD results are again all below ~4% and RMBE does not show bias toward negative of positive difference. 4a. Figures of Forest Stand Age # load GRM polygons poly_grm &lt;- vect(&#39;D:/ontario_inventory/segmentation/grm/shp/grm_10_01_05.shp&#39;) # add new data frame to polygons values(poly_grm) &lt;- dat_grm # save grm polygon output writeVector(poly_grm, &#39;D:/ontario_inventory/imputation/example/grm_10_01_05_imp.shp&#39;, overwrite = T) # load FRI polygons poly_fri &lt;- vect(&#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Polygons Inventory/RMF_PolygonForest.shp&#39;) # set age == 0 to NA poly_fri$AGE[poly_fri$AGE == 0] &lt;- NA # load species group data -- calculated in separate code (can provide details) sp_group &lt;- read.csv( &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Polygons Inventory/RMF_PolygonForest_SPGROUP.shp&#39; ) # load poly_fri dataframe dat_fri &lt;- as.data.frame(poly_fri) # change POLYID to numeric dat_fri %&lt;&gt;% mutate(POLYID = as.numeric(POLYID)) sp_group %&lt;&gt;% mutate(POLYID = as.numeric(POLYID)) # rename species groups sp_group %&lt;&gt;% rename(class3 = SpeciesGroup3, class5 = SpeciesGroup2) # join to dat dat_fri &lt;- left_join(dat_fri, sp_group %&gt;% select(POLYID, class3, class5), by = &#39;POLYID&#39;) # update age to 2018 dat_fri$AGE2018 &lt;- 2018 - dat_fri$YRORG # re-input attributes into FRI polygons values(poly_fri) &lt;- dat_fri rm(dat_fri) # since we only have values of age and species comp for dom_fom == yes # and p95 &gt;= 5 m we should only compare the screened FRI polygons that # contain those same attributes poly_fri$AGE2018[!(poly_fri$POLYID %in% dat_fri_scr$POLYID)] &lt;- NA poly_fri$class3[!(poly_fri$POLYID %in% dat_fri_scr$POLYID)] &lt;- NA poly_fri$class5[!(poly_fri$POLYID %in% dat_fri_scr$POLYID)] &lt;- NA # create df to plot age grm_sf &lt;- st_as_sf(poly_grm) fri_sf &lt;- st_as_sf(poly_fri) # cut dfs grm_sf %&lt;&gt;% mutate(age_cut = cut(age, breaks = c(seq(0, 130, 10), 250))) fri_sf %&lt;&gt;% mutate(age_cut = cut(AGE2018, breaks = c(seq(0, 130, 10), 250))) # plot age p1 &lt;- ggplot(grm_sf) + geom_sf(mapping = aes(fill = age_cut), linewidth = 0.05) + coord_sf() + scale_fill_manual(values = viridis(14), name = &#39;Age&#39;, na.translate = F) + theme_void(base_size = 30) + ggtitle(&#39;Imputed Age of GRM \\nSegmented Forest Stands&#39;) + theme(plot.title = element_text(hjust = 0.5)) p2 &lt;- ggplot(fri_sf) + geom_sf(mapping = aes(fill = age_cut), linewidth = 0.05) + coord_sf() + scale_fill_manual(values = viridis(14), name = &#39;Age&#39;, na.translate = F) + theme_void(base_size = 30) + ggtitle(&#39;Age of FRI \\nForest Stands&#39;) + theme(plot.title = element_text(hjust = 0.5)) grid.arrange(p1, p2, ncol = 1) Imputed age values show a similar spatial distribution to observed age values at a broad scale. Of course, the values should be scrutinized on a fine scale in specific areas. This task is much easier to do in a GIS software using the output shapefiles. 4b. Figures of Three Functional Group Classification # plot three func group classification p1 &lt;- ggplot(grm_sf) + geom_sf(mapping = aes(fill = class3), linewidth = 0.05) + coord_sf() + scale_fill_manual(values = c(&#39;#228833&#39;, &#39;#aa3377&#39;, &#39;#ccbb44&#39;), name = &#39;Species Class&#39;, na.translate = F) + theme_void(base_size = 30) + ggtitle(&#39;Imputed Three Functional \\nGroup Classification (GRM)&#39;) + theme(plot.title = element_text(hjust = 0.5)) p2 &lt;- ggplot(fri_sf) + geom_sf(mapping = aes(fill = class3), linewidth = 0.05) + coord_sf() + scale_fill_manual(values = c(&#39;#228833&#39;, &#39;#aa3377&#39;, &#39;#ccbb44&#39;), name = &#39;Species Class&#39;, na.translate = F) + theme_void(base_size = 30) + ggtitle(&#39;Three Functional \\nGroup Classification (FRI)&#39;) + theme(plot.title = element_text(hjust = 0.5)) grid.arrange(p1, p2, ncol = 1) We can also observe a similar distribution of species classes. 4c. Figures of Five Functional Group Classification # plot five func group classification p1 &lt;- ggplot(grm_sf) + geom_sf(mapping = aes(fill = class5), linewidth = 0.05) + coord_sf() + scale_fill_manual(values = c(&#39;#ccbb44&#39;, &#39;#228833&#39;, &#39;#4477aa&#39;, &#39;#ee6677&#39;, &#39;#aa3377&#39;), name = &#39;Species Class&#39;, na.translate = F) + theme_void(base_size = 30) + ggtitle(&#39;Imputed Five Functional \\nGroup Classification (GRM)&#39;) + theme(plot.title = element_text(hjust = 0.5)) p2 &lt;- ggplot(fri_sf) + geom_sf(mapping = aes(fill = class5), linewidth = 0.05) + coord_sf() + scale_fill_manual(values = c(&#39;#ccbb44&#39;, &#39;#228833&#39;, &#39;#4477aa&#39;, &#39;#ee6677&#39;, &#39;#aa3377&#39;), name = &#39;Species Class&#39;, na.translate = F) + theme_void(base_size = 30) + ggtitle(&#39;Five Functional \\nGroup Classification (FRI)&#39;) + theme(plot.title = element_text(hjust = 0.5)) grid.arrange(p1, p2, ncol = 1) 4d. Density Plots of Forest Stand Age # density plots of age p1 &lt;- ggplot(dat_grm, aes(x = age)) + geom_density(fill = &#39;grey&#39;) + geom_vline(aes(xintercept = median(age, na.rm = T)), linetype = &quot;dashed&quot;, size = 0.6) + xlim(c(0,200)) + ylim(c(0, 0.02)) + theme_bw() + xlab(&#39;Age&#39;) + ylab(&#39;Density&#39;) + ggtitle(&#39;Imputed Age of GRM Segmented Forest Stands&#39;) + theme(text = element_text(size = 25), plot.title = element_text(size=30)) p2 &lt;- ggplot(as.data.frame(poly_fri), aes(x = AGE2018)) + geom_density(fill = &#39;grey&#39;) + geom_vline(aes(xintercept = median(AGE2018, na.rm = T)), linetype = &quot;dashed&quot;, size = 0.6) + xlim(c(0, 200)) + ylim(c(0, 0.02)) + theme_bw() + xlab(&#39;Age&#39;) + ylab(&#39;Density&#39;) + ggtitle(&#39;Age of FRI Forest Stands&#39;) + theme(text = element_text(size = 25), plot.title = element_text(size=30)) grid.arrange(p1, p2, ncol = 1) The distribution of imputed age in GRM polygons closely matches that of observed age in FRI polygons. The median age values (dotted lines) are 78 (GRM) and 83 (FRI). 4e. Distribution of Three Functional Group Classification # create data frame for GRM 3 classes dat_grm_c3 &lt;- dat_grm %&gt;% tabyl(class3) %&gt;% filter(is.na(class3) == F) %&gt;% arrange(desc(class3)) %&gt;% mutate(prop = n / sum(.$n)*100) %&gt;% mutate(ypos = cumsum(prop) - 0.5*prop) %&gt;% mutate(lbl = round(prop)) # create data frame for FRI 3 classes dat_fri_c3 &lt;- poly_fri %&gt;% as.data.frame %&gt;% tabyl(class3) %&gt;% filter(is.na(class3) == F) %&gt;% arrange(desc(class3)) %&gt;% mutate(prop = n / sum(.$n)*100) %&gt;% mutate(ypos = cumsum(prop) - 0.5*prop) %&gt;% mutate(lbl = round(prop)) # plot p1 &lt;- ggplot(dat_grm_c3, aes(x = &quot;&quot;, y = prop, fill = class3)) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start = 0) + theme_void() + geom_text(aes(y = ypos, label = str_c(lbl, &quot;%&quot;)), size = 15) + theme(legend.title = element_text(size = 30), legend.text = element_text(size = 20), legend.key.width = unit(2, &#39;cm&#39;), plot.title = element_text(size=30)) + scale_fill_manual(values = c(&#39;#228833&#39;, &#39;#aa3377&#39;, &#39;#ccbb44&#39;)) + labs(fill = &quot;&quot;) + ggtitle(&quot;Imputed Three Functional \\nGroup Classification (GRM)&quot;) p2 &lt;- ggplot(dat_fri_c3, aes(x = &quot;&quot;, y = prop, fill = class3)) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start = 0) + theme_void() + geom_text(aes(y = ypos, label = str_c(lbl, &quot;%&quot;)), size = 15) + theme(legend.title = element_text(size = 30), legend.text = element_text(size = 20), legend.key.width = unit(2, &#39;cm&#39;), plot.title = element_text(size=30)) + scale_fill_manual(values = c(&#39;#228833&#39;, &#39;#aa3377&#39;, &#39;#ccbb44&#39;)) + labs(fill = &quot;&quot;) + ggtitle(&quot;Three Functional Group \\nClassification (FRI)&quot;) grid.arrange(p1, p2, ncol = 2) The distribution of three functional groups is equal in the hardwood class (20%). The distribution of mixedwood and softwood is slightly different in the imputed values, with 3% more softwood, and thus 3% less mixedwood. 4f. Distribution of Five Functional Group Classification # distribution of 5 species classes # create data frame for GRM 5 classes dat_grm_c5 &lt;- dat_grm %&gt;% tabyl(class5) %&gt;% filter(is.na(class5) == F) %&gt;% arrange(desc(class5)) %&gt;% mutate(prop = n / sum(.$n)*100) %&gt;% mutate(ypos = cumsum(prop) - 0.5*prop) %&gt;% mutate(lbl = round(prop)) # create data frame for FRI 5 classes dat_fri_c5 &lt;- poly_fri %&gt;% as.data.frame %&gt;% tabyl(class5) %&gt;% filter(is.na(class5) == F) %&gt;% arrange(desc(class5)) %&gt;% mutate(prop = n / sum(.$n)*100) %&gt;% mutate(ypos = cumsum(prop) - 0.5*prop) %&gt;% mutate(lbl = round(prop)) # plot p1 &lt;- ggplot(dat_grm_c5, aes(x = &quot;&quot;, y = prop, fill = class5)) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start = 0) + theme_void() + geom_text(aes(y = ypos, label = str_c(lbl, &quot;%&quot;)), size = 15) + theme(legend.title = element_text(size = 30), legend.text = element_text(size = 20), legend.key.width = unit(2, &#39;cm&#39;), plot.title = element_text(size=30)) + scale_fill_manual(values = c(&#39;#ccbb44&#39;, &#39;#228833&#39;, &#39;#4477aa&#39;, &#39;#ee6677&#39;, &#39;#aa3377&#39;)) + labs(fill = &quot;&quot;) + ggtitle(&quot;Imputed Five Functional \\nGroup Classification (GRM)&quot;) p2 &lt;- ggplot(dat_fri_c5, aes(x = &quot;&quot;, y = prop, fill = class5)) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start = 0) + theme_void() + geom_text(aes(y = ypos, label = str_c(lbl, &quot;%&quot;)), size = 15) + theme(legend.title = element_text(size = 30), legend.text = element_text(size = 20), legend.key.width = unit(2, &#39;cm&#39;), plot.title = element_text(size=30)) + scale_fill_manual(values = c(&#39;#ccbb44&#39;, &#39;#228833&#39;, &#39;#4477aa&#39;, &#39;#ee6677&#39;, &#39;#aa3377&#39;)) + labs(fill = &quot;&quot;) + ggtitle(&quot;Five Functional Group \\nClassification (FRI)&quot;) grid.arrange(p1, p2, ncol = 2) The distribution of imputed five classes of species is also very similar to the FRI distribution. The imputed values contain slightly more black spruce dominated stands (3% more than the FRI) and mixedwood stands (1% more than the FRI), and slightly less mixed conifers and jack pine dominated. 5. Conclusion This tutorial has demonstrated a novel approach to update forest stand polygons and populate them with important forest attributes derived from both expert interpretation of aerial photography and ALS point clouds. The imputation approach is open-source, reproducible, and scalable to meet the needs of operational and strategic planning at various levels. As we transition into the final months of this project, the focus will be on completing the following: Digital workshop and code demo Final packaging of open source methods and codes for delivery to all partners Peer-reviewed papers Project wrap-up meeting "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
