[["index.html", "Integration of Photo Interpreted and LIDAR Attributes Into a Polygonal Forest Inventory Framework Overview Project Summary Project Partners", " Integration of Photo Interpreted and LIDAR Attributes Into a Polygonal Forest Inventory Framework Ethan Berman 2022-06-29 Overview Welcome! This website serves as a platform to introduce the Integration of Photo Interpreted and LiDAR Attributes into a Polygonal Forest Inventory Framework project and provide updates as progress is made. The project is led by Prof. Nicholas Coops and made possible through a Forestry Futures Trust Ontario grant. Project Summary The acquisition of Single Photon Lidar (SPL) over the forested area of Ontario is redefining how forest attributes are predicted and monitored throughout the Province. A key question remains however, of how to aggregate these area-based (raster) estimates of forest attributes into traditional strategic or tactical-level inventory polygons. This project is designed to address this need. Outcomes of the project will be open source segmentation and attribute prediction tools to develop a polygon based eFRI inclusive of both SPL and interpreted forest stand attributes as well as knowledge transfer activities and demonstration at a number of forest management units. Project Partners This project is principally a partnership between the University of British Columbia, GreenFirst Forest Products, the Ontario Ministry of Northern Development, Mines, Natural Resources and Forestry, and Forestry Futures Trust Ontario. GreenFirst Forest Products (previously RYAM Forest Management) and the University of British Columbia have a history of successful collaboration including the use of LIDAR and aerial photogrammetry for spruce budworm assessment and regeneration mapping. This project will also build upon the very strong links UBC has with the Canadian Wood Fiber Centre (CWFC), which has thus far culminated in more than 20 peer-reviewed publications on CWFC-funded work, including the award winning best practices guide. In addition, UBC, GreenFirst Forest Products and the Ontario Ministry of Northern Development, Mines, Natural Resources and Forestry are involved in a large NSERC funded project, Silv@21, which is a collaborative research program across Canada focused on developing and applying new silvicultural approaches to forest management under changing forestry demand and markets, climate and diverse public expectations. The questions addressed throughout this project are beyond the scope of Silv@21 and therefore an additional effort was needed. Working with the Province and Industry with advanced remote sensing data sources such as SPL, ensures that the new eFRI provides detailed information on forest composition and structure to inform how much wood can be harvested sustainably across the province. By developing tools which allow for integration of raster-based forest inventory attributes from the SPL and utilizing the recognized benefits of the FRI polygonal inventory, Forestry Futures Trust Ontario has the opportunity to advance forest inventory within Ontario in a significant way. Issues around data management, modeling and fusion of these datasets together into one integrated accurate and operational inventory is of critical importance. This project provides open access tools and methodologies to allow this to be done. The partnerships assembled through private industry, government and universities to address the needs highlighted in this project are critically important and are in direct support of the new Ontario Forest Sector strategy and the Crown Forest Sustainability Act by providing foundational information for economically sustainable forest management planning and spatial planning in the Province. "],["people.html", "People University of British Columbia GreenFirst Forest Products Ontario Ministry of Northern Development, Mines, Natural Resources and Forestry", " People This project would not be possible without dedicated collaborators. University of British Columbia Prof. Nicholas Coops, project lead Professor Nicholas Coops holds a Canada Research Chair in Remote Sensing (Tier 1) at UBC. He has published &gt;460 total referenced peer-reviewed journal publications and is internationally recognised as a scientific leader in the field of remote sensing. He was the principal investigator of the AWARE project, a 5-year research project focused on developing LIDAR applications in Canada for forestry applications. He is a co-author on the Canadian Forest Service LIDAR best practice guide series, the most downloaded CFS information handbook ever, focused on LIDAR data processing. In 2020 he was the co-receipt of the Marcus Wallenberg prize for scientific achievements contributing to significantly broadening knowledge and technical development within the field of Forestry. Coops has lead a previous, successful Forestry Futures Trust Ontario grant investigating the application of SPL data for forest inventory and plot stratification in Ontario. Responsibilities Coops as the PI will manage the budget, an overall assignment of students and researchers to the project. A research scientist (RS) will be based and supervised by Coops at UBC. At the completion of the project the RS at UBC will ensure the software is able to be released openly and publicly to allow other forest management areas in Ontario covered by SPL data to be applied and for use by Provincial staff. Ethan Berman, research scientist Ethan Berman is a research scientist in the Integrated Remote Sensing Studio (IRSS) at UBC. He has a background in mathematics and remote sensing, and has worked extensively with large spatial datasets, developing novel approaches and building models to answer questions relating to forests, snow, vegetation, and climate change. Ethan received his MSc in 2019 under the supervision of Prof. Coops and before starting work on this project was a consultant for the United States Geological Survey, constructing, curating, and managing spatial datasets for a team of ecologists. Responsibilities Berman will be responsible for data management, algorithm development and testing, and delivery of open source software tools to the project partners. He will be supervised by Prof. Coops at UBC. GreenFirst Forest Products Chris McDonell, partner Chris McDonell is the Chief Forester for Ontario at GreenFirst Forest Products. He is accountable for relations between GreenFirst and First Nation and Metis communities in Ontario and Quebec. He is also the coordinator of forest certification standard implementation (FSC) and liaison with various public, private and community organizations. Grant McCartney, partner Grant McCartney was a spatial analyst and forest information systems coordinator with RYAM Forest Management (now GreenFirst Forest Products). He recently transitioned to a role at Forsite Consultants. He performed spatial analysis in support of forest management planning (FMP) and forestry operations on the Gordon Cosens, Romeo Malette and Martel – Magpie Forests in Northeastern Ontario, Canada and led the acquisition of remotely sensed data including Digital Aerial Photogrammetry (DAP) and LiDAR for RYAM. He has been an industrial partner in numerous research efforts including: the Assessment of Wood Attributes for REmote sensing (AWARE) project, CWFC – FIP , past FFT - KKTD projects and the recently approved Silv@21. Responsibilities We will work with GreenFirst Forest Products at a number of their forest management areas where FRI data and EFi data exist. GreenFirst partners will provide plot data, SPL and any other relevant datasets over the areas to ensure accurate model development. They will also attend project meetings, assess model accuracy with GreenFirst staff and test developed software as required. Ontario Ministry of Northern Development, Mines, Natural Resources and Forestry Ian Sinclair, partner Ian Sinclair is a terrestrial landscape analyst with the Ontario Ministry of Northern Development, Mines, Natural Resources and Forestry, where he is responsible for the development of remotely sensed inventory and landscape level monitoring and reporting programs. His focus includes the development of LiDAR attributes and remotely sensed applications to support a continuous forest inventory program. Ian also coordinates the acquisition program of the Ministry’s SPL program and supports the operational aspects of the Vegetation Sampling Network Protocol for development of LiDAR based forest inventories. Derek Landry, partner Derek Landry is the management coordinator of Ontario’s Forest Resources Inventory (FRI) Unit in the Ministry of Northern Development, Mines, Natural Resources and Forestry, where he is leading the modernization of provincial forest inventory and land cover mapping programs. Derek has been working in the Ontario Public Service for 25 years. Prior to joining the FRI unit, Derek managed the development and implementation of recommendations to modernize provincial scale fisheries, wildlife, and vegetation monitoring programs, through the Integrated Monitoring Framework initiative. Geordie Robere-McGugan, partner Geordie Robere-McGugan is the Inventory Development Specialist with the Ontario Ministry of Northern Development, Mines, Natural Resources and Forestry and the lead on the modernization of the Forest Resources Inventory ensuring the inventory products meet the Provincial Policy Framework. Responsibilities Sinclair, Landry and Robere-McGugan all of whom are in the Ontario Provincial government with significant expertise in the FRI mapping process, SPL data capture and specifications and iventory Development Specialist. Ensuring that government partners are on the project team ensures that the outcomes will have immediate uptake by other users within the Ministry thereby ensuring successful technology transfer and knowledge exchange. All three will be available for project update meetings, advice to the PDF and attending virtual call on project updates. All three will be available to test versions of the developed open source code and help ensure documentation is easy to access and appropriate. "],["project-details.html", "Project Details Objective Themes Description Design Methodology Schedule Knowledge &amp; Technology Transfer", " Project Details Objective The objective of this project is to develop open source tools to integrate current EFI SPL-based forest inventory attributes with pre-existing Ontario-wide Forest Resources Inventory data (FRI) to map key forest attributes of interest (volume, height, basal area, density, species composition and group, age and micro-site productivity) in a polygonal format, over entire forest management areas in Ontario. Themes Enhance the production of the Enhanced Forest Resource Inventory (eFRI) Integrate and refine photo interpreted attributes and LiDAR Develop object-based image assesment techniques to use with LiDAR for species identification Deliver open source code and methods Description A key requirement of sustainable forest management is the establishment and maintenance of forest inventories to provide accurate and timely information on the state of the forest to support a variety of purposes and information needs. In Ontario, where extensive forest management practices dominate, forest inventories are derived from a multi-stage process that involves acquiring aerial photography, using the photos to delineate homogenous units or forest stands, and then interpreting attributes from the photography for those delineated stands, often with the aid of stereo vision. Due to the capability of airborne Single Photon LiDAR (SPL) to provide a detailed three-dimensional view of forest structure, the technology and related analysis approaches have transformed the derivation of forest inventories. Attributes such as volume, biomass, diameter, and basal area as well as information on canopy cover and its vertical distribution have all been shown to meet or exceed accuracy requirements in operational forest management with relative root mean square errors (%RMSE) typically lower than 20%. A key issue in utilising SPL data to derive forest inventory information is that a number of forest attributes required to provide the full complement of variables necessary for strategic-level forest inventories such as species group, composition, stand age, and site productivity, are not well predicted from SPL data alone, due to the lack of spectral and other information in the returned point cloud. A second issue is that the development of Enhanced Forest Inventories (EFI) using the area-based approaches (ABA) results in raster-based predictions of forest attributes. While these raters are produced at a fine grid cell (typically 20 x 20 m), most forest management decision making processes, such as harvest planning and scheduling, growth and yield estimation and longer term estate planning relies on polygonal data structures, such as those manually derived from photo interpreters. As a result, two gaps are evident when introducing SPL-based inventories into a forest management framework for strategic and tactical decision making. First, raster based EFI information needs to be transformed into polygonal structures and second, new approaches are needed to derive the attributes required for sustainable forest management decision making that are not readily predicted from SPL data alone. The goal of this project is to develop and apply an open source methodological framework that combines raster EFI predictions of stand structure, with environmental and satellite data to produce comprehensive polygonal forest inventory information in a spatially exhaustive way, over large forested areas of Ontario. These new layers will be polygon based, with the full set of required attributes, at a scale comparable to existing photo-interpreted inventory, to ensure they are compatible with subsequent forest growth and yield, harvest and estate planning software and activities. Design We propose three forest management units as focus sites for this research. The Romeo Malette Forest, which is an actively managed forest by GreenFirst Forest Products and is a typical example of a boreal forest management unit in this region. Forest harvesting practices result in an array of stand development stages which are associated with a range of vegetation structures. A second focus site will be forest management areas within the Great Lakes forest FMU, including French Severn forest or Algonquin Provincial Park. These forest areas have distinct and complementary ecologies, resulting in a range of species and structures not evident in the northern boreal. Likewise additional datasets to utilise in the model development will be different, resulting in different approaches than the methodology developed for boreal systems. If time and resources are available we would apply the approach in the second year at a third site to ensure the approaches has been applied to all three regions. This would provide the Ministry with worked examples to aid tech/knowledge transfer at the regional level and testing operational scalability across Ontario. Methodology First, image segmentation (or object based image analysis) will be performed on key EFI-derived raster attributes of stand (Lorey’s) height and canopy cover to derive structurally-homogeneous micro-stand objects. We will use newly developed open source segmentation tools, specifically developed for forestry LIDAR data to produce wall-to-wall polygon representations of EFI raster layers. Second, the boundaries of these micro-stand objects will be used to extract information from other EFI layers (volume, basal area, crown closure) transforming them from rasters into attributed micro-stand polygons. Microstands will then be cleaned and merged based on size constraints and differences within and between segments (such as between vs within variations in volume and basal area). We will utilise extensive information already produced by the Ontario MNR on Ontario Forest Resources Inventory Photo Interpretation Specifications which state key area, perimeter and quality control standards for polygon delineation to meet Ontario forestry needs. These standards will be used to ensure that the derived polygons from the raster EFI’s will closely resemble the current FRI polygons in terms of area, shape and perimeter distributions. In the third step we will develop an imputation-based approach to derive the remaining attributes not readily predicted from LIDAR including age, species group / species composition or forest unit, and site index. We will utilise the existing Ontario FRI polygons within the forest management area. For each FRI polygon we will extract positional (latitude / longitude), terrain, and climatic information from auxiliary 30 m environmental data. We will also access considerable work completed and underway mapping forest age from historical Landsat data and spatial databases of fire, harvesting, and road disturbances compiled as part of the Boreal Disturbance Database. If available, this type of information will also be compiled and attributed into each delineated polygon. We will then develop an imputation reference dataset which links the four desired attributes to both the structural attributes within the FRI (volume, density, basal area and height), as well as the location, terrain and climate data. This will produce a reference library of every combination of structural and compositional conditions. In the final step, we will impute the additional forest inventory attributes for each SPL derived micro-stand stand including age, species group / composition, and site index combination for that given stand based on its nearest neighbours in attribute space. We will also work with other researchers funded by KTTD investigating the use of SPL data for additional attribute such as soils prediction to ensure consistency in the use of environmental attributes. We will assess accuracy in a number of ways. For delineation of the micro-stands themselves, we will use GIS spatial overlap algorithms to assess degree of spatial coherence between manually-delineated polygons vs the automatically delineated stands micro-stands from the SPL. For imputation, we will assess agreement using independent validation samples of a hold back of FRI polygons and compare the imputation results with the manually interpreted species, age and site index assessments. Schedule Project Dates June – December 2021: Compile all available SPL data and plot data. Build necessary EFI at each management area. Apply segmentation approaches on raster predictions. MILESTONE: Compiled SPL and EFI datasets over study areas. January – May 2022: Refinement of polygon size and shape based on Provincial input. Development of models for imputation of non-structure variables like species. MILESTONE: Segmentation / object based Algorithm refined, Species / composition databases developed. June – December 2022: Full model development of non-structure attributes. Imputation approach applied over all sites. Field program undertaken at key sites to ensure accuracy of predictions, visits to unusual stand conditions to verify predictions. MILESTONE: Full algorithm testing, Accuracy assessment. January – May 2023: Workshops, code demos, open source code packaging / delivery to all partners. Final validation, Final inventory polygon coverages provided to partners. MILESTONE: Open source code, workshops, peer reviewed papers. Deliverables Key deliverables for this project are: 15 December 2021: Digital Layers Compiled SPL and EFI datasets over study areas. Provided to ministry and industry staff. 15 May 2022: Object based segmentation approach with validation. Code available for broader scale testing and applications. 15 December 2022: Imputation draft paper developed. Imputation code developed ready for testing. 15 May 2023: Open source code release. Workshop for industry and government participants. Peer reviewed papers on approach. Knowledge &amp; Technology Transfer This project is designed to support research, development and technology transfer in the use of transformation technologies such as innovative remote sensing and environmental datasets such as SPL for forest management across Canada’s forest sector. Specifically this project is designed to support the Provincial government of Ontario to improve the accuracy of the forest inventory through the innovative use of SPL data, as well as exploiting the available information on species and structure in the current FRI. By undertaking these activities we contribute to the ongoing transformation of the forest sector through the development and adoption of innovative science-based solutions in particular by linking to the new Ontario governments Forest Sector Strategy. As the application of SPL and other 3-dimensional data to forest inventories becomes more common, the need for joint projects with industry is required to communicate these successes and caveats of these technologies and their appropriate application across Canada. The ultimate outcomes of this project will be a workforce that is more informed on the use of remote sensing technologies for next generation forest inventory applications. We will work with Ontario MNR FRI staff, and GreenFirst Forest Products managers on a number of key methods of outcome dissemination. These will include: Peer-reviewed scientific publications. A series of online workshop on the use of methods for generating the micro-stands and the additional attribution approaches. These will be regional, as needed, to ensure the developed approaches are consistent with regionally specific datasets and needs. Workshops will also be designed to focus on industry forest practitioners, as needed. A technical memo describing the methods developed for key technical staff as it is these staff that are applying these tools themselves as well as for consulting firms which can also be hired by smaller SFLs to implement solutions. Open access to R and other developed programming scripts to undertake the modeling and comparisons. More broadly key outputs of this project will be an increased awareness and capacity building of operational forest managers on the use of these technologies for next generation forest inventory which will lead to an increased ability to dissolve current boundaries between operational (tactical) and strategic (forest level) inventories. In addition, more thorough adoption of SPL technologies as a critical component of forest inventories will benefit other Canadian industries such as survey providers and technology developers by increasing the market and consolidating their international competitiveness, key goals of the Ontario Forest Sector strategy. "],["segmentation-example.html", "Segmentation Example Intro Create Multi-band Raster Run Mean Shift Segmentation Compare Datasets", " Segmentation Example Intro This page provides an example of the image segmentation approach used to derive polygonal forest stands from LiDAR-based raster data. The example data provided is from the Romeo Malette Forest (RMF). First, a three band raster at 20 m spatial resolution is generated from Enhanced Forest Inventory (EFI) metrics. The three bands comprise Lorey’s height, canopy cover (% cover above 2m) and coefficient of variation (standard deviation of LiDAR return heights divided by mean). These bands are re-scaled to comprise values from 0-100, setting the minimum and maximum values to be the 1st and 99th percentile of data observations. Large roads and waterbodies are also masked to prevent them from being included in forest stand polygons. Second, the mean shift algorithm is used to derive forest stand polygons from the input raster. The algorithm used is the LargeScaleMeanShift function from Orfeo ToolBox. Mean Shift is a robust and efficient segmentation algorithm that allows users to provide several parameters to ensure the output polygons meet certain specifications (input parameters described in above link). Although this example only runs the segmentation over a small part of the RMF, segmentation of the entire RMF only takes around 1.5 hrs on a single desktop machine. Third, the output segmentation is compared to polygons derived from manual interpretation. Create Multi-band Raster # load packages library(terra) library(meanShiftR) library(tidyverse) # load file names of SPL rasters to stack lor &lt;- &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/ABA layers SPL 2018/RMF_20m_T130cm_lor.tif&#39; cc &lt;- &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_2m_cov.tif&#39; cv &lt;- &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_cv.tif&#39; # stack rasters spl &lt;- rast(c(lor, cc, cv)) # load RMF shapefile along with sample to be used for example # reproject to match raster rmf &lt;- vect(&#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Land ownership/RMF_Ownership.shp&#39;) %&gt;% project(., spl) rmf_samp &lt;- vect(&#39;D:/ontario_inventory/romeo/RMF_Sample.shp&#39;) %&gt;% project(., spl) # plot RMF overlaid with example plot(rmf, col = &#39;grey&#39;, main = &#39;Romeo Malette Forest with Sample Area in Red&#39;) plot(rmf_samp, col = &#39;red&#39;, add = T) # crop raster stack to sample area spl &lt;- crop(spl, rmf_samp) # apply smoothing function on 5 cell square spl[[1]] &lt;- focal(spl[[1]], w=5, fun=&quot;mean&quot;) spl[[2]] &lt;- focal(spl[[2]], w=5, fun=&quot;mean&quot;) spl[[3]] &lt;- focal(spl[[3]], w=5, fun=&quot;mean&quot;) # load roads and waterbodies roads &lt;- vect(&#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Roads/RMF_roads.shp&#39;) %&gt;% project(., spl) waterb &lt;- vect(&#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Lakes and rivers/RMF_waterbodies.shp&#39;) %&gt;% project(., spl) # subset roads to only mask the main types used by interpreter # RDTYPE = H, P, B roads &lt;- roads[roads$RDTYPE %in% c(&#39;H&#39;, &#39;P&#39;, &#39;B&#39;),] # mask road and water body pixels to NA spl &lt;- spl %&gt;% mask(., roads, inverse = T) %&gt;% mask(., waterb, inverse = T) # if any band is missing values set all to NA spl[is.na(spl[[1]])] &lt;- NA spl[is.na(spl[[2]])] &lt;- NA spl[is.na(spl[[3]])] &lt;- NA # create function to rescale values from 0 to 100 using 1 and 99 percentile scale_100 &lt;- function(x){ # calculate 1st and 99th percentile of input raster perc &lt;- values(x, mat=F) %&gt;% quantile(., probs=c(0.01, 0.99), na.rm=T) # rescale raster using 1st and 99th % x &lt;- (x-perc[1])/(perc[2] - perc[1]) * 100 #reset values below 0 and above 100 x[x &lt; 0] &lt;- 0 x[x &gt; 100] &lt;- 100 return(x) } # rescale rasters from 0 to 100 spl[[1]] &lt;- scale_100(spl[[1]]) spl[[2]] &lt;- scale_100(spl[[2]]) spl[[3]] &lt;- scale_100(spl[[3]]) # plot raster layers plot(spl[[1]], main = &quot;Lorey&#39;s Height (scaled 0-100)&quot;) plot(spl[[2]], main = &quot;Canopy Cover (scaled 0-100)&quot;) plot(spl[[3]], main = &quot;Coefficient of Variation (scaled 0-100)&quot;) # write raster to tif writeRaster(spl, filename=&#39;D:/ontario_inventory/segmentation_ex/spl_stack.tif&#39;, overwrite=T) Run Mean Shift Segmentation # set working directory where temp files will be output setwd(&#39;D:/temp&#39;) # create function to run mean shift meanshift_otb &lt;- function(otb_path = &quot;&quot;, raster_in = &quot;&quot;, out_path = &quot;&quot;, name =&quot;&quot;, spatialr = &quot;10&quot;, ranger = &quot;10&quot;, minsize = &quot;100&quot;, tilesizex = &quot;500&quot;, tilesizey = &quot;500&quot;, outmode = &quot;vector&quot;, cleanup = &quot;true&quot;, ram = &quot;256&quot;){ # Set configuration conf &lt;- paste(&quot;-in&quot;, raster_in, &quot;-spatialr&quot;, spatialr, &quot;-ranger&quot;, ranger, &quot;-minsize&quot;, minsize, &quot;-tilesizex&quot;, tilesizex, &quot;-tilesizey&quot;, tilesizey, &quot;-mode&quot;, outmode, &quot;-mode.vector.out&quot;, paste(out_path, &quot;/&quot;, name, &quot;.shp&quot;, sep=&quot;&quot;), &quot;-cleanup&quot;, cleanup,&quot;-ram&quot;, ram) # apply function in command line system(paste(otb_path, &quot;/otbcli_LargeScaleMeanShift&quot;, &quot; &quot;, conf, sep=&quot;&quot;), intern=T) # save configuration for further use write.table(x = conf,file = paste(out_path,&quot;/&quot;,name,&quot;_conf.txt&quot;,sep=&quot;&quot;),row.names = F, col.names = F) } # run mean shift meanshift_otb(otb_path = &quot;C:/OTB/bin&quot;, raster_in = &#39;D:/ontario_inventory/segmentation_ex/spl_stack.tif&#39;, out_path = &quot;D:/ontario_inventory/segmentation_ex&quot;, name = &quot;ms_10_10_100&quot;, spatialr = &quot;10&quot;, ranger = &quot;10&quot;, minsize = &quot;100&quot;, ram = &quot;1024&quot;) # since mean shift segments missing values into polygons of 1 pixel, aggregate into single polygon # segmented dataset p &lt;- vect(&#39;D:/ontario_inventory/segmentation_ex/ms_10_10_100.shp&#39;) # subset by polygons that only have one pixel (NA) and polygons that have more p_na &lt;- p[p$nbPixels==1,] p_real &lt;- p[p$nbPixels&gt;1,] # dissolve polygons that only have 1 pixels p2 &lt;- aggregate(p_na, by=&#39;nbPixels&#39;) # add back into single file p3 &lt;- rbind(p_real, p2) # write to file writeVector(p3, &#39;D:/ontario_inventory/segmentation_ex/ms_10_10_100_agg_na.shp&#39;, overwrite = T) Compare Datasets # load base imagery base &lt;- rast(&#39;D:/ontario_inventory/romeo/RMF_Sample_Base.tif&#39;) # load manually interpreted polygons and change to raster proj man_poly &lt;- vect(&#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Polygons Inventory/RMF_PolygonForest.shp&#39;) %&gt;% project(., base) %&gt;% crop(., base) # load mean shift derived polygons and change to raster proj ms_poly &lt;- vect(&#39;D:/ontario_inventory/segmentation_ex/ms_10_10_100_agg_na.shp&#39;) %&gt;% project(., base) # plot base imagery with interpreter polygons plot(man_poly, main = &#39;Polygonal Forest Stands Derived Manually&#39;) plotRGB(base, stretch = &#39;lin&#39;, add = T) plot(man_poly, border = &#39;mediumvioletred&#39;, add = T) The above plot shows the manually derived forest stand polygons in the sample area, overlaid on recent true color imagery. Note the clean edges around water bodies, rivers/streams, and road features. # plot base imagery with mean shift polygons plot(ms_poly, main = &#39;Polygonal Forest Stands Derived from EFI Layers using Mean Shift&#39;) plotRGB(base, stretch = &#39;lin&#39;, add = T) plot(ms_poly, border = &#39;red2&#39;, add = T) The above plot shows the mean shift derived forest stand polygons in the sample area. Note that the automated segmentation also has clean edges around water bodies and road features, since these features were masked in the first step. Rivers/streams were not masked due to no clear pattern dictating which features manual interpreters included/excluded. The workflow gives the user control over which features in the landscape should be excluded from the segmentation process. Also note that the polygons never reach the same maximum size as the polygons derived manually. The algorithm takes minimum polygon size as an input parameter, but in general does not cluster the landscape into as large polygons. We will continue to tweak the segmentation algorithm until we are satisfied with the polygonal output. We have also begun working on the imputation step of the project and will provide further results as they become available. We look forward to receiving feedback! "],["segmentation-update.html", "Segmentation Update Intro 1. Create Multi-band Raster 2. Run GRM Segmentation 3a. Calculate Summary Statistics 3b. Compare GRM Segmentation Output to FRI", " Segmentation Update Intro This page provides an update of the segmentation approach used to derive polygons from Airborne Laser Scanning (ALS) gridded summary metrics. Based on feedback from our meetings I have now tested three segmentation algorithms to find the best approach to balance a) the visual component of the segmentation (e.g. shape, overlapping parallel lines) with b) the spectral component (outputting polygons with homogeneous values different from nearby polygons). The three segmentation algorithms I tested are: 1. Large Scale Mean Shift (showcased in the initial segmentation example) 2. Generic Region Merging 3. Simple Linear Iterative Clustering This example will run a segmentation of a small area in the Romeo Malette Forest using the Generic Region Merging algorithm, which I found to be the best candidate algorithm for this application. GRM is an open-source region merging algorithm available from Orfeo Toolbox that uses the same segmentation criterion (called the Baatz &amp; Schape criterion) as the subscription based Multi-resolution Segmentation (MRS) from eCognition software, which is the most popular segmentation method for remote sensing applications. I found GRM to be the most appropriate algorithm to derive meaningful and consistent forest stands, due to its ability to easily balance segment shape with spectral homogeneity, an important trade-off to ensure homogeneous forest stands while also maintaining spatial standards needed for forest operations. First, a three band raster at 20 m spatial resolution is generated from gridded ALS metrics. The three bands comprise P95 height (95th percentile of vegetation return heights above 1.3 m), canopy cover (% cover above 2m) and coefficient of variation (standard deviation of LiDAR return heights divided by mean). I initially used Lorey’s Height instead of P95 height, but am now using P95 height because the two variables are highly correlated (Pearson’s correlation = 0.91) and P95 height provides wall-to-wall coverage within ALS transects whereas Lorey’s Height is modelled for a specific area and only valid within forested landcover types. These bands are re-scaled to comprise values from 0-100, setting the minimum and maximum values to be the 1st and 99th percentile of data observations. Large roads and waterbodies are also masked to prevent them from being included in forest stand polygons. These polygons are re-added to the dataset after segmentation. Second, the GRM algorithm is used to derive forest stand polygons from the input raster. I am using the algorithm parameters I found to work best after conducting an iterative sensitivity analysis, which is described in the corresponding paper. Third, the segmentation output is compared to polygons derived from manual interpretation, both visually and with a full suite of summary and performance metrics including: Minimum, maximum, mean, and median polygon size (expressed in number of hectares) Total number of polygons Mean, standard error, and standard deviation of polygon area, perimeter, and shape index R2 to assess the performance of segmentation outputs in explaining variation in the input variables: P95 height, canopy cover, and coefficient of variation 1. Create Multi-band Raster # load packages library(terra) library(tidyverse) library(janitor) library(sf) library(exactextractr) library(lwgeom) library(magrittr) library(gridExtra) ################################################### ### LOAD MULTI BAND ALS RASTER FOR SEGMENTATION ### ################################################### # load file names of SPL rasters to stack p95 &lt;- &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_p95.tif&#39; cc &lt;- &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_2m_cov.tif&#39; cv &lt;- &#39;D:/ontario_inventory/romeo/RMF_EFI_layers/SPL100 metrics/RMF_20m_T130cm_cv.tif&#39; # stack rasters spl &lt;- rast(c(p95, cc, cv)) # load RMF shapefile along with sample to be used for example # reproject to match raster rmf &lt;- vect(&#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Land ownership/RMF_Ownership.shp&#39;) %&gt;% project(., spl) rmf_samp &lt;- vect(&#39;D:/ontario_inventory/romeo/RMF_Sample.shp&#39;) %&gt;% project(., spl) # plot RMF overlaid with example plot(rmf, col = &#39;grey&#39;) plot(rmf_samp, col = &#39;red&#39;, add = T, main = &#39;Romeo Malette Forest with Sample Area in Red&#39;, cex.main = 3) # crop raster stack to sample area spl &lt;- crop(spl, rmf_samp) # apply smoothing function on 5 cell square spl[[1]] &lt;- focal(spl[[1]], w=5, fun=&quot;mean&quot;) spl[[2]] &lt;- focal(spl[[2]], w=5, fun=&quot;mean&quot;) spl[[3]] &lt;- focal(spl[[3]], w=5, fun=&quot;mean&quot;) #################################### ### MASK ROAD AND WATER POLYGONS ### #################################### # create spl template with all values equal to 1 spl_temp &lt;- spl[[1]] spl_temp[] &lt;- 1 # load photo interpreted polygons poly &lt;- vect(&#39;D:/ontario_inventory/romeo/RMF_EFI_layers/Polygons Inventory/RMF_PolygonForest.shp&#39;) # reproject whole FRI to match lidar poly &lt;- project(poly, spl) # crop FRI to sample area poly &lt;- crop(poly, rmf_samp) # subset polygons that are WAT OR UCL poly_sub &lt;- poly[poly$POLYTYPE %in% c(&#39;WAT&#39;, &#39;UCL&#39;)] # loop through water and unclassified polygons, mask raster, and vectorize for(i in seq_along(poly_sub)){ pt &lt;- poly_sub$POLYTYPE[i] if(i == 1){ spl_pt &lt;- spl_temp %&gt;% crop(poly_sub[i], snap = &#39;out&#39;) %&gt;% mask(poly_sub[i], touches = T) npix &lt;- sum(values(spl_pt), na.rm = T) spl_pt &lt;- as.polygons(spl_pt) names(spl_pt) &lt;- &#39;POLYTYPE&#39; spl_pt$POLYTYPE &lt;- pt spl_pt$nbPixels &lt;- npix } else{ spl_hold &lt;- spl_temp %&gt;% crop(poly_sub[i], snap = &#39;out&#39;) %&gt;% mask(poly_sub[i], touches = T) npix &lt;- sum(values(spl_hold), na.rm = T) spl_hold &lt;- as.polygons(spl_hold) names(spl_hold) &lt;- &#39;POLYTYPE&#39; spl_hold$POLYTYPE &lt;- pt spl_hold$nbPixels &lt;- npix spl_pt &lt;- rbind(spl_pt, spl_hold) } } # mask lidar outside of FRI spl &lt;- mask(spl, poly, inverse = F, touches = T) # mask WAT and UCL polygons spl &lt;- mask(spl, poly_sub, inverse = T, touches = T) ########################################## ### DEAL WITH MISSING DATA AND RESCALE ### ########################################## # if any band is missing values set all to NA spl[is.na(spl[[1]])] &lt;- NA spl[is.na(spl[[2]])] &lt;- NA spl[is.na(spl[[3]])] &lt;- NA # create function to rescale values from 0 to 100 using 1 and 99 percentile scale_100 &lt;- function(x){ # calculate 1st and 99th percentile of input raster perc &lt;- values(x, mat=F) %&gt;% quantile(., probs=c(0.01, 0.99), na.rm=T) # rescale raster using 1st and 99th % x &lt;- (x-perc[1])/(perc[2] - perc[1]) * 100 #reset values below 0 and above 100 x[x &lt; 0] &lt;- 0 x[x &gt; 100] &lt;- 100 return(x) } # rescale rasters from 0 to 100 spl[[1]] &lt;- scale_100(spl[[1]]) spl[[2]] &lt;- scale_100(spl[[2]]) spl[[3]] &lt;- scale_100(spl[[3]]) # plot raster layers plot(spl[[1]]) title(main = &quot;P95 Height (scaled 0-100)&quot;, cex.main = 3, line = -0.0001) plot(spl[[2]]) title(main = &quot;Canopy Cover (scaled 0-100)&quot;, cex.main = 3, line = -0.0001) plot(spl[[3]]) title(main = &quot;Coefficient of Variation (scaled 0-100)&quot;, cex.main = 3, line = -0.0001) # write raster to tif writeRaster(spl, filename=&#39;D:/ontario_inventory/segmentation_ex/spl_stack_grm.tif&#39;, overwrite=T) # write masked water and unclassified polygons to shp writeVector(spl_pt, &#39;D:/ontario_inventory/segmentation_ex/wat_ucl_polygons.shp&#39;, overwrite = T) 2. Run GRM Segmentation ######################### ### RUN GRM ALGORITHM ### ######################### # set working directory where temp files will be output setwd(&#39;D:/temp&#39;) # create function to run generic region merging grm_otb &lt;- function(otb_path = &quot;&quot;, raster_in = &quot;&quot;, out_path = &quot;&quot;, name = &quot;&quot;, method = &quot;bs&quot;, thresh = &quot;&quot;, spec = &quot;0.5&quot;, spat = &quot;0.5&quot;){ # Set configuration conf &lt;- paste(&quot;-in&quot;, raster_in, &quot;-out&quot;, paste(out_path, &quot;/&quot;, name, &quot;.tif&quot;, sep=&quot;&quot;), &quot;-criterion&quot;, method, &quot;-threshold&quot;, thresh, &quot;-cw&quot;, spec, &quot;-sw&quot;, spat) # apply function in command line system(paste(otb_path, &quot;/otbcli_GenericRegionMerging&quot;, &quot; &quot;, conf, sep=&quot;&quot;)) # save configuration for further use write.table(x = conf,file = paste(out_path,&quot;/&quot;,name,&quot;_conf.txt&quot;,sep=&quot;&quot;),row.names = F, col.names = F) } # run grm grm_otb(otb_path = &quot;C:/OTB/bin&quot;, raster_in = &quot;D:/ontario_inventory/segmentation_ex/spl_stack_grm.tif&quot;, out_path = &quot;D:/ontario_inventory/segmentation_ex&quot;, name = &quot;grm_ex&quot;, thresh = &quot;10&quot;, spec = &quot;0.1&quot;, spat = &quot;0.5&quot;) ########################### ### MASK MISSING VALUES ### ########################### # load grm segments raster p &lt;- rast(&quot;D:/ontario_inventory/segmentation_ex/grm_ex.tif&quot;) # load single band of raster input for segmentation to use as mask mask &lt;- rast(&quot;D:/ontario_inventory/segmentation_ex/spl_stack_grm.tif&quot;) %&gt;% .[[1]] # mask grm segments raster p &lt;- mask(p, mask) # re-write masked grm raster writeRaster(p, &quot;D:/ontario_inventory/segmentation_ex/grm_ex.tif&quot;, overwrite = T) ################################################## ### CONVERT TO POLYGONS AND CALC POLYGON SIZES ### ################################################## # convert to vector based on cell value vec &lt;- as.polygons(p) # create table of number of pixels in each polygon num &lt;- as.vector(values(p)) num_pix &lt;- tabyl(num) # drop na row num_pix &lt;- na.omit(num_pix) # get pixel ids from vector vec_dat &lt;- tibble(id = values(vec)[,1]) colnames(vec_dat) &lt;- &#39;id&#39; # loop through values and add to vector data vec_dat$nbPixels &lt;- NA for(i in 1:NROW(vec_dat)){ vec_dat$nbPixels[i] &lt;- num_pix$n[num_pix$num == vec_dat$id[i]] } # remove current column of data and add id # add nbPixels to vector vec &lt;- vec[,-1] vec$id &lt;- vec_dat$id vec$nbPixels &lt;- vec_dat$nbPixels 3a. Calculate Summary Statistics ############################################## ### RE-ADD WATER AND UNCLASSIFIED POLYGONS ### ############################################## # load polygon dataset as p p &lt;- vec # reproject segmented polygons to ensure same crs as wat and ucl polygons p &lt;- project(p, spl_pt) # add WAT/UCL POLYTYPE polygons back in p &lt;- rbind(p, spl_pt) ####################################################### ### EXTRACT ALS METRICS AND CALCULATE SUMMARY STATS ### ####################################################### # convert to sf p_sf &lt;- st_as_sf(p) # calculate perimeter p$perim &lt;- st_perimeter(p_sf) %&gt;% as.numeric # calculate area p$area &lt;- st_area(p_sf) %&gt;% as.numeric # calculate mean shape index p$msi &lt;- p$perim/sqrt(pi * p$area) # write to file to backup writeVector(p, &quot;D:/ontario_inventory/segmentation_ex/grm_ex.shp&quot;, overwrite = T) # subset non masked WAT and UCL polygons p2_sf &lt;- p[is.na(p$POLYTYPE)] %&gt;% st_as_sf p2 &lt;- p[is.na(p$POLYTYPE)] %&gt;% as.data.frame # load original raster input file ras &lt;- rast(&quot;D:/ontario_inventory/segmentation_ex/spl_stack_grm.tif&quot;) # rename bands names(ras) &lt;- c(&#39;p95&#39;, &#39;cc&#39;, &#39;cv&#39;) # extract pixel values pvals &lt;- exact_extract(ras, p2_sf) # calculate SSE sse &lt;- sapply(pvals, FUN = function(x){ p95_mean &lt;- mean(x$p95, na.rm = T) cc_mean &lt;- mean(x$cc, na.rm = T) cv_mean &lt;- mean(x$cv, na.rm = T) return(c(sum((x$p95 - p95_mean)^2, na.rm = T), sum((x$cc - cc_mean)^2, na.rm = T), sum((x$cv - cv_mean)^2, na.rm = T))) }) # transpose sse &lt;- t(sse) # calculate final sums sse &lt;- colSums(sse) # unlist values pvals2 &lt;- do.call(rbind, pvals) # calculate global mean values p95_mean &lt;- mean(pvals2$p95, na.rm = T) cc_mean &lt;- mean(pvals2$cc, na.rm = T) cv_mean &lt;- mean(pvals2$cv, na.rm = T) rm(pvals2) # calculate SST sst &lt;- sapply(pvals, FUN = function(x){ return(c(sum((x$p95 - p95_mean)^2, na.rm = T), sum((x$cc - cc_mean)^2, na.rm = T), sum((x$cv - cv_mean)^2, na.rm = T))) }) # transpose sst &lt;- t(sst) # calculate final sums sst &lt;- colSums(sst) # calculate r2 values r2_p95 &lt;- 1 - (sse[1]/sst[1]) %&gt;% round(3) r2_cc &lt;- 1 - (sse[2]/sst[2]) %&gt;% round(3) r2_cv &lt;- 1 - (sse[3]/sst[3]) %&gt;% round(3) r2_all &lt;- (sum(r2_p95, r2_cc, r2_cv) / 3) %&gt;% round(3) # create standard error function se &lt;- function(x){sd(x) / sqrt(length(x))} # create dataframe of all summary stats df &lt;- data.frame( alg = &#39;GRM&#39;, min_pix = (min(p2$nbPixels)), max_pix = (max(p2$nbPixels)), mean_pix = (mean(p2$nbPixels)), med_pix = (median(p2$nbPixels)), num_poly = NROW(p2), mean_area = mean(p2$area), se_area = se(p2$area), sd_area = sd(p2$area), mean_perim = mean(p2$perim), se_perim = se(p2$perim), sd_perim = sd(p2$perim), mean_msi = mean(p2$msi), se_msi = se(p2$msi), sd_msi = sd(p2$msi), r2_p95 &lt;- r2_p95, r2_cc &lt;- r2_cc, r2_cv &lt;- r2_cv, r2_all &lt;- r2_all ) # round numeric columns df %&lt;&gt;% mutate_at(c(&#39;min_pix&#39;, &#39;max_pix&#39;, &#39;mean_pix&#39;, &#39;med_pix&#39;, &#39;mean_area&#39;, &#39;se_area&#39;, &#39;sd_area&#39;, &#39;mean_perim&#39;, &#39;se_perim&#39;, &#39;sd_perim&#39;), function(x) round(x, 2)) %&gt;% mutate_at(c(&#39;mean_msi&#39;, &#39;se_msi&#39;, &#39;sd_msi&#39;), function(x) round(x, 4)) ####################################### ### CALCULATE SUMMARY STATS FOR FRI ### ####################################### # load interpreter derived polygons to extract statistics p &lt;- poly # convert to sf p_sf &lt;- st_as_sf(p) # calculate perimeter p$perim &lt;- st_perimeter(p_sf) %&gt;% as.numeric # calculate area p$area &lt;- st_area(p_sf) %&gt;% as.numeric # calculate msi p$msi &lt;- p$perim/sqrt(pi * p$area) # calculate nbPixels p$nbPixels &lt;- p$area / 400 # write to file to backup writeVector(p, &quot;D:/ontario_inventory/segmentation_ex/fri_ex.shp&quot;, overwrite = T) # subset all non water/ucl polygons p2_sf &lt;- p[!(p$POLYTYPE %in% c(&#39;WAT&#39;, &#39;UCL&#39;))] %&gt;% st_as_sf p2 &lt;- p[!(p$POLYTYPE %in% c(&#39;WAT&#39;, &#39;UCL&#39;))] %&gt;% as.data.frame # extract pixel values pvals &lt;- exact_extract(ras, p2_sf) # calculate SSE sse &lt;- sapply(pvals, FUN = function(x){ # subset values based on coverage fraction x %&lt;&gt;% filter(coverage_fraction &gt;= 0.5) p95_mean &lt;- mean(x$p95, na.rm = T) cc_mean &lt;- mean(x$cc, na.rm = T) cv_mean &lt;- mean(x$cv, na.rm = T) return(c(sum((x$p95 - p95_mean)^2, na.rm = T), sum((x$cc - cc_mean)^2, na.rm = T), sum((x$cv - cv_mean)^2, na.rm = T))) }) # transpose sse &lt;- t(sse) # calculate final sums sse &lt;- colSums(sse) # unlist values pvals2 &lt;- do.call(rbind, pvals) # subset values based on coverage fraction pvals2 %&lt;&gt;% filter(coverage_fraction &gt;= 0.5) # calculate global mean values p95_mean &lt;- mean(pvals2$p95, na.rm = T) cc_mean &lt;- mean(pvals2$cc, na.rm = T) cv_mean &lt;- mean(pvals2$cv, na.rm = T) rm(pvals2) # calculate SST sst &lt;- sapply(pvals, FUN = function(x){ # subset values based on coverage fraction x %&lt;&gt;% filter(coverage_fraction &gt;= 0.5) return(c(sum((x$p95 - p95_mean)^2, na.rm = T), sum((x$cc - cc_mean)^2, na.rm = T), sum((x$cv - cv_mean)^2, na.rm = T))) }) # transpose sst &lt;- t(sst) # calculate final sums sst &lt;- colSums(sst) # calculate r2 values r2_p95 &lt;- 1 - (sse[1]/sst[1]) %&gt;% round(3) r2_cc &lt;- 1 - (sse[2]/sst[2]) %&gt;% round(3) r2_cv &lt;- 1 - (sse[3]/sst[3]) %&gt;% round(3) r2_all &lt;- (sum(r2_p95, r2_cc, r2_cv) / 3) %&gt;% round(3) # create dataframe with values wanted df_fri &lt;- data.frame(alg = &#39;FRI&#39;, min_pix = (min(p2$area / 400)), max_pix = (max(p2$area / 400)), mean_pix = (mean(p2$area / 400)), med_pix = (median(p2$area / 400)), num_poly = NROW(p2), mean_area = mean(p2$area), se_area = se(p2$area), sd_area = sd(p2$area), mean_perim = mean(p2$perim), se_perim = se(p2$perim), sd_perim = sd(p2$perim), mean_msi = mean(p2$msi), se_msi = se(p2$msi), sd_msi = sd(p2$msi), r2_p95 &lt;- r2_p95, r2_cc &lt;- r2_cc, r2_cv &lt;- r2_cv, r2_all &lt;- r2_all) # round numeric columns df_fri %&lt;&gt;% mutate_at(c(&#39;min_pix&#39;, &#39;max_pix&#39;, &#39;mean_pix&#39;, &#39;med_pix&#39;), function(x) round(x)) %&gt;% mutate_at(c(&#39;mean_area&#39;, &#39;se_area&#39;, &#39;sd_area&#39;, &#39;mean_perim&#39;, &#39;se_perim&#39;, &#39;sd_perim&#39;), function(x) round(x, 2)) %&gt;% mutate_at(c(&#39;mean_msi&#39;, &#39;se_msi&#39;, &#39;sd_msi&#39;), function(x) round(x, 4)) # bind df df &lt;- rbind(df, df_fri) # write summary stats to csv as backup write.csv(df, file = &#39;D:/ontario_inventory/segmentation_ex/grm_ex_summary_stats.csv&#39;, row.names = F) 3b. Compare GRM Segmentation Output to FRI # load base imagery base &lt;- rast(&#39;D:/ontario_inventory/romeo/RMF_Sample_Base.tif&#39;) # load FRI and change to raster proj fri_poly &lt;- vect(&quot;D:/ontario_inventory/segmentation_ex/fri_ex.shp&quot;) %&gt;% project(., base) %&gt;% crop(., base) # load GRM polygons and change to raster proj grm_poly &lt;- vect(&quot;D:/ontario_inventory/segmentation_ex/grm_ex.shp&quot;) %&gt;% project(., base) %&gt;% crop(., base) # plot base imagery with FRI polygons plot(fri_poly) plotRGB(base, stretch = &#39;lin&#39;, add = T) plot(fri_poly, border = &#39;mediumvioletred&#39;, lwd = 2, add = T) title(main = &#39;FRI Polygons Overlaid on True Color Image&#39;, line = -2, cex.main = 3) The above plot shows the FRI polygons in the sample area, overlaid on recent true color imagery. Note the clean edges around water bodies, rivers/streams, and road features. # plot base imagery with GRM polygons plot(grm_poly) plotRGB(base, stretch = &#39;lin&#39;, add = T) plot(grm_poly, border = &#39;red2&#39;, lwd = 2, add = T) title(main = &#39;GRM Polygons Overlaid on True Color Image&#39;, line = -2, cex.main = 3) The above plot shows the GRM derived polygons in the sample area. Note that the automated segmentation also has clean edges around water and road features, since these features were masked in the first step, and the polygons re-added after segmentation. The GRM polygons are also much more compact and uniform than the FRI polygons, and generally do not have issues with the line-work (overlapping and parallel lines), which was an important point mentioned in our meetings. #################################### ### DISTRIBUTION OF POLYGON SIZE ### #################################### # remove wat and ucl polygons and convert to df fri_poly &lt;- fri_poly[!(fri_poly$POLYTYPE %in% c(&#39;WAT&#39;, &#39;UCL&#39;))] %&gt;% as.data.frame grm_poly &lt;- grm_poly[is.na(grm_poly$POLYTYPE)] %&gt;% as.data.frame # plot polygon size FRI in Ha g1 &lt;- ggplot(data.frame(nbPixels = fri_poly$nbPixels/25), aes(x = nbPixels)) + geom_density(fill = &#39;grey&#39;) + xlim(c(0, 500/25)) + ylim(c(0, 0.17)) + geom_vline(aes(xintercept = median(nbPixels)), linetype = &quot;dashed&quot;, size = 0.6) + theme_bw() + xlab(&#39;Number of Hectares&#39;) + ylab(&#39;Density&#39;) + ggtitle(&#39;FRI Distribution of Polygon Size&#39;) + theme(text = element_text(size = 25)) # plot polygon size GRM in Ha g2 &lt;- ggplot(data.frame(nbPixels = grm_poly$nbPixels/25), aes(x = nbPixels)) + geom_density(fill = &#39;grey&#39;) + xlim(c(0, 500/25)) + ylim(c(0, 0.17)) + geom_vline(aes(xintercept = median(nbPixels)), linetype = &quot;dashed&quot;, size = 0.6) + theme_bw() + xlab(&#39;Number of Hectares&#39;) + ylab(&#39;Density&#39;) + ggtitle(&#39;GRM Distribution of Polygon Size&#39;) + theme(text = element_text(size = 25)) # plot together grid.arrange(g1, g2) Above is the distribution of polygon size within the sample area. The dotted line denotes the median values, which are very close between the FRI and GRM polygons. Both distributions have a long tail toward large polygons, but the GRM polygons have a more uniform size and less large polygons. Water and Unclassified polygons are not included. ################################### ### DISTRIBUTION OF SHAPE INDEX ### ################################### # plot shape index FRI g1 &lt;- ggplot(data.frame(msi = as.numeric(fri_poly$msi)), aes(x = msi)) + geom_density(fill = &#39;grey&#39;) + xlim(c(0, 8)) + ylim(c(0, 1.5)) + geom_vline(aes(xintercept = median(msi)), linetype = &quot;dashed&quot;, size = 0.6) + theme_bw() + xlab(&#39;Shape Index&#39;) + ylab(&#39;Density&#39;) + ggtitle(&#39;FRI Distribution of Shape Index&#39;) + theme(text = element_text(size = 25)) # plot shape index GRM g2 &lt;- ggplot(data.frame(msi = as.numeric(grm_poly$msi)), aes(x = msi)) + geom_density(fill = &#39;grey&#39;) + xlim(c(0, 8)) + ylim(c(0, 1.5)) + geom_vline(aes(xintercept = median(msi)), linetype = &quot;dashed&quot;, size = 0.6) + theme_bw() + xlab(&#39;Shape Index&#39;) + ylab(&#39;Density&#39;) + ggtitle(&#39;GRM Distribution of Shape Index&#39;) + theme(text = element_text(size = 25)) # plot together grid.arrange(g1, g2) The above plots show Shape Index for FRI and GRM polygons in the sample area (Water and Unclassified polygons are not included). The dotted line is the median value. Shape Index has a value of 1 for circular objects and increases without limits with increased complexity and irregularity. The GRM polygons have a much more compact shape (lower Shape Index) as well as a more uniform and tight distribution. These characteristics are visible in the above plots of the polygons. ########################################### ### Summary Stats and Additional Tables ### ########################################### # Size t1 &lt;- as.tibble(df[, 1:6]) # change to ha t1[, 2:5] &lt;- round(t1[, 2:5] / 25, 2) names(t1) &lt;- c(&#39;Dataset&#39;, &quot;Min Ha&quot;, &#39;Max Ha&#39;, &quot;Mean Ha&quot;, &quot;Median Ha&quot;, &quot;Number of Polygons&quot;) knitr::kable(t1, caption = &quot;Table 1: Polygon Size Stats&quot;, label = NA) Table 1: Polygon Size Stats Dataset Min Ha Max Ha Mean Ha Median Ha Number of Polygons GRM 0.04 42.84 4.94 4.12 25774 FRI 0.00 184.00 7.56 3.92 18202 Table 1 shows Polygon size stats. The median values are very close, but the mean GRM polygon size is smaller, as is the maximum polygon size. Thus, the GRM output has more polygons overall than the FRI. # Area and Perim t2 &lt;- as.tibble(df[, c(1, 7:12)]) names(t2) &lt;- c(&quot;Dataset&quot;, &quot;Mean Area (m2)&quot;, &quot;SE Area&quot;, &quot;SD Area&quot;, &quot;Mean Perimeter (m)&quot;, &quot;SE Perimeter&quot;, &quot;SD Perimeter&quot;) knitr::kable(t2, caption = &quot;Table 2: Polygon Area and Perimeter Stats&quot;, label = NA) Table 2: Polygon Area and Perimeter Stats Dataset Mean Area (m2) SE Area SD Area Mean Perimeter (m) SE Perimeter SD Perimeter GRM 49366.70 209.81 33682.75 1133.89 2.30 368.70 FRI 75643.27 830.58 112057.39 1664.71 11.87 1600.98 Table 2 shows area and perimeter stats. The mean area and perimeter of GRM polygons are smaller than the FRI. The variation is also smaller, since the GRM polygons are more uniform. # Mean Shape Index t3 &lt;- as.tibble(df[, c(1, 13:15)]) names(t3) &lt;- c(&quot;Dataset&quot;, &quot;Mean Shape Index&quot;, &quot;SE Shape Index&quot;, &quot;SD Shape Index&quot;) knitr::kable(t3, caption = &quot;Table 3: Polygon Shape Index Stats&quot;, label = NA) Table 3: Polygon Shape Index Stats Dataset Mean Shape Index SE Shape Index SD Shape Index GRM 3.0626 0.0028 0.4501 FRI 3.8114 0.0097 1.3145 Table 3 presents Mean Shape Index, an indication of the regularity of polygon shape, with values of 1 representing perfect circles and values increasing without maximum with more complexity. We can see that GRM polygons are less complex (more compact) than FRI polygons and that shape varies less in the GRM output. # R2 t4 &lt;- as.tibble(df[,c(1, 16:19)]) names(t4) &lt;- c(&#39;Dataset&#39;, &#39;R2 P95&#39;, &#39;R2 Can Cov&#39;, &#39;R2 Coeff Var&#39;, &#39;R2 Overall&#39;) knitr::kable(t4, caption = &quot;Table 4: Polygon R2 Stats&quot;, label = NA) Table 4: Polygon R2 Stats Dataset R2 P95 R2 Can Cov R2 Coeff Var R2 Overall GRM 0.894 0.932 0.847 0.891 FRI 0.819 0.861 0.701 0.794 The last table shows R2 values, which give an indication of how well the segmentation output is explaining the variation among ALS input variables. We can see that the GRM output has a high R2 value of 0.89 (overall R2), which is a strong indicator that the segmentation algorithm is effectively explaining structural differences between polygons. The manually segmented FRI polygons also have a very good R2 value, indicating the skill of interpreters in delineating forest structure. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
